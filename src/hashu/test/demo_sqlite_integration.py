"""
SQLiteå­˜å‚¨é›†æˆæ¼”ç¤º
å±•ç¤ºhashuæ¨¡å—çš„SQLite + JSONåŒå­˜å‚¨åŠŸèƒ½å’Œæ™ºèƒ½æŸ¥è¯¢ç‰¹æ€§
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Any

from hashu.core.calculate_hash_custom import ImageHashCalculator, HashCache, get_db_cached
from hashu.utils.hash_process_config import setup_multiprocess_hash_environment
from loguru import logger


def setup_demo_environment():
    """è®¾ç½®æ¼”ç¤ºçŽ¯å¢ƒ"""
    logger.info("ðŸš€ è®¾ç½®SQLiteå­˜å‚¨æ¼”ç¤ºçŽ¯å¢ƒ")
    
    # é…ç½®å¤šè¿›ç¨‹çŽ¯å¢ƒå¹¶å¯ç”¨SQLite
    HashCache.configure_multiprocess(
        enable_auto_save=True,
        enable_global_cache=True,
        use_sqlite=True,
        sqlite_priority=True
    )
    
    logger.info("âœ… SQLiteå­˜å‚¨å·²å¯ç”¨")


def demo_basic_hash_calculation():
    """æ¼”ç¤ºåŸºæœ¬å“ˆå¸Œè®¡ç®—å’Œå­˜å‚¨"""
    logger.info("\n=== åŸºæœ¬å“ˆå¸Œè®¡ç®—æ¼”ç¤º ===")
    
    # æµ‹è¯•å›¾ç‰‡è·¯å¾„ï¼ˆè¯·æ ¹æ®å®žé™…æƒ…å†µè°ƒæ•´ï¼‰
    test_images = [
        r"E:\2EHV\test\0.jpg",
        r"E:\2EHV\test\1.jpg",
        r"D:\1VSCODE\Projects\ImageAll\ImageFilter\test_images\sample1.jpg",
        ".",  # å½“å‰ç›®å½•æŸ¥æ‰¾
    ]
    
    # æŸ¥æ‰¾å®žé™…å­˜åœ¨çš„æµ‹è¯•å›¾ç‰‡
    actual_images = []
    for img_path in test_images:
        if img_path == ".":
            # åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
            for ext in ['*.jpg', '*.png', '*.jpeg', '*.webp']:
                found = list(Path(".").glob(ext))
                actual_images.extend([str(f) for f in found[:2]])  # æœ€å¤šå–2ä¸ª
        elif os.path.exists(img_path):
            actual_images.append(img_path)
    
    if not actual_images:
        logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡ï¼Œåˆ›å»ºç¤ºä¾‹è·¯å¾„")
        # åˆ›å»ºä¸€äº›archive://åè®®çš„ç¤ºä¾‹URIç”¨äºŽæµ‹è¯•
        test_uris = [
            "archive:///test/sample.zip!/images/photo1.jpg",
            "archive:///test/sample.zip!/images/photo1.png",
            "archive:///test/sample.zip!/images/photo1.webp",
            "/regular/path/to/image.jpg"
        ]
        
        # ç›´æŽ¥æ·»åŠ åˆ°æ•°æ®åº“è¿›è¡Œæµ‹è¯•
        db = get_db_cached()
        for i, uri in enumerate(test_uris):
            hash_value = f"test_hash_{i:04d}"
            metadata = {
                'file_size': 1024 * (i + 1),
                'width': 800 + i * 100,
                'height': 600 + i * 100
            }
            success = db.add_hash(uri, hash_value, metadata=metadata)
            logger.info(f"  æ·»åŠ æµ‹è¯•è®°å½• {uri}: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        return
    
    logger.info(f"æ‰¾åˆ° {len(actual_images)} ä¸ªæµ‹è¯•å›¾ç‰‡")
    
    # è®¡ç®—å“ˆå¸Œå€¼
    results = []
    for img_path in actual_images[:3]:  # æœ€å¤šå¤„ç†3ä¸ªå›¾ç‰‡
        logger.info(f"\nðŸ“¸ å¤„ç†å›¾ç‰‡: {img_path}")
        
        start_time = time.time()
        result = ImageHashCalculator.calculate_phash(img_path)
        end_time = time.time()
        
        if result:
            results.append(result)
            logger.info(f"  âœ… å“ˆå¸Œå€¼: {result['hash']}")
            logger.info(f"  ðŸ“Š æ¥æº: {result.get('storage_backend', 'æœªçŸ¥')}")
            logger.info(f"  â±ï¸ è€—æ—¶: {end_time - start_time:.3f}ç§’")
            logger.info(f"  ðŸ—‚ï¸ ç¼“å­˜å‘½ä¸­: {'æ˜¯' if result.get('from_cache') else 'å¦'}")
            
            # æ˜¾ç¤ºå…ƒæ•°æ®
            if metadata := result.get('metadata'):
                logger.info(f"  ðŸ“ å°ºå¯¸: {metadata.get('width')}x{metadata.get('height')}")
                logger.info(f"  ðŸ“„ æ ¼å¼: {metadata.get('format')}")
        else:
            logger.error(f"  âŒ å“ˆå¸Œè®¡ç®—å¤±è´¥")
    
    return results


def demo_smart_query():
    """æ¼”ç¤ºæ™ºèƒ½æŸ¥è¯¢åŠŸèƒ½ï¼ˆæ ¼å¼è½¬æ¢åŒ¹é…ï¼‰"""
    logger.info("\n=== æ™ºèƒ½æŸ¥è¯¢æ¼”ç¤º ===")
    
    db = get_db_cached()
    
    # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®ç”¨äºŽæ™ºèƒ½æŸ¥è¯¢
    test_data = [
        ("archive:///test/photos.zip!/vacation/beach.jpg", "abc123def456"),
        ("archive:///test/photos.zip!/vacation/beach.png", "abc123def456"),
        ("archive:///test/photos.zip!/vacation/beach.webp", "abc123def456"),
        ("archive:///test/photos.zip!/vacation/mountain.jpg", "xyz789uvw012"),
        ("/local/path/image.jpg", "local123hash"),
    ]
    
    logger.info("ðŸ“ æ·»åŠ æµ‹è¯•æ•°æ®...")
    for uri, hash_value in test_data:
        metadata = {'test_data': True, 'width': 1920, 'height': 1080}
        db.add_hash(uri, hash_value, metadata=metadata)
        logger.info(f"  æ·»åŠ : {uri}")
    
    # æµ‹è¯•æ™ºèƒ½æŸ¥è¯¢
    test_queries = [
        "archive:///test/photos.zip!/vacation/beach.gif",  # ä¸å­˜åœ¨çš„æ ¼å¼ï¼Œåº”è¯¥æ‰¾åˆ°å…¶ä»–æ ¼å¼
        "archive:///test/photos.zip!/vacation/beach.avif", # å¦ä¸€ä¸ªä¸å­˜åœ¨çš„æ ¼å¼
        "/local/path/image.png",  # æœ¬åœ°è·¯å¾„çš„æ ¼å¼è½¬æ¢
    ]
    
    logger.info("\nðŸ” æµ‹è¯•æ™ºèƒ½æŸ¥è¯¢...")
    for query_uri in test_queries:
        logger.info(f"\nðŸŽ¯ æŸ¥è¯¢: {query_uri}")
        
        # ä½¿ç”¨HashCacheçš„æ™ºèƒ½æŸ¥è¯¢
        hash_value = HashCache.get_hash(query_uri)
        if hash_value:
            logger.info(f"  âœ… æ‰¾åˆ°å“ˆå¸Œå€¼: {hash_value}")
        else:
            logger.info("  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„å“ˆå¸Œå€¼")
        
        # ä½¿ç”¨SQLiteçš„è¯¦ç»†æ™ºèƒ½æŸ¥è¯¢
        results = HashCache.smart_query_with_formats(query_uri, ['jpg', 'png', 'webp'])
        if results:
            logger.info(f"  ðŸ“‹ æ™ºèƒ½æŸ¥è¯¢æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æžœ:")
            for result in results:
                logger.info(f"    - {result['uri']} -> {result['hash_value']}")
        else:
            logger.info("  ðŸ“‹ æ™ºèƒ½æŸ¥è¯¢æœªæ‰¾åˆ°ç»“æžœ")


def demo_migration_and_export():
    """æ¼”ç¤ºæ•°æ®è¿ç§»å’Œå¯¼å‡ºåŠŸèƒ½"""
    logger.info("\n=== æ•°æ®è¿ç§»å’Œå¯¼å‡ºæ¼”ç¤º ===")
    
    # æ‰§è¡ŒJSONåˆ°SQLiteçš„è¿ç§»
    logger.info("ðŸ“¦ æ‰§è¡ŒJSONåˆ°SQLiteè¿ç§»...")
    migrated_count = HashCache.migrate_to_sqlite(force_refresh=True)
    logger.info(f"âœ… è¿ç§»äº† {migrated_count} æ¡è®°å½•")
    
    # èŽ·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    logger.info("\nðŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
    stats = HashCache.get_database_statistics()
    
    if memory_stats := stats.get('memory_cache'):
        logger.info(f"  å†…å­˜ç¼“å­˜: {memory_stats['cache_size']} æ¡è®°å½•")
    
    if sqlite_stats := stats.get('sqlite'):
        if 'error' in sqlite_stats:
            logger.error(f"  SQLiteé”™è¯¯: {sqlite_stats['error']}")
        else:
            logger.info(f"  SQLiteè®°å½•: {sqlite_stats.get('total_records', 0)} æ¡")
            logger.info(f"  æ•°æ®åº“å¤§å°: {sqlite_stats.get('db_size_mb', 0)} MB")
            
            # æ˜¾ç¤ºæ ¼å¼åˆ†å¸ƒ
            if by_ext := sqlite_stats.get('by_extension'):
                logger.info("  æ ¼å¼åˆ†å¸ƒ:")
                for ext, count in list(by_ext.items())[:5]:
                    logger.info(f"    {ext}: {count} ä¸ª")
    
    # å¯¼å‡ºåˆ°JSONï¼ˆå…¼å®¹æ€§æµ‹è¯•ï¼‰
    logger.info("\nðŸ’¾ å¯¼å‡ºSQLiteæ•°æ®åˆ°JSON...")
    export_file = "hash_export_demo.json"
    success = HashCache.export_sqlite_to_json(export_file, format_type='new')
    if success:
        logger.info(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {export_file}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if os.path.exists(export_file):
            file_size = os.path.getsize(export_file)
            logger.info(f"  æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                os.remove(export_file)
                logger.info("ðŸ—‘ï¸ å·²æ¸…ç†å¯¼å‡ºçš„æµ‹è¯•æ–‡ä»¶")
            except:
                pass
    else:
        logger.error("âŒ å¯¼å‡ºå¤±è´¥")


def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
    logger.info("\n=== æ€§èƒ½å¯¹æ¯”æ¼”ç¤º ===")
    
    # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
    test_uris = [
        "archive:///test/large.zip!/folder/image001.jpg",
        "archive:///test/large.zip!/folder/image002.png", 
        "/path/to/local/image.jpg",
        "https://example.com/remote/image.webp"
    ]
    
    # æ·»åŠ æµ‹è¯•æ•°æ®
    db = get_db_cached()
    for i, uri in enumerate(test_uris):
        hash_value = f"perf_test_{i:04d}"
        db.add_hash(uri, hash_value)
    
    # SQLiteæŸ¥è¯¢æ€§èƒ½æµ‹è¯•
    logger.info("ðŸš€ SQLiteæŸ¥è¯¢æ€§èƒ½æµ‹è¯•...")
    start_time = time.time()
    
    for _ in range(100):  # 100æ¬¡æŸ¥è¯¢
        for uri in test_uris:
            HashCache.get_hash(uri)
    
    sqlite_time = time.time() - start_time
    logger.info(f"  SQLite: {sqlite_time:.3f}ç§’ (400æ¬¡æŸ¥è¯¢)")
    
    # å†…å­˜ç¼“å­˜æ€§èƒ½æµ‹è¯•ï¼ˆç¦ç”¨SQLiteï¼‰
    logger.info("ðŸ§  å†…å­˜ç¼“å­˜æ€§èƒ½æµ‹è¯•...")
    HashCache.configure_multiprocess(use_sqlite=False)
    
    start_time = time.time()
    
    for _ in range(100):
        for uri in test_uris:
            HashCache.get_hash(uri)
    
    memory_time = time.time() - start_time
    logger.info(f"  å†…å­˜ç¼“å­˜: {memory_time:.3f}ç§’ (400æ¬¡æŸ¥è¯¢)")
    
    # æ¢å¤SQLiteé…ç½®
    HashCache.configure_multiprocess(use_sqlite=True)
    
    # æ€§èƒ½æ¯”è¾ƒ
    if sqlite_time > 0 and memory_time > 0:
        ratio = sqlite_time / memory_time
        logger.info(f"ðŸ“ˆ æ€§èƒ½æ¯”è¾ƒ: SQLite/å†…å­˜ = {ratio:.2f}x")
        
        if ratio < 1.5:
            logger.info("âœ… SQLiteæ€§èƒ½è¡¨çŽ°è‰¯å¥½")
        elif ratio < 3.0:
            logger.info("âš ï¸ SQLiteæ€§èƒ½å¯æŽ¥å—")
        else:
            logger.info("â— SQLiteæ€§èƒ½éœ€è¦ä¼˜åŒ–")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    logger.info("ðŸŽ­ SQLiteå­˜å‚¨é›†æˆæ¼”ç¤ºå¼€å§‹")
    
    try:
        # è®¾ç½®çŽ¯å¢ƒ
        setup_demo_environment()
        
        # åŸºæœ¬åŠŸèƒ½æ¼”ç¤º
        demo_basic_hash_calculation()
        
        # æ™ºèƒ½æŸ¥è¯¢æ¼”ç¤º
        demo_smart_query()
        
        # è¿ç§»å’Œå¯¼å‡ºæ¼”ç¤º
        demo_migration_and_export()
        
        # æ€§èƒ½å¯¹æ¯”æ¼”ç¤º
        demo_performance_comparison()
        
        logger.info("\nðŸŽ‰ SQLiteå­˜å‚¨é›†æˆæ¼”ç¤ºå®Œæˆ!")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        logger.info("\nðŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
        stats = HashCache.get_cache_stats()
        logger.info(f"  å†…å­˜ç¼“å­˜å¤§å°: {stats['cache_size']}")
        logger.info(f"  SQLiteå·²å¯ç”¨: {stats['sqlite_enabled']}")
        logger.info(f"  å¤šè¿›ç¨‹é…ç½®: {stats['multiprocess_config']}")
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
