import time
import numpy as np
import pickle
import json
from pathlib import Path
from PIL import Image
import pillow_avif
import pillow_jxl 
from skimage import filters, feature, measure
from skimage.transform import resize
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn, SpinnerColumn
from rich.console import Console
from rich.table import Table
from rich.prompt import Prompt, IntPrompt, Confirm
import hashlib
import sqlite3

# åˆ›å»ºæ§åˆ¶å°
console = Console()

class StructureHashExtractor:
    """ç»“æ„ç‰¹å¾å“ˆå¸Œæå–å™¨ - ç”Ÿæˆå¯å­˜å‚¨åˆ°æ•°æ®åº“çš„çŸ­ç‰¹å¾"""
    
    def __init__(self, target_size=(128, 128), feature_dim=64):
        self.target_size = target_size
        self.feature_dim = feature_dim
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=feature_dim)
        self.is_fitted = False
        
        console.print(f"[bold green]ç»“æ„å“ˆå¸Œæå–å™¨åˆå§‹åŒ– - å°ºå¯¸: {target_size}, ç‰¹å¾ç»´åº¦: {feature_dim}[/bold green]")
    
    def extract_basic_features(self, img_array):
        """æå–åŸºç¡€ç»Ÿè®¡ç‰¹å¾"""
        features = []
        
        # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features.extend([
            np.mean(img_array),           # å‡å€¼
            np.std(img_array),            # æ ‡å‡†å·®
            np.median(img_array),         # ä¸­ä½æ•°
            np.min(img_array),            # æœ€å°å€¼
            np.max(img_array),            # æœ€å¤§å€¼
        ])
        
        # 2. è¾¹ç¼˜ç‰¹å¾
        edges = filters.sobel(img_array)
        features.extend([
            np.mean(edges),
            np.std(edges),
            np.sum(edges > 0.1),         # è¾¹ç¼˜åƒç´ æ•°é‡
        ])
        
        # 3. çº¹ç†ç‰¹å¾ï¼ˆå±€éƒ¨äºŒå€¼æ¨¡å¼ï¼‰
        lbp = feature.local_binary_pattern(img_array, P=8, R=1, method='uniform')
        lbp_hist, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10))
        features.extend(lbp_hist.tolist())
        
        return np.array(features)
    
    def extract_block_features(self, img_array, block_size=16):
        """æå–åˆ†å—ç‰¹å¾"""
        h, w = img_array.shape
        features = []
        
        # å°†å›¾åƒåˆ†æˆè‹¥å¹²å—
        for i in range(0, h, block_size):
            for j in range(0, w, block_size):
                block = img_array[i:i+block_size, j:j+block_size]
                if block.size > 0:
                    features.extend([
                        np.mean(block),
                        np.std(block),
                        np.median(block)
                    ])
        
        return np.array(features)
    
    def extract_gradient_features(self, img_array):
        """æå–æ¢¯åº¦ç‰¹å¾"""
        # è®¡ç®—æ¢¯åº¦
        grad_x = filters.sobel_h(img_array)
        grad_y = filters.sobel_v(img_array)
        
        # æ¢¯åº¦å¹…å€¼å’Œæ–¹å‘
        magnitude = np.sqrt(grad_x**2 + grad_y**2)
        orientation = np.arctan2(grad_y, grad_x)
        
        features = []
        
        # æ¢¯åº¦ç»Ÿè®¡
        features.extend([
            np.mean(magnitude),
            np.std(magnitude),
            np.percentile(magnitude, 25),
            np.percentile(magnitude, 75),
        ])
        
        # æ–¹å‘ç›´æ–¹å›¾
        orientation_hist, _ = np.histogram(orientation.ravel(), bins=8, range=(-np.pi, np.pi))
        features.extend(orientation_hist.tolist())
        
        return np.array(features)
    
    def extract_all_features(self, img_path):
        """æå–å®Œæ•´ç‰¹å¾å‘é‡"""
        try:
            with Image.open(img_path) as img:
                # è½¬æ¢ä¸ºç°åº¦
                if img.mode != 'L':
                    img = img.convert('L')
                
                # è°ƒæ•´å°ºå¯¸
                img = img.resize(self.target_size, Image.Resampling.LANCZOS)
                img_array = np.array(img, dtype=np.float64) / 255.0
                
                # æå–å„ç§ç‰¹å¾
                basic_features = self.extract_basic_features(img_array)
                block_features = self.extract_block_features(img_array)
                gradient_features = self.extract_gradient_features(img_array)
                
                # åˆå¹¶æ‰€æœ‰ç‰¹å¾
                all_features = np.concatenate([
                    basic_features,
                    block_features,
                    gradient_features
                ])
                
                return all_features
        except Exception as e:
            console.print(f"[red]ç‰¹å¾æå–å¤±è´¥: {img_path}, é”™è¯¯: {e}")
            return None
    
    def fit_transform(self, image_files):
        """è®­ç»ƒPCAå¹¶è½¬æ¢ç‰¹å¾"""
        console.print("[bold]å¼€å§‹æå–ç‰¹å¾å¹¶è®­ç»ƒé™ç»´æ¨¡å‹...[/bold]")
        
        all_features = []
        valid_files = []
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("[bold green]{task.completed}/{task.total}"),
            TimeElapsedColumn(),
        ) as progress:
            task = progress.add_task("[cyan]æå–ç‰¹å¾", total=len(image_files))
            
            for img_file in image_files:
                features = self.extract_all_features(img_file)
                if features is not None:
                    all_features.append(features)
                    valid_files.append(img_file)
                progress.update(task, advance=1)
        
        if not all_features:
            console.print("[red]æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾ï¼")
            return {}, []
          # æ ‡å‡†åŒ–å’Œé™ç»´
        X = np.array(all_features)
        n_samples, n_features = X.shape
        
        # è°ƒæ•´ç‰¹å¾ç»´åº¦ï¼Œä¸èƒ½è¶…è¿‡æ ·æœ¬æ•°å’ŒåŸå§‹ç‰¹å¾æ•°çš„æœ€å°å€¼
        max_components = min(n_samples, n_features)
        actual_feature_dim = min(self.feature_dim, max_components)
        
        console.print(f"[cyan]åŸå§‹ç‰¹å¾ç»´åº¦: {n_features}, æ ·æœ¬æ•°: {n_samples}[/cyan]")
        console.print(f"[cyan]ç›®æ ‡ç»´åº¦: {self.feature_dim} -> å®é™…ç»´åº¦: {actual_feature_dim}[/cyan]")
        
        # æ›´æ–°PCAç»„ä»¶æ•°
        self.pca = PCA(n_components=actual_feature_dim)
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # PCAé™ç»´
        X_reduced = self.pca.fit_transform(X_scaled)
        
        self.is_fitted = True
        
        # ç”Ÿæˆç»“æ„å“ˆå¸Œ
        structure_hashes = {}
        for i, img_file in enumerate(valid_files):
            # å°†ç‰¹å¾å‘é‡è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å“ˆå¸Œ
            hash_vector = X_reduced[i]
            structure_hashes[str(img_file)] = hash_vector
        
        console.print(f"[bold green]æˆåŠŸç”Ÿæˆ {len(structure_hashes)} ä¸ªç»“æ„å“ˆå¸Œ[/bold green]")
        console.print(f"[cyan]PCAè§£é‡Šæ–¹å·®æ¯”: {self.pca.explained_variance_ratio_.sum():.3f}[/cyan]")
        
        return structure_hashes, valid_files
    
    def transform(self, img_path):
        """è½¬æ¢å•å¼ å›¾ç‰‡ä¸ºç»“æ„å“ˆå¸Œ"""
        if not self.is_fitted:
            console.print("[red]æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit_transform!")
            return None
        
        features = self.extract_all_features(img_path)
        if features is None:
            return None
        
        # æ ‡å‡†åŒ–å’Œé™ç»´
        features_scaled = self.scaler.transform([features])
        hash_vector = self.pca.transform(features_scaled)[0]
        
        return hash_vector

class StructureHashDatabase:
    """ç»“æ„å“ˆå¸Œæ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, db_path):
        self.db_path = Path(db_path)
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS structure_hashes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_path TEXT UNIQUE NOT NULL,
                file_name TEXT NOT NULL,
                file_size INTEGER,
                file_mtime REAL,
                hash_vector BLOB NOT NULL,
                feature_dim INTEGER NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        console.print(f"[bold green]æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}[/bold green]")
    
    def save_hashes(self, structure_hashes):
        """ä¿å­˜ç»“æ„å“ˆå¸Œåˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        for file_path, hash_vector in structure_hashes.items():
            try:
                path_obj = Path(file_path)
                if not path_obj.exists():
                    continue
                
                stat = path_obj.stat()
                
                # åºåˆ—åŒ–å“ˆå¸Œå‘é‡
                hash_blob = pickle.dumps(hash_vector.astype(np.float32))
                
                cursor.execute('''
                    INSERT OR REPLACE INTO structure_hashes 
                    (file_path, file_name, file_size, file_mtime, hash_vector, feature_dim)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    str(path_obj),
                    path_obj.name,
                    stat.st_size,
                    stat.st_mtime,
                    hash_blob,
                    len(hash_vector)
                ))
                saved_count += 1
            except Exception as e:
                console.print(f"[red]ä¿å­˜å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        
        conn.commit()
        conn.close()
        console.print(f"[bold green]æˆåŠŸä¿å­˜ {saved_count} ä¸ªç»“æ„å“ˆå¸Œåˆ°æ•°æ®åº“[/bold green]")
    
    def load_hashes(self):
        """ä»æ•°æ®åº“åŠ è½½ç»“æ„å“ˆå¸Œ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT file_path, hash_vector FROM structure_hashes')
        rows = cursor.fetchall()
        
        structure_hashes = {}
        for file_path, hash_blob in rows:
            try:
                hash_vector = pickle.loads(hash_blob)
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
                if Path(file_path).exists():
                    structure_hashes[file_path] = hash_vector
            except Exception as e:
                console.print(f"[yellow]åŠ è½½å“ˆå¸Œå¤±è´¥: {file_path}, é”™è¯¯: {e}")
        
        conn.close()
        console.print(f"[bold green]ä»æ•°æ®åº“åŠ è½½ {len(structure_hashes)} ä¸ªç»“æ„å“ˆå¸Œ[/bold green]")
        return structure_hashes
    
    def get_database_stats(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM structure_hashes')
        total_count = cursor.fetchone()[0]
        
        cursor.execute('SELECT AVG(feature_dim) FROM structure_hashes')
        avg_dim = cursor.fetchone()[0] or 0
        
        cursor.execute('SELECT SUM(LENGTH(hash_vector)) FROM structure_hashes')
        total_size = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_count': total_count,
            'avg_dimension': int(avg_dim),
            'total_size_bytes': total_size,
            'avg_size_per_hash': total_size / total_count if total_count > 0 else 0
        }

def calculate_structure_similarity(hash1, hash2, method='cosine'):
    """è®¡ç®—ç»“æ„å“ˆå¸Œç›¸ä¼¼åº¦"""
    if method == 'cosine':
        # ä½™å¼¦ç›¸ä¼¼åº¦ (0-1, 1æœ€ç›¸ä¼¼)
        similarity = cosine_similarity([hash1], [hash2])[0][0]
        return similarity
    elif method == 'euclidean':
        # æ¬§æ°è·ç¦» (è¶Šå°è¶Šç›¸ä¼¼)
        distance = euclidean_distances([hash1], [hash2])[0][0]
        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ (0-1, 1æœ€ç›¸ä¼¼)
        max_dist = np.sqrt(len(hash1)) * 2  # ä¼°è®¡æœ€å¤§è·ç¦»
        similarity = max(0, 1 - distance / max_dist)
        return similarity
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•: {method}")

def find_similar_pairs_by_hash(structure_hashes, top_n=10, similarity_method='cosine'):
    """åŸºäºç»“æ„å“ˆå¸ŒæŸ¥æ‰¾ç›¸ä¼¼å›¾ç‰‡å¯¹"""
    console.print(f"[bold]ä½¿ç”¨ç»“æ„å“ˆå¸Œè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ–¹æ³•: {similarity_method}ï¼‰...[/bold]")
    
    items = list(structure_hashes.items())
    all_pairs = []
    total_pairs = len(items) * (len(items) - 1) // 2
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[bold green]{task.completed}/{task.total}"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
    ) as progress:
        task = progress.add_task("[cyan]è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦", total=total_pairs)
        
        for i, (img1, hash1) in enumerate(items):
            for j in range(i + 1, len(items)):
                img2, hash2 = items[j]
                
                similarity = calculate_structure_similarity(hash1, hash2, similarity_method)
                all_pairs.append((img1, img2, similarity))
                progress.update(task, advance=1)
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    all_pairs.sort(key=lambda x: x[2], reverse=True)
    return all_pairs[:top_n], all_pairs

def generate_structure_hash_report(top_pairs, all_pairs, output_path, stats=None):
    """ç”Ÿæˆç»“æ„å“ˆå¸ŒæŠ¥å‘Š"""
    console.print("[bold]æ­£åœ¨ç”Ÿæˆç»“æ„å“ˆå¸ŒHTMLæŠ¥å‘Š...[/bold]")
    
    html = [
        "<!DOCTYPE html><html><head><meta charset='utf-8'>",
        "<title>ç»“æ„å“ˆå¸Œç›¸ä¼¼åº¦æŠ¥å‘Š</title>",
        "<style>",
        "body{font-family:Arial,sans-serif;margin:30px;background:#f8f9fa;}",
        ".container{max-width:1200px;margin:0 auto;background:white;padding:20px;border-radius:10px;box-shadow:0 2px 10px rgba(0,0,0,0.1);}",
        "h1{color:#2c3e50;text-align:center;margin-bottom:30px;}",
        ".stats{background:#e8f4fd;padding:15px;border-radius:8px;margin:20px 0;}",
        ".hash-stats{background:#f0f9ff;padding:15px;border-radius:8px;margin:20px 0;border-left:4px solid #0ea5e9;}",
        ".pair-grid{display:grid;gap:20px;margin:20px 0;}",
        ".pair-item{border:1px solid #ddd;border-radius:8px;padding:15px;background:#fff;}",
        ".image-container{display:flex;align-items:center;gap:20px;}",
        ".image-box{text-align:center;flex:1;}",
        "img{max-width:200px;max-height:200px;border-radius:5px;box-shadow:0 2px 5px rgba(0,0,0,0.2);}",
        ".filename{font-size:12px;color:#666;margin-top:5px;word-break:break-all;}",
        ".similarity-value{font-size:24px;font-weight:bold;text-align:center;flex:0 0 120px;}",
        ".high{color:#27ae60;} .medium{color:#f39c12;} .low{color:#e74c3c;}",
        "</style>",
        "</head><body>",
        "<div class='container'>",
        "<h1>ğŸ”— ç»“æ„å“ˆå¸Œç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š</h1>",
    ]
    
    # ç»“æ„å“ˆå¸Œç»Ÿè®¡
    if stats:
        html.append("<div class='hash-stats'>")
        html.append("<h3>ğŸ”¢ ç»“æ„å“ˆå¸Œç»Ÿè®¡</h3>")
        html.append(f"<p><strong>å“ˆå¸Œæ€»æ•°:</strong> {stats.get('total_count', 0)}</p>")
        html.append(f"<p><strong>å¹³å‡ç»´åº¦:</strong> {stats.get('avg_dimension', 0)}</p>")
        html.append(f"<p><strong>å­˜å‚¨å¤§å°:</strong> {stats.get('total_size_bytes', 0) / 1024:.1f} KB</p>")
        html.append(f"<p><strong>å¹³å‡æ¯ä¸ªå“ˆå¸Œ:</strong> {stats.get('avg_size_per_hash', 0):.1f} å­—èŠ‚</p>")
        html.append("</div>")
    
    # ç›¸ä¼¼åº¦ç»Ÿè®¡
    if all_pairs:
        similarities = [p[2] for p in all_pairs]
        avg_sim = sum(similarities) / len(similarities)
        max_sim = max(similarities)
        min_sim = min(similarities)
        
        html.append("<div class='stats'>")
        html.append("<h3>ğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡</h3>")
        html.append(f"<p><strong>å¯¹æ¯”æ€»æ•°:</strong> {len(all_pairs)}</p>")
        html.append(f"<p><strong>å¹³å‡ç›¸ä¼¼åº¦:</strong> {avg_sim:.4f}</p>")
        html.append(f"<p><strong>æœ€å¤§ç›¸ä¼¼åº¦:</strong> {max_sim:.4f}</p>")
        html.append(f"<p><strong>æœ€å°ç›¸ä¼¼åº¦:</strong> {min_sim:.4f}</p>")
        html.append("</div>")
    
    # æœ€ç›¸ä¼¼å›¾ç‰‡å¯¹
    html.append("<h2>ğŸ† æœ€ç›¸ä¼¼çš„å›¾ç‰‡å¯¹</h2>")
    html.append("<div class='pair-grid'>")
    
    for i, (img1, img2, similarity) in enumerate(top_pairs, 1):
        img1_path = Path(img1)
        img2_path = Path(img2)
        
        # æ ¹æ®ç›¸ä¼¼åº¦è®¾ç½®é¢œè‰²
        sim_class = "high" if similarity >= 0.8 else "medium" if similarity >= 0.6 else "low"
        
        html.append(f"<div class='pair-item'>")
        html.append(f"<h4>ç¬¬ {i} å¯¹ (ç›¸ä¼¼åº¦: {similarity:.4f})</h4>")
        html.append(f"<div class='image-container'>")
        html.append(f"<div class='image-box'>")
        html.append(f"<img src='file:///{img1_path.resolve()}' alt='å›¾ç‰‡1'>")
        html.append(f"<div class='filename'>{img1_path.name}</div>")
        html.append(f"</div>")
        html.append(f"<div class='similarity-value {sim_class}'>{similarity:.4f}</div>")
        html.append(f"<div class='image-box'>")
        html.append(f"<img src='file:///{img2_path.resolve()}' alt='å›¾ç‰‡2'>")
        html.append(f"<div class='filename'>{img2_path.name}</div>")
        html.append(f"</div>")
        html.append(f"</div>")
        html.append(f"</div>")
    
    html.append("</div>")
    html.append("</div></body></html>")
    
    Path(output_path).write_text('\n'.join(html), encoding="utf-8")
    console.print(f"[bold green]æŠ¥å‘Šå·²ä¿å­˜: {Path(output_path).resolve()}[/bold green]")

def main():
    console.print("[bold blue]ğŸ”— ç»“æ„å“ˆå¸Œå›¾ç‰‡ç›¸ä¼¼åº¦åˆ†æå·¥å…·[/bold blue]")
    console.print("[yellow]åŸºäºå›¾åƒç»“æ„ç‰¹å¾ç”ŸæˆçŸ­å“ˆå¸Œï¼Œæ”¯æŒæ•°æ®åº“å­˜å‚¨[/yellow]\n")
    
    # è¾“å…¥å‚æ•°
    folder = Prompt.ask("ğŸ“ è¯·è¾“å…¥å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„", default="E:\\2EHV\\test")
    
    # ç‰¹å¾å‚æ•°
    feature_dim = IntPrompt.ask("ğŸ”¢ ç»“æ„å“ˆå¸Œç»´åº¦ï¼ˆæ¨è32-128ï¼‰", default=64)
    
    # æ•°æ®åº“é€‰é¡¹
    use_database = Confirm.ask("ğŸ’¾ æ˜¯å¦ä½¿ç”¨æ•°æ®åº“å­˜å‚¨å“ˆå¸Œï¼Ÿ", default=True)
    db_path = None
    if use_database:
        db_path = Path(folder) / "structure_hashes.db"
    
    # å›¾ç‰‡é™åˆ¶
    max_images = IntPrompt.ask("ğŸ”¢ æœ€å¤§å¤„ç†å›¾ç‰‡æ•°é‡ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰", default=50)
    top_n = IntPrompt.ask("ğŸ† æ˜¾ç¤ºæœ€ç›¸ä¼¼çš„å‰Nå¯¹", default=15)
    
    # ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
    similarity_method = Prompt.ask("ğŸ“ ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•", 
                                 choices=["cosine", "euclidean"], 
                                 default="cosine")
    
    # æŸ¥æ‰¾å›¾ç‰‡
    folder_path = Path(folder)
    image_extensions = [".jpg", ".jpeg", ".png", ".webp", ".bmp", ".gif", ".tiff", ".jxl", ".avif"]
    
    console.print("\n[bold]ğŸ” æœç´¢å›¾ç‰‡æ–‡ä»¶...[/bold]")
    image_files = []
    for ext in image_extensions:
        image_files.extend(list(folder_path.rglob(f"*{ext}")))
        image_files.extend(list(folder_path.rglob(f"*{ext.upper()}")))
    
    console.print(f"[green]ğŸ“· æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡[/green]")
    
    # é™åˆ¶å›¾ç‰‡æ•°é‡
    if max_images > 0 and len(image_files) > max_images:
        console.print(f"[yellow]âš¡ éšæœºé€‰æ‹© {max_images} å¼ å›¾ç‰‡è¿›è¡Œåˆ†æ[/yellow]")
        import random
        image_files = random.sample(image_files, max_images)
    
    if len(image_files) < 2:
        console.print("[red]âŒ å›¾ç‰‡æ•°é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2å¼ å›¾ç‰‡[/red]")
        return
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db_manager = None
    if use_database:
        db_manager = StructureHashDatabase(db_path)
    
    # å°è¯•åŠ è½½å·²æœ‰å“ˆå¸Œ
    existing_hashes = {}
    if db_manager:
        existing_hashes = db_manager.load_hashes()
    
    # æå–ç‰¹å¾
    start_time = time.time()
    extractor = StructureHashExtractor(feature_dim=feature_dim)
    
    if existing_hashes:
        console.print(f"[cyan]å‘ç° {len(existing_hashes)} ä¸ªå·²æœ‰å“ˆå¸Œï¼Œå°†å¢é‡å¤„ç†[/cyan]")
        # TODO: å¢é‡å¤„ç†é€»è¾‘ï¼ˆæ–°å›¾ç‰‡æå–ç‰¹å¾ï¼Œå·²æœ‰å›¾ç‰‡å¤ç”¨ï¼‰
        structure_hashes, valid_files = extractor.fit_transform(image_files)
    else:
        structure_hashes, valid_files = extractor.fit_transform(image_files)
    
    elapsed_time = time.time() - start_time
    console.print(f"\n[bold green]âœ… ç‰¹å¾æå–å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f} ç§’[/bold green]")
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    if db_manager and structure_hashes:
        db_manager.save_hashes(structure_hashes)
        db_stats = db_manager.get_database_stats()
    else:
        db_stats = None
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    console.print("\n[bold]ğŸ” å¼€å§‹è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦...[/bold]")
    top_pairs, all_pairs = find_similar_pairs_by_hash(
        structure_hashes, top_n, similarity_method
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if db_stats:
        stats_table = Table(title="ğŸ’¾ æ•°æ®åº“ç»Ÿè®¡")
        stats_table.add_column("æŒ‡æ ‡", style="cyan")
        stats_table.add_column("å€¼", style="green", justify="right")
        
        stats_table.add_row("å“ˆå¸Œæ€»æ•°", str(db_stats['total_count']))
        stats_table.add_row("å¹³å‡ç»´åº¦", str(db_stats['avg_dimension']))
        stats_table.add_row("å­˜å‚¨å¤§å°", f"{db_stats['total_size_bytes'] / 1024:.1f} KB")
        stats_table.add_row("æ¯å“ˆå¸Œå¤§å°", f"{db_stats['avg_size_per_hash']:.1f} å­—èŠ‚")
        
        console.print(stats_table)
    
    # æ˜¾ç¤ºç»“æœ
    if top_pairs:
        result_table = Table(title="ğŸ† æœ€ç›¸ä¼¼çš„å›¾ç‰‡å¯¹")
        result_table.add_column("æ’å", style="cyan", justify="center")
        result_table.add_column("å›¾ç‰‡1", style="blue")
        result_table.add_column("å›¾ç‰‡2", style="blue")
        result_table.add_column("ç›¸ä¼¼åº¦", style="green", justify="center")
        result_table.add_column("ç›¸ä¼¼ç¨‹åº¦", style="yellow")
        
        for i, (img1, img2, similarity) in enumerate(top_pairs, 1):
            img1_name = Path(img1).name
            img2_name = Path(img2).name
            level = "å¾ˆé«˜" if similarity >= 0.9 else "é«˜" if similarity >= 0.8 else "ä¸­ç­‰" if similarity >= 0.6 else "ä½"
            result_table.add_row(str(i), img1_name, img2_name, f"{similarity:.4f}", level)
        
        console.print(result_table)
    
    # ç”ŸæˆæŠ¥å‘Š
    output_path = folder_path / "structure_hash_report.html"
    generate_structure_hash_report(top_pairs, all_pairs, output_path, db_stats)
    
    console.print(f"\n[bold blue]ğŸ‰ åˆ†æå®Œæˆï¼[/bold blue]")
    console.print(f"[green]ğŸ“„ HTMLæŠ¥å‘Š: {output_path}[/green]")
    if use_database:
        console.print(f"[green]ğŸ’¾ æ•°æ®åº“: {db_path}[/green]")
        console.print(f"[cyan]ğŸ’¡ ä¸‹æ¬¡è¿è¡Œå¯ç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­çš„å“ˆå¸Œï¼Œå¤§å¹…æå‡é€Ÿåº¦[/cyan]")

if __name__ == "__main__":
    main()
