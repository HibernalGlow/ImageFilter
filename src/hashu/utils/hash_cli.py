"""
å“ˆå¸Œè®¡ç®—å‘½ä»¤è¡Œå·¥å…·
"""
import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
import time

from hashu import calculate_hash_for_artist_folder, process_duplicates_with_hash_file
from loguru import logger

def setup_logger():
    """é…ç½®æ—¥å¿—è¾“å‡º"""
    logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    logger.add(
        sys.stdout,
        level="INFO",
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>"
    )

def main():
    """å‘½ä»¤è¡Œå·¥å…·ä¸»å‡½æ•°"""
    setup_logger()
    
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="å“ˆå¸Œè®¡ç®—å‘½ä»¤è¡Œå·¥å…·")
    
    # æ·»åŠ å‚æ•°
    parser.add_argument("--path", "-p", type=str, required=True, help="è¦å¤„ç†çš„ç”»å¸ˆæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--workers", "-w", type=int, default=4, help="å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4")
    parser.add_argument("--force", "-f", action="store_true", help="æ˜¯å¦å¼ºåˆ¶æ›´æ–°å“ˆå¸Œå€¼")
    parser.add_argument("--duplicate-check", "-d", action="store_true", help="æ˜¯å¦è¿›è¡Œé‡å¤æ£€æµ‹")
    parser.add_argument("--target", "-t", type=str, help="è¿›è¡Œé‡å¤æ£€æµ‹çš„ç›®æ ‡è·¯å¾„ï¼Œå¯ä¸--pathç›¸åŒ")
    parser.add_argument("--hamming", type=int, default=16, help="æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤ä¸º16")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # ç¡®ä¿æ–‡ä»¶å¤¹è·¯å¾„å­˜åœ¨
    folder_path = Path(args.path)
    if not folder_path.exists():
        logger.error(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
        return 1
        
    # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ç”»å¸ˆæ–‡ä»¶å¤¹: {folder_path}")
    logger.info(f"âš™ï¸ å·¥ä½œçº¿ç¨‹æ•°: {args.workers}")
    logger.info(f"âš™ï¸ å¼ºåˆ¶æ›´æ–°: {args.force}")
    
    # è®¡æ—¶å¼€å§‹
    start_time = time.time()
    
    # è®¡ç®—å“ˆå¸Œå€¼
    hash_file_path = calculate_hash_for_artist_folder(
        folder_path, 
        workers=args.workers, 
        force_update=args.force
    )
    
    # æ£€æŸ¥å“ˆå¸Œè®¡ç®—ç»“æœ
    if not hash_file_path:
        logger.error("âŒ å“ˆå¸Œè®¡ç®—å¤±è´¥")
        return 1
        
    # è®¡ç®—è€—æ—¶
    elapsed = time.time() - start_time
    logger.info(f"âœ… å“ˆå¸Œè®¡ç®—å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    logger.info(f"ğŸ“„ å“ˆå¸Œæ–‡ä»¶: {hash_file_path}")
    
    # å¦‚æœéœ€è¦è¿›è¡Œé‡å¤æ£€æµ‹
    if args.duplicate_check:
        target_path = args.target or args.path
        logger.info(f"ğŸ” å¼€å§‹è¿›è¡Œé‡å¤æ£€æµ‹ï¼Œç›®æ ‡è·¯å¾„: {target_path}")
        
        # å‡†å¤‡å‚æ•°
        params = {
            'ref_hamming_distance': args.hamming,
            'hash_size': 10,
            'filter_white_enabled': False
        }
        
        # è¿›è¡Œé‡å¤æ£€æµ‹
        duplicate_start_time = time.time()
        process_duplicates_with_hash_file(
            hash_file_path,
            [target_path],
            params,
            args.workers
        )
        
        # è®¡ç®—é‡å¤æ£€æµ‹è€—æ—¶
        duplicate_elapsed = time.time() - duplicate_start_time
        logger.info(f"âœ… é‡å¤æ£€æµ‹å®Œæˆï¼Œè€—æ—¶: {duplicate_elapsed:.2f}ç§’")
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 