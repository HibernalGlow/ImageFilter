
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import List

# å¯¼å…¥ä¼˜åŒ–å·¥å…·
from hashu.utils.hash_process_config import setup_multiprocess_hash_environment
from hashu.core.calculate_hash_custom import ImageHashCalculator, HashCache
from hashu.log import logger



def calculate_hash_worker(image_path: str) -> dict:
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šè®¡ç®—å•ä¸ªå›¾ç‰‡çš„å“ˆå¸Œå€¼
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        
    Returns:
        dict: åŒ…å«è·¯å¾„å’Œå“ˆå¸Œç»“æœçš„å­—å…¸
    """
    try:
        # åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨é¢„åŠ è½½ç¼“å­˜
        result = ImageHashCalculator.calculate_phash(
            image_path, 
            auto_save=False,  # å¤šè¿›ç¨‹ä¸‹å…³é—­è‡ªåŠ¨ä¿å­˜
            use_preload=True  # ä½¿ç”¨é¢„åŠ è½½ç¼“å­˜
        )
        
        return {
            'path': image_path,
            'result': result,
            'success': True
        }
    except Exception as e:
        return {
            'path': image_path,
            'result': None,
            'success': False,
            'error': str(e)
        }


def batch_calculate_hashes_multiprocess(image_paths: List[str], 
                                       max_workers: int = 4) -> List[dict]:
    """å¤šè¿›ç¨‹æ‰¹é‡è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼
    
    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        
    Returns:
        List[dict]: è®¡ç®—ç»“æœåˆ—è¡¨
    """
    logger.info(f"ğŸš€ å¼€å§‹å¤šè¿›ç¨‹å“ˆå¸Œè®¡ç®—ï¼Œå…± {len(image_paths)} ä¸ªæ–‡ä»¶ï¼Œ{max_workers} ä¸ªè¿›ç¨‹")
    
    # é…ç½®å¤šè¿›ç¨‹ç¯å¢ƒ
    setup_multiprocess_hash_environment(
        enable_auto_save=False,  # å…³é—­è‡ªåŠ¨ä¿å­˜ï¼Œé¿å…æ–‡ä»¶å†™å…¥å†²çª
        enable_global_cache=True,  # å¯ç”¨å…¨å±€ç¼“å­˜æŸ¥è¯¢
        preload_cache_from_files=True  # é¢„åŠ è½½ç¼“å­˜æ–‡ä»¶
    )
    
    results = []
    start_time = time.time()
    
    try:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_path = {
                executor.submit(calculate_hash_worker, path): path 
                for path in image_paths
            }
            
            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_path):
                try:
                    result = future.result()
                    results.append(result)
                    completed_count += 1
                    
                    if completed_count % 10 == 0:  # æ¯10ä¸ªæ–‡ä»¶è¾“å‡ºä¸€æ¬¡è¿›åº¦
                        logger.info(f"ğŸ“Š è¿›åº¦: {completed_count}/{len(image_paths)} "
                                  f"({completed_count/len(image_paths)*100:.1f}%)")
                        
                except Exception as e:
                    path = future_to_path[future]
                    logger.error(f"âŒ å¤„ç†å¤±è´¥ {path}: {e}")
                    results.append({
                        'path': path,
                        'result': None,
                        'success': False,
                        'error': str(e)
                    })
                    
    except Exception as e:
        logger.error(f"âŒ å¤šè¿›ç¨‹æ‰§è¡Œå¤±è´¥: {e}")
        return []
    
    end_time = time.time()
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for r in results if r['success'])
    failed = len(results) - successful
    
    logger.info(f"âœ… å¤šè¿›ç¨‹å“ˆå¸Œè®¡ç®—å®Œæˆ!")
    logger.info(f"ğŸ“Š æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    logger.info(f"ğŸ“Š æˆåŠŸ: {successful}, å¤±è´¥: {failed}")
    logger.info(f"ğŸ“Š å¹³å‡é€Ÿåº¦: {len(image_paths)/(end_time - start_time):.2f} æ–‡ä»¶/ç§’")
    
    return results


def compare_single_vs_multiprocess(image_dir: str, max_workers: int = 4) -> None:
    """æ¯”è¾ƒå•è¿›ç¨‹å’Œå¤šè¿›ç¨‹çš„æ€§èƒ½å·®å¼‚
    
    Args:
        image_dir: å›¾ç‰‡ç›®å½•
        max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    """
    from hashu.core.calculate_hash_custom import ImgUtils
    
    # è·å–å›¾ç‰‡æ–‡ä»¶
    image_files = ImgUtils.get_img_files(image_dir)
    if not image_files:
        logger.warning(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {image_dir}")
        return
    
    # é™åˆ¶æµ‹è¯•æ–‡ä»¶æ•°é‡ï¼ˆé¿å…æµ‹è¯•æ—¶é—´è¿‡é•¿ï¼‰
    test_files = image_files[:50] if len(image_files) > 50 else image_files
    logger.info(f"ğŸ§ª æ€§èƒ½æµ‹è¯•å¼€å§‹ï¼Œä½¿ç”¨ {len(test_files)} ä¸ªæ–‡ä»¶")
    
    # å•è¿›ç¨‹æµ‹è¯•
    logger.info("\n=== å•è¿›ç¨‹æµ‹è¯• ===")
    start_time = time.time()
    single_results = []
    
    for i, img_path in enumerate(test_files):
        try:
            result = ImageHashCalculator.calculate_phash(img_path, auto_save=False)
            single_results.append({'path': img_path, 'result': result, 'success': True})
            
            if (i + 1) % 10 == 0:
                logger.info(f"ğŸ“Š å•è¿›ç¨‹è¿›åº¦: {i+1}/{len(test_files)}")
                
        except Exception as e:
            single_results.append({'path': img_path, 'result': None, 'success': False, 'error': str(e)})
    
    single_time = time.time() - start_time
    single_success = sum(1 for r in single_results if r['success'])
    
    # å¤šè¿›ç¨‹æµ‹è¯•
    logger.info("\n=== å¤šè¿›ç¨‹æµ‹è¯• ===")
    multi_results = batch_calculate_hashes_multiprocess(test_files, max_workers)
    multi_time = time.time() - start_time - single_time
    multi_success = sum(1 for r in multi_results if r['success'])
    
    # æ€§èƒ½æ¯”è¾ƒ
    logger.info("\n=== æ€§èƒ½æ¯”è¾ƒ ===")
    logger.info(f"ğŸ“Š å•è¿›ç¨‹: {single_time:.2f}ç§’, æˆåŠŸ: {single_success}/{len(test_files)}")
    logger.info(f"ğŸ“Š å¤šè¿›ç¨‹: {multi_time:.2f}ç§’, æˆåŠŸ: {multi_success}/{len(test_files)}")
    
    if multi_time > 0:
        speedup = single_time / multi_time
        logger.info(f"ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        logger.info(f"ğŸ“ˆ æ•ˆç‡æå‡: {(speedup-1)*100:.1f}%")


if __name__ == "__main__":
    # æµ‹è¯•å¤šè¿›ç¨‹ä¼˜åŒ–åŠŸèƒ½
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•å¤šè¿›ç¨‹å“ˆå¸Œè®¡ç®—ä¼˜åŒ–åŠŸèƒ½")
    
    # é…ç½®å¤šè¿›ç¨‹ç¯å¢ƒ
    logger.info("âš™ï¸ é…ç½®å¤šè¿›ç¨‹ç¯å¢ƒ...")
    setup_multiprocess_hash_environment(
        enable_auto_save=False,
        enable_global_cache=True, 
        preload_cache_from_files=True
    )
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = HashCache.get_cache_stats()
    logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: å¤§å°={stats['cache_size']}, å·²åˆå§‹åŒ–={stats['initialized']}")
    # logger.info(f"ğŸ“Š å¤šè¿›ç¨‹é…ç½®: {stats['multiprocess_config']}")
    
    # å°è¯•æŸ¥æ‰¾ä¸€äº›æµ‹è¯•å›¾ç‰‡
    test_dirs = [
        r"E:\2EHV\test",
        r"D:\1VSCODE\Projects\ImageAll\ImageFilter\test_images",
        ".",  # å½“å‰ç›®å½•
    ]
    
    found_images = []
    for test_dir in test_dirs:
        if Path(test_dir).exists():
            from hashu.core.calculate_hash_custom import ImgUtils
            images = ImgUtils.get_img_files(test_dir)
            if images:
                found_images.extend(images[:5])  # æœ€å¤šå–5ä¸ªæ–‡ä»¶
                logger.info(f"âœ… åœ¨ {test_dir} æ‰¾åˆ° {len(images)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
                break
    
    if found_images:
        logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯• {len(found_images)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
        
        # æµ‹è¯•å•ä¸ªæ–‡ä»¶è®¡ç®—
        test_file = found_images[0]
        logger.info(f"ğŸ“ æµ‹è¯•å•ä¸ªæ–‡ä»¶: {test_file}")
        
        result = ImageHashCalculator.calculate_phash(
            test_file, 
            auto_save=False,
            use_preload=True
        )
        
        if result:
            logger.info(f"âœ… å“ˆå¸Œè®¡ç®—æˆåŠŸ: {result['hash']}")
            logger.info(f"ğŸ“Š ç¼“å­˜å‘½ä¸­: {'æ˜¯' if result.get('from_cache') else 'å¦'}")
        else:
            logger.error("âŒ å“ˆå¸Œè®¡ç®—å¤±è´¥")
        
        # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œæµ‹è¯•æ‰¹é‡å¤„ç†
        if len(found_images) > 1:
            logger.info(f"ğŸ”„ æµ‹è¯•æ‰¹é‡å¤„ç† {len(found_images)} ä¸ªæ–‡ä»¶")
            batch_results = batch_calculate_hashes_multiprocess(found_images, max_workers=2)
            
            successful = sum(1 for r in batch_results if r['success'])
            logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœ: æˆåŠŸ {successful}/{len(found_images)}")
            
    else:
        logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡æ–‡ä»¶")
        logger.info("ğŸ’¡ å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®æ”¾ç½®æµ‹è¯•å›¾ç‰‡:")
        for test_dir in test_dirs:
            logger.info(f"   - {test_dir}")
        
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹é…ç½®
        logger.info("\nğŸ“ å¤šè¿›ç¨‹ä¼˜åŒ–ä½¿ç”¨ç¤ºä¾‹:")
        logger.info("```python")
        logger.info("from hashu.utils.hash_process_config import setup_multiprocess_hash_environment")
        logger.info("from hashu.core.calculate_hash_custom import ImageHashCalculator")
        logger.info("")
        logger.info("# é…ç½®å¤šè¿›ç¨‹ç¯å¢ƒ")
        logger.info("setup_multiprocess_hash_environment(")
        logger.info("    enable_auto_save=False,  # å¤šè¿›ç¨‹ä¸‹å…³é—­è‡ªåŠ¨ä¿å­˜")
        logger.info("    enable_global_cache=True,  # å¯ç”¨å…¨å±€ç¼“å­˜")
        logger.info("    preload_cache_from_files=True  # é¢„åŠ è½½ç¼“å­˜")
        logger.info(")")
        logger.info("")
        logger.info("# è®¡ç®—å“ˆå¸Œå€¼")
        logger.info("result = ImageHashCalculator.calculate_phash(")
        logger.info("    'image.jpg',")
        logger.info("    auto_save=False,  # å¤šè¿›ç¨‹ä¸‹å…³é—­è‡ªåŠ¨ä¿å­˜")
        logger.info("    use_preload=True  # ä½¿ç”¨é¢„åŠ è½½ç¼“å­˜")
        logger.info(")")
        logger.info("```")
        
    logger.info("âœ… å¤šè¿›ç¨‹ä¼˜åŒ–æµ‹è¯•å®Œæˆ")
