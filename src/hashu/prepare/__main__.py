import os
import argparse
import logging
import loguru
import yaml

import numpy as np
import cv2
from PIL import Image
import pillow_jxl
import pillow_avif
import tempfile
import subprocess
import logging
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor
import threading
from typing import Dict, Tuple, Union, List, Optional
from dataclasses import dataclass
from urllib.parse import quote, unquote, urlparse, ParseResult
import re
import json
import zipfile
import io
import sys
from datetime import datetime
import warnings  # æ–°å¢ï¼šå¯¼å…¥warningsæ¨¡å—
from PIL.Image import DecompressionBombWarning  # æ–°å¢ï¼šå¯¼å…¥PILçš„è­¦å‘Šç±»
import orjson
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from textual_preset import create_config_app
from hashu.core.calculate_hash_custom import ImageHashCalculator
from textual_logger import TextualLoggerManager
from loguru import logger
import os
import sys
from pathlib import Path
from datetime import datetime

def setup_logger(app_name="app", project_root=None, console_output=True):
    """é…ç½® Loguru æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        app_name: åº”ç”¨åç§°ï¼Œç”¨äºæ—¥å¿—ç›®å½•
        project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        console_output: æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns:
        tuple: (logger, config_info)
            - logger: é…ç½®å¥½çš„ logger å®ä¾‹
            - config_info: åŒ…å«æ—¥å¿—é…ç½®ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    if project_root is None:
        project_root = Path(__file__).parent.resolve()
    
    # æ¸…é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æœ‰æ¡ä»¶åœ°æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆç®€æ´ç‰ˆæ ¼å¼ï¼‰
    if console_output:
        logger.add(
            sys.stdout,
            level="INFO",
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <blue>{elapsed}</blue> | <level>{level.icon} {level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>"
        )
    
    # ä½¿ç”¨ datetime æ„å»ºæ—¥å¿—è·¯å¾„
    current_time = datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    hour_str = current_time.strftime("%H")
    minute_str = current_time.strftime("%M%S")
    
    # æ„å»ºæ—¥å¿—ç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    log_dir = os.path.join(project_root, "logs", app_name, date_str, hour_str)
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{minute_str}.log")
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        log_file,
        level="DEBUG",
        rotation="10 MB",
        retention="30 days",
        compression="zip",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {elapsed} | {level.icon} {level: <8} | {name}:{function}:{line} - {message}",
        enqueue=True,     )
    
    # åˆ›å»ºé…ç½®ä¿¡æ¯å­—å…¸
    config_info = {
        'log_file': log_file,
    }
    
    logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œåº”ç”¨åç§°: {app_name}")
    return logger, config_info

logger, config_info = setup_logger(app_name="hashpre", console_output=False)

# åœ¨å…¨å±€é…ç½®éƒ¨åˆ†æ·»åŠ ä»¥ä¸‹å†…å®¹
# ================= æ—¥å¿—é…ç½® =================

# æ·»åŠ é»˜è®¤é…ç½®å¸¸é‡
DEFAULT_PROCESS_CONFIG = {
    'max_workers': 16,        # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    'force_update': False,   # æ˜¯å¦å¼ºåˆ¶æ›´æ–°
    'dry_run': False,        # æ˜¯å¦ä»…é¢„è§ˆ
    'extract_dir': r"E:\2400EHV\extracted_archives",  # è§£å‹ç›®å½•
    'hash_size': 10,        # å“ˆå¸Œå¤§å°
    'hash_version': 1       # å“ˆå¸Œç‰ˆæœ¬
}
# å“ˆå¸Œè®¡ç®—å‚æ•°
params = {
    'hash_size': 10,  # é»˜è®¤å“ˆå¸Œå¤§å°
    'hash_version': 1  # å“ˆå¸Œç‰ˆæœ¬å·ï¼Œç”¨äºåç»­å…¼å®¹æ€§å¤„ç†
}

# ç¦ç”¨PILçš„DecompressionBombWarningè­¦å‘Š
warnings.filterwarnings("ignore", category=DecompressionBombWarning)
# è®¾ç½®PILçš„æ—¥å¿—çº§åˆ«ä¸ºERROR
logging.getLogger("PIL").setLevel(logging.ERROR)
# å…¨å±€é…ç½®
GLOBAL_HASH_CACHE = os.path.expanduser(r"E:/1BACKUP/ehv/config/image_hashes_global.json")
HASH_FILES_LIST = os.path.expanduser(r"E:/1BACKUP/ehv/config/hash_files_list.txt")  # æ–°å¢ï¼šä¿å­˜æ‰€æœ‰å“ˆå¸Œæ–‡ä»¶è·¯å¾„çš„æ–‡ä»¶

# é€€å‡ºç å®šä¹‰
EXIT_SUCCESS = 0  # æˆåŠŸå®Œæˆ
EXIT_NO_FILES = 1  # æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶
EXIT_PATH_NOT_EXIST = 2  # è¾“å…¥è·¯å¾„ä¸å­˜åœ¨
EXIT_PROCESS_ERROR = 3  # å¤„ç†è¿‡ç¨‹å‡ºé”™

# å¿…é¡»æ˜¾å¼å®šä¹‰å®Œæ•´å¸ƒå±€é…ç½®
FULL_LAYOUT_CONFIG = {
    "current_stats": {
        "ratio": 2, 
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightblue"
    },
    # "current_progress": {
    #     "ratio": 2,
    #     "title": "ğŸ”„ å½“å‰è¿›åº¦",
    #     "style": "lightgreen"
    # },
    "hash_progress": {  # æ–°å¢å“ˆå¸Œè¿›åº¦é¢æ¿
        "ratio": 2,
        "title": "â³ å“ˆå¸Œè¿›åº¦",
        "style": "lightcyan"
    },
    "hash_calc": {  # æ–°å¢å“ˆå¸Œè®¡ç®—é¢æ¿
        "ratio": 3,
        "title": "ğŸ§® å“ˆå¸Œè®¡ç®—",
        "style": "lightyellow"
    },

    "file_ops": {  # æ–°å¢æ–‡ä»¶æ“ä½œé¢æ¿
        "ratio": 2,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightpink"
    }
}

# æ·»åŠ é»‘åå•å…³é”®è¯å¸¸é‡
BLACKLIST_KEYWORDS = ['temp', 'trash', 'ç”»é›†', 'å›¾é›†', 'cg', '00ä¸éœ€è¦', 'åŠ¨ç”»']

@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœçš„æ•°æ®ç±»"""
    uri: str  # æ ‡å‡†åŒ–çš„URI
    hash_value: dict  # å›¾ç‰‡å“ˆå¸Œå€¼ï¼ˆæ–°æ ¼å¼ï¼šåŒ…å«hashã€sizeå’Œurlï¼‰
    file_type: str  # æ–‡ä»¶ç±»å‹ï¼ˆ'image' æˆ– 'archive'ï¼‰
    original_path: str  # åŸå§‹æ–‡ä»¶è·¯å¾„



# å…¨å±€å˜é‡å®šä¹‰
global_hashes = {}

def get_artist_folder_path(base_path):
    """è·å–ç”»å¸ˆæ–‡ä»¶å¤¹è·¯å¾„"""
    try:
        base_path = Path(base_path).resolve()
        
        if '[' in str(base_path) and ']' in str(base_path):
            if base_path.exists():
                return base_path
            else:
                logging.info( f'âŒ æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨: {base_path}')
                return None
        
        artist_folders = []
        for entry in base_path.iterdir():
            if entry.is_dir() and '[' in entry.name and ']' in entry.name:
                artist_folders.append(entry)
                    
        if not artist_folders:
            logging.info( f'âŒ åœ¨è·¯å¾„ {base_path} ä¸‹æœªæ‰¾åˆ°ç”»å¸ˆæ–‡ä»¶å¤¹')
            return None
            
        if len(artist_folders) == 1:
            logging.info( f'âœ… æ‰¾åˆ°ç”»å¸ˆæ–‡ä»¶å¤¹: {artist_folders[0]}')
            return artist_folders[0]
            
        logging.info( f"\næ‰¾åˆ°ä»¥ä¸‹ç”»å¸ˆæ–‡ä»¶å¤¹:")
        for i, folder in enumerate(artist_folders, 1):
            logging.info( f"{i}. {folder}")
            
        # ä½¿ç”¨æ™®é€šç»ˆç«¯è¾“å…¥
        valid_paths = []
        print("è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶å¤¹ç¼–å·(ç›´æ¥å›è½¦ç¡®è®¤ç¬¬ä¸€ä¸ª):")
        while True:
            choice = input().strip()
            if not choice:
                break
            try:
                index = int(choice) - 1
                if 0 <= index < len(artist_folders):
                    return artist_folders[index]
                logging.info( f'âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•')
                return None
            except ValueError:
                logging.info( f'âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—')
                return None

            return None
    except Exception as e:
        logging.info( f'âŒ è·å–ç”»å¸ˆæ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}')
        return None

def process_single_image(image_path, lock) -> Dict[str, ProcessResult]:
    """å¤„ç†å•ä¸ªå›¾ç‰‡æ–‡ä»¶"""
    try:
        img_hash = ImageHashCalculator.calculate_phash(image_path)
        if img_hash and isinstance(img_hash, dict) and 'hash' in img_hash:
            uri = ImageHashCalculator.normalize_path(image_path)
            result = ProcessResult(
                uri=uri,
                hash_value=img_hash,  # img_hash å·²ç»æ˜¯å­—å…¸æ ¼å¼
                file_type='image',
                original_path=str(image_path)
            )
            with lock:
                # ä¿®æ”¹æ—¥å¿—è¾“å‡ºåˆ°å“ˆå¸Œè®¡ç®—é¢æ¿
                logging.info(f"[#hash_calc][âœ¨æ–°è®¡ç®—] å›¾ç‰‡: {image_path}  å“ˆå¸Œå€¼: {img_hash['hash']}")  
                return {uri: result}
        return {}
    except Exception as e:
        logging.error(f"[#hash_calc]å¤„ç†å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
        return {}

def check_zip_integrity(zip_path: Path) -> bool:
    """æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
    
    Args:
        zip_path: å‹ç¼©åŒ…è·¯å¾„
    
    Returns:
        bool: å‹ç¼©åŒ…æ˜¯å¦å®Œæ•´
    """
    try:
        cmd = ['7z', 't', str(zip_path)]
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            startupinfo=startupinfo,
            creationflags=subprocess.CREATE_NO_WINDOW
        )
        
        if result.returncode != 0:
            logging.info(f"å‹ç¼©åŒ…æŸåæˆ–æ— æ³•è¯»å–: {zip_path}")
            return False
        return True
    except Exception as e:
        logging.info(f"æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§æ—¶å‡ºé”™: {zip_path}: {e}")
        return False

def decode_zip_filename(name: bytes) -> str:
    """å°è¯•ä½¿ç”¨ä¸åŒç¼–ç è§£ç zipæ–‡ä»¶å"""
    """å¢å¼ºç‰ˆZIPæ–‡ä»¶åè§£ç """
    zip_flags = 0x800
    # æ£€æµ‹ZIPçš„UTF-8æ ‡å¿—ä½ï¼ˆç¬¬11ä½ï¼‰
    if (zip_flags & 0x800) != 0:
        try:
            return name.decode('utf-8')
        except UnicodeDecodeError:
            pass
    # å…¶ä»–ç¼–ç å›é€€é€»è¾‘...

    encodings = ['utf-8', 'gbk', 'cp437', 'shift-jis', 'gb18030', 'big5']


    try:
        import chardet
        detected = chardet.detect(name)
        if detected and detected['confidence'] > 0.6:  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
            encodings = [detected['encoding']] + encodings
    except ImportError:
        pass
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    encodings = [x for x in encodings if not (x in seen or seen.add(x))]
    
    # å°è¯•ä¸åŒçš„ç¼–ç ï¼ˆä½¿ç”¨surrogateescapeé”™è¯¯å¤„ç†ï¼‰
    for encoding in encodings:
        try:
            return name.decode(encoding, errors='surrogateescape')
        except (UnicodeDecodeError, LookupError):
            continue
            
    # ä½¿ç”¨surrogateescapeæ¨¡å¼å›é€€
    return name.decode('utf-8', errors='surrogateescape')



def update_stats_panel(total_files, processed_files, success_files, total_size, processed_size):
    """ç»Ÿä¸€ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤º"""
    progress_percent = int((processed_files / total_files * 100) if total_files > 0 else 0)
    stats_text = (
        f"æ€»è·¯å¾„æ•°: {total_files} å·²å¤„ç†: {processed_files} æˆåŠŸ: {success_files} "
        f"æ€»å¤§å°: {total_size:.2f}MB å·²å¤„ç†å¤§å°: {processed_size:.2f}MB"
    )
    logging.info(f"[@current_stats]  æ€»è¿›åº¦: {progress_percent}%")
    logging.info(f"[#current_stats]{stats_text}")


def update_performance_panel(thread_count, batch_size=None):
    """æ›´æ–°æ€§èƒ½é¢æ¿"""
    perf_text = (
        f"çº¿ç¨‹æ•°: {thread_count}\n"
        f"æ‰¹å¤„ç†å¤§å°: {batch_size if batch_size else 'æœªä½¿ç”¨'}"
    )
    logging.info( perf_text)

class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ç±»"""
    def __init__(self):
        self.total_files = 0
        self.processed_files = 0
        self.total_size = 0
        self.processed_size = 0
        self.current_file = ""
        self.current_progress = 0
        self._lock = threading.Lock()
        
    def update(self, processed_files=None, total_size=None, processed_size=None, 
               current_file=None, current_progress=None):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            if processed_files is not None:
                self.processed_files = processed_files
            if total_size is not None:
                self.total_size = total_size
            if processed_size is not None:
                self.processed_size = processed_size
            if current_file is not None:
                self.current_file = current_file
            if current_progress is not None:
                self.current_progress = current_progress
                
            # æ›´æ–°é¢æ¿
            update_stats_panel(self.total_files, self.processed_files, 
                             self.total_size, self.processed_size)
            if current_progress is not None:
                size_info = f" ({self.processed_size:.2f}MB)" if self.processed_size else ""
                # logging.info(f"[@current_progress] è¿›åº¦ {size_info} {current_progress}%")


def process_single_zip(zip_path, extract_base_dir, lock, force_update=False, inner_workers=4) -> Dict[str, ProcessResult]:
    """å¤„ç†å•ä¸ªå‹ç¼©åŒ…ä¸­çš„å›¾ç‰‡"""
    try:
        # ç¡®ä¿zip_pathæ˜¯å­—ç¬¦ä¸²ç±»å‹
        zip_path = str(zip_path)
        
        # é¦–å…ˆæ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
        if not check_zip_integrity(Path(zip_path)):
            logging.error(f"[#hash_calc]å‹ç¼©åŒ…æŸåæˆ–æ— æ³•è¯»å–: {zip_path}")
            return {}
            
        # æ£€æŸ¥å…¨å±€å“ˆå¸Œç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥å‹ç¼©åŒ…çš„è®°å½•
        if not force_update:
            results = ImageHashCalculator.match_existing_hashes(Path(zip_path), global_hashes, is_global=True)
            if results:
                return results

        zip_hashes = {}
        
        def process_image(args):
            """å¤„ç†å•ä¸ªå›¾ç‰‡çš„å‡½æ•°"""
            filename, img_data = args
            try:
                # æ„é€ å‹ç¼©åŒ…å†…å›¾ç‰‡çš„å®Œæ•´è·¯å¾„
                img_path = f"{zip_path}!/{filename}"
                # ç”Ÿæˆæ ‡å‡†åŒ–çš„URI
                uri = ImageHashCalculator.normalize_path(zip_path, filename)
                # è®¡ç®—å“ˆå¸Œå€¼æ—¶ä¼ å…¥URI
                img_hash = ImageHashCalculator.calculate_phash(io.BytesIO(img_data), url=uri)
                if img_hash and isinstance(img_hash, dict) and 'hash' in img_hash:
                    result = ProcessResult(
                        uri=uri,
                        hash_value=img_hash,  # img_hash å·²ç»æ˜¯å­—å…¸æ ¼å¼
                        file_type='archive',
                        original_path=str(zip_path)
                    )
                    with lock:
                        zip_hashes[uri] = result
            except Exception as e:
                logging.error(f"[#hash_calc]å¤„ç†å‹ç¼©åŒ…å†…å›¾ç‰‡å¤±è´¥ï¼Œè·³è¿‡: {filename}: {e}")

        
        # é¦–å…ˆå°è¯•ä½¿ç”¨zipfile
        with zipfile.ZipFile(zip_path, 'r') as zf:
            # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶åç¼–ç 
            image_files = []
            for info in zf.filelist:
                # å¤„ç†æ–‡ä»¶åç¼–ç 
                filename = decode_zip_filename(info.filename.encode('utf8'))
                if any(filename.lower().endswith(ext) for ext in 
                        ('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.bmp')):
                    # ç›´æ¥è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜
                    with zf.open(info) as f:
                        image_files.append((filename, f.read()))
            
            total_files = len(image_files)
            if total_files == 0:
                logging.info(f"[#file_ops]å‹ç¼©åŒ…å†…æ— å›¾ç‰‡æ–‡ä»¶: {zip_path}")
                return {}
                
            logging.info(f"[#file_ops]å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {zip_path.encode('utf-8', 'replace').decode('utf-8')}  å…± {total_files} ä¸ªå›¾ç‰‡æ–‡ä»¶")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†å›¾ç‰‡
            processed_count = 0
            with ThreadPoolExecutor(max_workers=inner_workers) as executor:
                futures = []
                for filename, img_data in image_files:
                    futures.append(executor.submit(process_image, (filename, img_data)))
            
            for future in futures:
                future.result()
                processed_count += 1
                progress = int((processed_count / total_files) * 100)
                with lock:
                    # ä¿®æ”¹ä¸ºä¸“ç”¨è¿›åº¦æ¡æ ¼å¼
                    logging.info(f"[@hash_progress] è¿›åº¦{progress}%")  # å‹ç¼©åŒ…å†…è¿›åº¦

                
        return zip_hashes
                        
    except Exception as e:
        logging.error(f"[#hash_calc]å¤„ç†å‹ç¼©åŒ…å¤±è´¥ï¼Œè·³è¿‡: {str(zip_path).encode('utf-8', 'replace').decode('utf-8')}: {e}")
        return {}

def get_default_output_path(input_path: Path) -> Path:
    """è·å–é»˜è®¤çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Args:
        input_path: è¾“å…¥è·¯å¾„
        
    Returns:
        é»˜è®¤çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    input_path = Path(input_path)
    
    # å¦‚æœæ˜¯ç”»å¸ˆæ¨¡å¼ï¼Œè·å–ç”»å¸ˆæ–‡ä»¶å¤¹
    artist_folder = get_artist_folder_path(input_path)
    if artist_folder:
        base_dir = artist_folder
    else:
        # å¦‚æœæ˜¯å‹ç¼©åŒ…ï¼Œä½¿ç”¨å…¶æ‰€åœ¨ç›®å½•
        if input_path.is_file() and input_path.suffix.lower() == '.zip':
            base_dir = input_path.parent
        else:
            base_dir = input_path
        
    # å½“è·¯å¾„åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ç”¨æˆ·è·¯å¾„ä¸­çš„ [ ] å’Œ ã€ï¼‰æ—¶å¯èƒ½éœ€è¦å¤„ç†
    return base_dir / 'image_hashes.json'  # éœ€è¦ç¡®ä¿ base_dir æ­£ç¡®

def should_process_path(path: Path, base_path: Path) -> bool:
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦åº”è¯¥è¢«å¤„ç†
    
    Args:
        path: è¦æ£€æŸ¥çš„è·¯å¾„
        base_path: åŸºå‡†è·¯å¾„ï¼ˆè¾“å…¥è·¯å¾„ï¼‰
        
    Returns:
        bool: æ˜¯å¦åº”è¯¥å¤„ç†è¯¥è·¯å¾„
    """
    # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„è¿›è¡Œæ£€æŸ¥
    try:
        path_str = str(path).lower()
        # æ£€æŸ¥å®Œæ•´è·¯å¾„æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
        if any(keyword.lower() in path_str for keyword in BLACKLIST_KEYWORDS):
            logging.info(f"[#hash_calc]è·³è¿‡æ–‡ä»¶ï¼ˆé»‘åå•ï¼‰: {path}")
            return False
            
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä¹Ÿæ£€æŸ¥ä¸€ä¸‹
        try:
            rel_path = path.relative_to(base_path)
            for part in rel_path.parts:
                if any(keyword.lower() in part.lower() for keyword in BLACKLIST_KEYWORDS):
                    logging.info(f"[#hash_calc]è·³è¿‡æ–‡ä»¶ï¼ˆé»‘åå•ï¼‰: {path}")
                    return False
        except ValueError:
            pass
            
        return True
            
    except Exception as e:
        logging.error(f"[#hash_calc]æ£€æŸ¥è·¯å¾„æ—¶å‡ºé”™: {e}")
        return False

def process_path(path: str, config: dict = None) -> Dict[str, ProcessResult]:
    """å¤„ç†æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶æˆ–ç›®å½•
    
    Args:
        path: è¦å¤„ç†çš„è·¯å¾„
        config: å¤„ç†é…ç½®ï¼ŒåŒ…å«ä»¥ä¸‹å¯é€‰é¡¹ï¼š
            - max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            - force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            - dry_run: æ˜¯å¦ä»…é¢„è§ˆ
            - extract_dir: è§£å‹ç›®å½•
            - hash_size: å“ˆå¸Œå¤§å°
            - hash_version: å“ˆå¸Œç‰ˆæœ¬
    
    Returns:
        Dict[str, ProcessResult]: å¤„ç†ç»“æœ
    """
    try:
        # åˆå¹¶é…ç½®
        cfg = DEFAULT_PROCESS_CONFIG.copy()
        if config:
            cfg.update(config)
            
        path = Path(path)
        if not path.exists():
            logging.error(f"[#hash_calc]è·¯å¾„ä¸å­˜åœ¨: {path}")
            return {}
        
        # ä½¿ç”¨é…ç½®çš„è§£å‹ç›®å½•
        extract_base_dir = Path(cfg['extract_dir'])
        extract_base_dir.mkdir(exist_ok=True, parents=True)
        logging.info(f"[#hash_calc]è§£å‹ç›®å½•: {extract_base_dir}")
        
        # åŠ è½½å…¨å±€å“ˆå¸Œç¼“å­˜
        if global_hashes:
            logging.info(f"[#hash_calc]å·²åŠ è½½ {len(global_hashes)} ä¸ªç¼“å­˜å“ˆå¸Œå€¼")
            
        # å¦‚æœä¸æ˜¯å¼ºåˆ¶æ›´æ–°ï¼Œå…ˆå°è¯•åŒ¹é…å…¨å±€å“ˆå¸Œ
        if not cfg['force_update']:
            # è¿‡æ»¤æ‰åŒ…å«é»‘åå•å…³é”®è¯çš„å…¨å±€å“ˆå¸Œ
            filtered_global_hashes = {}
            for uri, hash_value in global_hashes.items():
                if not any(keyword.lower() in uri.lower() for keyword in BLACKLIST_KEYWORDS):
                    filtered_global_hashes[uri] = hash_value
            
            # ä½¿ç”¨è¿‡æ»¤åçš„å…¨å±€å“ˆå¸Œè¿›è¡ŒåŒ¹é…
            results = ImageHashCalculator.match_existing_hashes(path, filtered_global_hashes, is_global=True)
            if results:
                return results
                
            # å†å°è¯•æœ¬åœ°å“ˆå¸ŒåŒ¹é…
            try:
                with open(get_default_output_path(path), 'rb') as f:
                    local_hashes = orjson.loads(f.read()).get('hashes', {})
                    if local_hashes:
                        results = ImageHashCalculator.match_existing_hashes(path, local_hashes)
                        if results:
                            return results
            except:
                pass
        
        # åˆ›å»ºçº¿ç¨‹é”
        lock = threading.Lock()
        results: Dict[str, ProcessResult] = {}
        
        # æ ¹æ®è·¯å¾„ç±»å‹å¤„ç†
        if path.is_file():
            results.update(process_single_file(path, cfg, lock, extract_base_dir))
        elif path.is_dir():
            results.update(process_directory(path, cfg, lock, extract_base_dir))
        
        # ä¿å­˜ç»“æœ
        if results and not cfg['dry_run']:
            save_results(results, path, cfg)
            
        logging.info(f"[#hash_calc]å¤„ç†å®Œæˆ")
        return results
        
    except Exception as e:
        logging.error(f"[#hash_calc]å¤„ç†è·¯å¾„æ—¶å‡ºé”™: {e}")
        return {}

def process_single_file(path: Path, config: dict, lock: threading.Lock, extract_dir: Path) -> Dict[str, ProcessResult]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    results = {}
    if not should_process_path(path, path.parent):
        logging.info(f"[#hash_calc]è·³è¿‡æ–‡ä»¶ï¼ˆé»‘åå•ï¼‰: {path}")
        return results
        
    file_size = path.stat().st_size / (1024 * 1024)
    logging.info(f"[@hash_progress] è¿›åº¦ 0% ")  # åˆå§‹è¿›åº¦
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨åˆ†ç»„ä¿¡æ¯
    if config.get('use_groups') and path.suffix.lower() in ['.zip']:
        # å°è¯•è¯»å–åˆ†ç»„ä¿¡æ¯
        group_info_path = os.path.join(path.parent, 'group_info.json')
        if os.path.exists(group_info_path):
            try:
                with open(group_info_path, 'r', encoding='utf-8') as f:
                    group_info = json.load(f)
                    
                # æ£€æŸ¥å½“å‰æ–‡ä»¶çš„åˆ†ç»„ä¿¡æ¯
                rel_path = str(path.name)
                if rel_path in group_info:
                    file_info = group_info[rel_path]
                    if file_info['type'] == 'trash':
                        logging.info(f"[#hash_calc]è·³è¿‡trashæ–‡ä»¶: {path}")
                        return results
                    elif file_info['type'] == 'multi' and not file_info.get('is_main', False):
                        logging.info(f"[#hash_calc]è·³è¿‡éä¸»è¦multiæ–‡ä»¶: {path}")
                        return results
                    else:
                        logging.info(f"[#hash_calc]å¤„ç†{file_info['type']}æ–‡ä»¶: {path}")
            except Exception as e:
                logging.error(f"[#hash_calc]è¯»å–åˆ†ç»„ä¿¡æ¯å¤±è´¥: {e}")
    
    # ç»§ç»­åŸæœ‰çš„å¤„ç†é€»è¾‘
    if path.suffix.lower() in ['.zip']:
        results = process_single_zip(path, extract_dir, lock, config['force_update'])
    elif path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.bmp']:
        results = process_single_image(path, lock)
        
    return results

def process_directory(path: Path, config: dict, lock: threading.Lock, extract_dir: Path) -> Dict[str, ProcessResult]:
    """å¤„ç†ç›®å½•"""
    results = {}
    files_to_process = []
    skipped_files = []
    skipped_existing = []
    total_size = 0
    
    # å°è¯•åŠ è½½æœ¬åœ°å“ˆå¸Œæ–‡ä»¶
    try:
        hash_file_path = str(get_default_output_path(path))
        with open(hash_file_path, 'rb') as f:
            local_hashes = orjson.loads(f.read()).get('hashes', {})
    except:
        local_hashes = {}
    
    # æ”¶é›†æ–‡ä»¶
    for item in path.rglob("*"):
        if not item.is_file() or not should_process_path(item, path):
            continue
            
        item_path = str(item).replace('\\', '/')
        # å…ˆå°è¯•åŒ¹é…ç°æœ‰å“ˆå¸Œ
        if local_hashes:
            results = ImageHashCalculator.match_existing_hashes(item, local_hashes)
            if results:
                skipped_existing.append(item)
                continue
                
        if item.suffix.lower() in ['.zip', '.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.bmp']:
            files_to_process.append(('zip' if item.suffix.lower() == '.zip' else 'image', item))
            total_size += item.stat().st_size / (1024 * 1024)
    
    # å¤„ç†æ–‡ä»¶
    if files_to_process:
        results.update(process_file_batch(
            files_to_process, 
            config, 
            lock, 
            extract_dir, 
            total_size
        ))
    
    # æ›´æ–°ç»Ÿè®¡æ˜¾ç¤º
    processed_size = 0
    for result in results.values():
        try:
            file_path = Path(result.original_path)
            if file_path.exists():
                processed_size += file_path.stat().st_size / (1024 * 1024)
        except:
            continue
            
    update_stats_panel(
        total_files=len(files_to_process),
        processed_files=len(results) - len(skipped_files),
        success_files=len(results) - len(skipped_files),
        total_size=total_size,
        processed_size=processed_size
    )
    
    return results

def process_file_batch(files: list, config: dict, lock: threading.Lock, 
                      extract_dir: Path, total_size: float) -> Dict[str, ProcessResult]:
    """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
    results = {}
    processed_count = 0
    processed_size = 0
    success_count = 0  # æ·»åŠ æˆåŠŸè®¡æ•°å™¨åˆå§‹åŒ–
    total_count = len(files)
    
    # åˆå§‹åŒ–ç»Ÿè®¡é¢æ¿
    update_stats_panel(
        total_files=total_count,
        processed_files=0,
        success_files=0,
        total_size=total_size,
        processed_size=0
    )

    with ThreadPoolExecutor(max_workers=config['max_workers']) as executor:
        # ä¿®æ”¹futureså…ƒç»„æºå¸¦æ–‡ä»¶ç±»å‹ä¿¡æ¯
        futures = []
        for file_type, file_path in files:
            if file_type == 'zip':
                future = executor.submit(process_single_zip, file_path, extract_dir, lock, config['force_update'])
            else:
                future = executor.submit(process_single_image, file_path, lock)
            futures.append((future, file_path, file_path.stat().st_size / (1024 * 1024), file_type))
        
        # å¤„ç†å®Œæˆæ—¶æ›´æ–°æ€»ä½“è¿›åº¦
        for future, file_path, file_size, file_type in futures:
            try:
                file_results = future.result()
                if file_results:  # æœ‰ç»“æœè¡¨ç¤ºå¤„ç†æˆåŠŸ
                    success_count += 1
                    results.update(file_results)
                    
                # å®æ—¶æ›´æ–°ç»Ÿè®¡ï¼ˆæ¯æ¬¡å¾ªç¯éƒ½æ›´æ–°ï¼‰
                update_stats_panel(
                    total_files=total_count,
                    processed_files=processed_count,
                    success_files=success_count,
                    total_size=total_size,
                    processed_size=processed_size
                )
                
            except Exception as e:
                logging.error(f"[#hash_calc]å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            finally:
                processed_count += 1
                processed_size += file_size
                progress = int(processed_count/total_count*100)
                # æ–°è¿›åº¦æ ¼å¼
                logging.info(f"[@hash_progress] è¿›åº¦ {progress}%")
                
    return results

def save_results(results: Dict[str, ProcessResult], path: Path, config: dict) -> None:
    """ä¿å­˜å¤„ç†ç»“æœ"""
    output_path = get_default_output_path(path)
    output = {
        "_hash_params": f"hash_size={config['hash_size']};hash_version={config['hash_version']}",
        "dry_run": config['dry_run'],
        "input_paths": [str(path)],
        "hashes": {}
    }
    
    # æ­£ç¡®å¤„ç†hash_valueå­—æ®µ
    for uri, result in results.items():
        if isinstance(result.hash_value, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨hashå­—æ®µ
            output["hashes"][uri] = {'hash': result.hash_value['hash']}
        else:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            output["hashes"][uri] = {'hash': result.hash_value}
    
    # ä¿®æ”¹ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶æ“ä½œé¢æ¿
    logging.info(f"[#file_ops]å‡†å¤‡ä¿å­˜ {len(output['hashes'])} ä¸ªå“ˆå¸Œå€¼")
    logging.info(f"[#file_ops]âœ… å·²ä¿å­˜å“ˆå¸Œç»“æœåˆ°: {output_path}")
    logging.info(f"[#file_ops]å·²æ›´æ–°ç¼“å­˜ï¼Œç°æœ‰ {len(global_hashes)} ä¸ªå“ˆå¸Œå€¼")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    ImageHashCalculator.save_hash_file_path(str(output_path))
    
    # æ›´æ–°å…¨å±€ç¼“å­˜
    hash_dict = {k: v['hash'] for k, v in output["hashes"].items()}
    global_hashes.update(hash_dict)
    ImageHashCalculator.save_global_hashes(global_hashes)

def main():
    """ä¸»å‡½æ•°
    
    Returns:
        int: é€€å‡ºç 
            0: æˆåŠŸå®Œæˆ
            1: æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶
            2: è¾“å…¥è·¯å¾„ä¸å­˜åœ¨
            3: å¤„ç†è¿‡ç¨‹å‡ºé”™
    """
    try:
        # åœ¨mainå‡½æ•°é¡¶éƒ¨è®¾ç½®æ ‡å‡†æµç¼–ç 
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        
        # åœ¨mainå‡½æ•°é¡¶éƒ¨å£°æ˜å…¨å±€å˜é‡
        global global_hashes
        global_hashes = ImageHashCalculator.load_global_hashes()
        
        # è®¾ç½®Pythonçš„é»˜è®¤ç¼–ç ä¸ºUTF-8
        import locale
        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
        
        # è®¾ç½®æ ‡å‡†è¾“å…¥è¾“å‡ºçš„ç¼–ç 
        if sys.stdout.encoding != 'utf-8':
            sys.stdout.reconfigure(encoding='utf-8')
        if sys.stderr.encoding != 'utf-8':
            sys.stderr.reconfigure(encoding='utf-8')
            
        print("å¼€å§‹æ‰§è¡Œä¸»å‡½æ•°...", flush=True)
        
        parser = argparse.ArgumentParser(description='å›¾ç‰‡å“ˆå¸Œé¢„çƒ­å·¥å…·')
        parser.add_argument('-w', '--workers', type=int, default=4, help='çº¿ç¨‹æ•° (é»˜è®¤: 4)')
        parser.add_argument('-f', '--force', action='store_true', help='å¼ºåˆ¶æ›´æ–°æ‰€æœ‰å“ˆå¸Œå€¼ï¼Œå¿½ç•¥ç¼“å­˜')
        parser.add_argument('-d', '--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼,ä¸å®é™…ä¿®æ”¹æ–‡ä»¶')
        parser.add_argument('--cache', type=str, default=GLOBAL_HASH_CACHE, help='ç¼“å­˜æ–‡ä»¶è·¯å¾„')
        parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--path', type=str, help='è¦å¤„ç†çš„æ–‡ä»¶å¤¹è·¯å¾„')
        parser.add_argument('--paths', type=str, nargs='+', help='è¦å¤„ç†çš„å¤šä¸ªæ–‡ä»¶å¤¹è·¯å¾„')
        parser.add_argument('--hash-size', type=int, default=10, help='å“ˆå¸Œå¤§å° (é»˜è®¤: 10)')
        parser.add_argument('--use-groups', action='store_true', help='ä½¿ç”¨å·²æœ‰çš„åˆ†ç»„ä¿¡æ¯JSONæ–‡ä»¶è¿‡æ»¤æ–‡ä»¶')

        print("è§£æå‘½ä»¤è¡Œå‚æ•°...", flush=True)
        args = parser.parse_args()
        print(f"å‘½ä»¤è¡Œå‚æ•°: {args}", flush=True)
        
        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆè·¯å¾„
        valid_paths = []
        
        # å¤„ç†å•ä¸ªè·¯å¾„å‚æ•°
        if args.path:
            if os.path.exists(args.path):
                valid_paths.append(args.path)
                print(f"æ·»åŠ æœ‰æ•ˆè·¯å¾„: {args.path}")
            else:
                print(f"è·¯å¾„ä¸å­˜åœ¨: {args.path}")
                
        # å¤„ç†å¤šä¸ªè·¯å¾„å‚æ•°
        if args.paths:
            for path in args.paths:
                if os.path.exists(path):
                    valid_paths.append(path)
                    print(f"æ·»åŠ æœ‰æ•ˆè·¯å¾„: {path}")
                else:
                    print(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
        
        # å¦‚æœæ²¡æœ‰é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨äº¤äº’å¼è¾“å…¥
        if not valid_paths:
            try:
                print("è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶å¤¹æˆ–å‹ç¼©åŒ…è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
                while True:
                    path = input().strip().strip('"').strip("'")
                    if not path:
                        break
                    if os.path.exists(path):
                        valid_paths.append(path)
                        print(f"æ·»åŠ æœ‰æ•ˆè·¯å¾„: {path}")
                    else:
                        print(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
            except Exception as e:
                print(f"è·å–è·¯å¾„å¤±è´¥: {e}")
                return EXIT_PROCESS_ERROR
        
        if not valid_paths:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è·¯å¾„")
            return EXIT_NO_FILES
            
        print(f"å…±æ‰¾åˆ° {len(valid_paths)} ä¸ªæœ‰æ•ˆè·¯å¾„")
        
        # åœ¨è¿™é‡Œåˆå§‹åŒ–æ—¥å¿—é¢æ¿ï¼Œå› ä¸ºå·²ç»æ”¶é›†å®Œäº†æ‰€æœ‰è·¯å¾„
        TextualLoggerManager.set_layout(FULL_LAYOUT_CONFIG,config_info['log_file'])
        
        # æ›´æ–°å“ˆå¸Œå‚æ•°
        params['hash_size'] = args.hash_size
        
        # æ›´æ–°é…ç½®
        config = {
            'max_workers': args.workers,
            'force_update': args.force,
            'dry_run': args.dry_run,
            'use_groups': args.use_groups  # æ·»åŠ åˆ†ç»„é…ç½®
        }
        
        # å¤„ç†æ‰€æœ‰è·¯å¾„
        success_count = 0
        total_paths = len(valid_paths)
        
        # æ›´æ–°æ€»ä½“è¿›åº¦
        
        for index, path in enumerate(valid_paths, 1):
            # æ›´æ–°å½“å‰è¿›åº¦ - åªæ˜¾ç¤ºå½“å‰å¤„ç†çš„æ–‡ä»¶ï¼Œä¸æ˜¾ç¤ºè¿›åº¦
            try:
                file_size = Path(path).stat().st_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
            except:
                file_size = None
            logging.info(f"[@hash_progress] è¿›åº¦ 0%")  # åˆå§‹è¿›åº¦
            
            try:
                # æ›´æ–°å¤„ç†æ—¥å¿—
                logging.info(f"[#hash_calc]å¼€å§‹å¤„ç†è·¯å¾„: {path}")
                results = process_path(path, config)
                
                if results:
                    # è·å–å½“å‰è·¯å¾„çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
                    output_path = get_default_output_path(Path(path))
                    
                    # è¿‡æ»¤æ‰åŒ…å«é»‘åå•å…³é”®è¯çš„ç»“æœ
                    filtered_results = {}
                    for uri, result in results.items():
                        if not any(keyword.lower() in uri.lower() for keyword in BLACKLIST_KEYWORDS):
                            filtered_results[uri] = result
                    
                    if filtered_results:
                        logging.info(f"[#hash_calc]å‡†å¤‡ä¿å­˜ {len(filtered_results)} ä¸ªå“ˆå¸Œå€¼")
                        ImageHashCalculator.save_hash_results(filtered_results, output_path, args.dry_run)
                    else:
                        logging.info(f"[#hash_calc]æ²¡æœ‰éœ€è¦ä¿å­˜çš„å“ˆå¸Œå€¼ï¼ˆå…¨éƒ¨è¢«è¿‡æ»¤ï¼‰")
                    
                    success_count += 1
                    # æ›´æ–°å½“å‰æ–‡ä»¶è¿›åº¦ä¸ºå®Œæˆ
                    logging.info(f"[@hash_progress] è¿›åº¦100%")
                    
            except Exception as e:
                logging.error(f"[#hash_calc]å¤„ç†è·¯å¾„å¤±è´¥: {path}: {e}")
                # æ›´æ–°æ€»ä½“è¿›åº¦
                continue
        
        if success_count == 0:
            logging.error("[#hash_calc]æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è·¯å¾„")
            return EXIT_NO_FILES
            
        logging.info(f"[#hash_calc]å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {success_count} ä¸ªè·¯å¾„")
        return EXIT_SUCCESS
            
    except Exception as e:
        logging.error(f"[#hash_calc]å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return EXIT_PROCESS_ERROR



if __name__ == "__main__":
    sys.exit(main()) 