import streamlit as st
import json
import time
from pathlib import Path
from PIL import Image
import pillow_avif
import pillow_jxl 
import imagehash
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import base64
import io
import zipfile
import tempfile
import hashlib
import os
from functools import lru_cache

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="pHash å›¾ç‰‡ç›¸ä¼¼åº¦åˆ†æ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(90deg, #f0f2f6, #ffffff);
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .stExpander > div > div > div > div {
        padding-top: 0.5rem;
    }
    .image-comparison {
        border: 2px solid #e0e0e0;
        border-radius: 8px;
        padding: 10px;
        margin: 5px;
    }
    .distance-low { color: #d32f2f; font-weight: bold; }
    .distance-medium { color: #f57c00; font-weight: bold; }
    .distance-high { color: #388e3c; font-weight: bold; }
    
    /* ç–‘ä¼¼é‡å¤å›¾ç‰‡ç»„çš„æ ·å¼ */
    .duplicate-group {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 10px;
        margin: 5px;
        background-color: #fafafa;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .duplicate-header {
        text-align: center;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 10px;
    }
    .vs-separator {
        text-align: center;
        font-weight: bold;
        color: #ff6b6b;
        margin: 5px 0;
    }
</style>
""", unsafe_allow_html=True)

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / "config.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        # é»˜è®¤é…ç½®
        return {
            "app_config": {
                "title": "pHash å›¾ç‰‡ç›¸ä¼¼åº¦åˆ†æ",
                "icon": "ğŸ”",
                "layout": "wide",
                "max_images_display": 50,
                "thumbnail_size": [150, 150],
                "default_folder": "E:\\1Hub\\EH\\2EHV\\test"
            },
            "hash_presets": {
                "å¿«é€Ÿæµ‹è¯•": [8, 10],
                "æ ‡å‡†åˆ†æ": [10, 12, 16],
                "ç²¾ç»†åˆ†æ": [12, 16, 20, 24],
                "å…¨é¢åˆ†æ": [8, 10, 12, 16, 20, 24, 32]
            },
            "available_sizes": [4, 6, 8, 10, 12, 16, 20, 24, 32, 48, 64],
            "distance_thresholds": {
                "very_similar": 5,
                "similar": 15,
                "different": 25
            },
            "image_extensions": [
                ".jpg", ".jpeg", ".png", ".webp", 
                ".bmp", ".gif", ".tiff", ".jxl", ".avif"
            ]
        }

def load_session_state():
    """åˆå§‹åŒ–session state"""
    if 'results' not in st.session_state:
        st.session_state.results = {}
    if 'image_files' not in st.session_state:
        st.session_state.image_files = []
    if 'analysis_complete' not in st.session_state:
        st.session_state.analysis_complete = False
    if 'hash_cache' not in st.session_state:
        st.session_state.hash_cache = {}

@st.cache_data(show_spinner=False)
def get_file_info(file_path):
    """è·å–æ–‡ä»¶ä¿¡æ¯ç”¨äºç¼“å­˜é”®"""
    try:
        stat = os.stat(file_path)
        return {
            'size': stat.st_size,
            'mtime': stat.st_mtime,
            'path': str(file_path)
        }
    except Exception:
        return None

def create_cache_key(file_path, hash_size):
    """åˆ›å»ºç¼“å­˜é”®"""
    file_info = get_file_info(file_path)
    if file_info is None:
        return None
    
    # åŸºäºæ–‡ä»¶è·¯å¾„ã€å¤§å°ã€ä¿®æ”¹æ—¶é—´å’Œå“ˆå¸Œå°ºå¯¸åˆ›å»ºé”®
    cache_data = f"{file_info['path']}_{file_info['size']}_{file_info['mtime']}_{hash_size}"
    return hashlib.md5(cache_data.encode()).hexdigest()

@lru_cache(maxsize=1000)
def _calculate_single_phash(file_path_str, hash_size, cache_key):
    """è®¡ç®—å•ä¸ªå›¾ç‰‡çš„pHashï¼ˆå¸¦LRUç¼“å­˜ï¼‰"""
    try:
        with Image.open(file_path_str) as im:
            return str(imagehash.phash(im, hash_size=hash_size))
    except Exception as e:
        st.warning(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {file_path_str}ï¼ŒåŸå› : {e}")
        return None

def calculate_image_hash_cached(file_path, hash_size):
    """è®¡ç®—å›¾ç‰‡å“ˆå¸Œï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    file_path_str = str(file_path)
    cache_key = create_cache_key(file_path_str, hash_size)
    
    if cache_key is None:
        # æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ï¼Œç›´æ¥è®¡ç®—
        try:
            with Image.open(file_path_str) as im:
                return str(imagehash.phash(im, hash_size=hash_size))
        except Exception as e:
            st.warning(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {file_path_str}ï¼ŒåŸå› : {e}")
            return None
    
    # æ£€æŸ¥session stateç¼“å­˜
    if cache_key in st.session_state.hash_cache:
        return st.session_state.hash_cache[cache_key]
    
    # ä½¿ç”¨LRUç¼“å­˜è®¡ç®—
    hash_value = _calculate_single_phash(file_path_str, hash_size, cache_key)
    
    # å­˜å‚¨åˆ°session stateç¼“å­˜
    if hash_value is not None:
        st.session_state.hash_cache[cache_key] = hash_value
    
    return hash_value

def scan_images(image_dir, config):
    """æ‰«æå›¾ç‰‡æ–‡ä»¶"""
    image_dir = Path(image_dir)
    image_extensions = config["image_extensions"]
    image_files = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    status_text.text("æ­£åœ¨æ‰«æå›¾ç‰‡æ–‡ä»¶...")
    
    for ext in image_extensions:
        files = list(image_dir.rglob(f"*{ext}"))
        files.extend(list(image_dir.rglob(f"*{ext.upper()}")))
        image_files.extend(files)
    
    progress_bar.progress(1.0)
    status_text.text(f"æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
    
    return image_files

def calc_hashes_for_images(image_files, hash_size):
    """è®¡ç®—å›¾ç‰‡å“ˆå¸Œï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰"""
    hashes = {}
    total = len(image_files)
    cache_hits = 0
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, img in enumerate(image_files):
        # æ£€æŸ¥ç¼“å­˜
        cache_key = create_cache_key(str(img), hash_size)
        if cache_key and cache_key in st.session_state.hash_cache:
            # ç¼“å­˜å‘½ä¸­
            hashes[str(img)] = st.session_state.hash_cache[cache_key]
            cache_hits += 1
        else:
            # è®¡ç®—æ–°å“ˆå¸Œ
            hash_value = calculate_image_hash_cached(img, hash_size)
            if hash_value is not None:
                hashes[str(img)] = hash_value
        
        progress = (i + 1) / total
        progress_bar.progress(progress)
        status_text.text(f"è®¡ç®—å“ˆå¸Œ ({hash_size}): {i + 1}/{total} (ç¼“å­˜å‘½ä¸­: {cache_hits})")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    if cache_hits > 0:
        cache_ratio = cache_hits / total * 100
        st.success(f"âœ… å“ˆå¸Œè®¡ç®—å®Œæˆï¼ç¼“å­˜å‘½ä¸­ç‡: {cache_ratio:.1f}% ({cache_hits}/{total})")
    
    return hashes

def calc_hamming_pairs(hashes):
    """è®¡ç®—æ±‰æ˜è·ç¦»"""
    pairs = []
    items = list(hashes.items())
    total_pairs = len(items) * (len(items) - 1) // 2
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    processed = 0
    for i, (img1, h1_str) in enumerate(items):
        h1 = imagehash.hex_to_hash(h1_str)
        for j in range(i + 1, len(items)):
            img2, h2_str = items[j]
            h2 = imagehash.hex_to_hash(h2_str)
            dist = h1 - h2
            pairs.append({
                "image1": img1,
                "image2": img2,
                "distance": dist,
                "hash1": h1_str,
                "hash2": h2_str
            })
            
            processed += 1
            if processed % 100 == 0:  # æ¯100æ¬¡æ›´æ–°ä¸€æ¬¡è¿›åº¦
                progress = processed / total_pairs
                progress_bar.progress(progress)
                status_text.text(f"è®¡ç®—æ±‰æ˜è·ç¦»: {processed}/{total_pairs}")
    
    progress_bar.progress(1.0)
    status_text.text(f"æ±‰æ˜è·ç¦»è®¡ç®—å®Œæˆ: {total_pairs} å¯¹")
    
    return pairs

def save_results_to_json(results, image_files, output_path):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    data = {
        "timestamp": datetime.now().isoformat(),
        "total_images": len(image_files),
        "image_files": [str(f) for f in image_files],
        "results": results
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return data

def load_results_from_json(json_path):
    """ä»JSONæ–‡ä»¶åŠ è½½ç»“æœ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        st.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        return None

def get_image_base64(image_path, max_size=(150, 150)):
    """å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç """
    try:
        with Image.open(image_path) as img:
            img.thumbnail(max_size, Image.Resampling.LANCZOS)
            buffer = io.BytesIO()
            img.save(buffer, format='PNG')
            img_str = base64.b64encode(buffer.getvalue()).decode()
            return f"data:image/png;base64,{img_str}"
    except:
        return None

def get_image_info(image_path):
    """è·å–å›¾ç‰‡çš„å°ºå¯¸å’Œæ–‡ä»¶å¤§å°ä¿¡æ¯"""
    try:
        if not Path(image_path).exists():
            return None
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = Path(image_path).stat().st_size
        size_str = format_file_size(file_size)
        
        # è·å–å›¾ç‰‡å°ºå¯¸
        with Image.open(image_path) as img:
            width, height = img.size
            return {
                'size': file_size,
                'size_str': size_str,
                'width': width,
                'height': height,
                'dimensions': f"{width}Ã—{height}"
            }
    except Exception:
        return None

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f} {size_names[i]}"

def get_distance_color_class(distance, config):
    """æ ¹æ®è·ç¦»è¿”å›CSSç±»å"""
    thresholds = config["distance_thresholds"]
    if distance <= thresholds["very_similar"]:
        return "distance-low"
    elif distance <= thresholds["similar"]:
        return "distance-medium"
    else:
        return "distance-high"

def export_results_excel(results, image_files):
    """å¯¼å‡ºç»“æœåˆ°Excelæ–‡ä»¶"""
    output = io.BytesIO()
    
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # æ€»ä½“ç»Ÿè®¡è¡¨
        summary_data = []
        for size, info in results.items():
            stats = info["statistics"]
            summary_data.append({
                "å“ˆå¸Œå°ºå¯¸": size,
                "è®¡ç®—è€—æ—¶(ç§’)": info['elapsed_time'],
                "å¹³å‡è·ç¦»": stats['avg_distance'],
                "æœ€å¤§è·ç¦»": stats['max_distance'],
                "æœ€å°è·ç¦»": stats['min_distance'],
                "å¯¹æ¯”æ€»æ•°": stats['total_pairs']
            })
        
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_excel(writer, sheet_name='æ€»ä½“ç»Ÿè®¡', index=False)
        
        # æ¯ä¸ªå°ºå¯¸çš„è¯¦ç»†ç»“æœ
        for size, info in results.items():
            pairs_data = []
            for pair in info["pairs"]:
                img1_path = Path(pair["image1"])
                img2_path = Path(pair["image2"])
                pairs_data.append({
                    "å›¾ç‰‡1åç§°": img1_path.name,
                    "å›¾ç‰‡1è·¯å¾„": str(img1_path),
                    "å›¾ç‰‡2åç§°": img2_path.name,
                    "å›¾ç‰‡2è·¯å¾„": str(img2_path),
                    "æ±‰æ˜è·ç¦»": pair["distance"],
                    "å“ˆå¸Œ1": pair["hash1"],
                    "å“ˆå¸Œ2": pair["hash2"]
                })
            
            if pairs_data:
                df_pairs = pd.DataFrame(pairs_data)
                # æŒ‰è·ç¦»æ’åº
                df_pairs = df_pairs.sort_values('æ±‰æ˜è·ç¦»')
                df_pairs.to_excel(writer, sheet_name=f'å°ºå¯¸{size}', index=False)
    
    output.seek(0)
    return output

def create_duplicate_report(results, similarity_threshold=10):
    """ç”Ÿæˆç–‘ä¼¼é‡å¤å›¾ç‰‡æŠ¥å‘Š"""
    duplicates = {}
    
    for size, info in results.items():
        size_duplicates = []
        for pair in info["pairs"]:
            if pair["distance"] <= similarity_threshold:
                size_duplicates.append(pair)
        
        # æŒ‰æ±‰æ˜è·ç¦»å‡åºæ’åºï¼ˆæœ€ç›¸ä¼¼çš„åœ¨å‰é¢ï¼‰
        size_duplicates.sort(key=lambda x: x["distance"])
        duplicates[size] = size_duplicates
    
    return duplicates

def clear_hash_cache():
    """æ¸…ç†å“ˆå¸Œç¼“å­˜"""
    if 'hash_cache' in st.session_state:
        cache_size = len(st.session_state.hash_cache)
        st.session_state.hash_cache.clear()
        _calculate_single_phash.cache_clear()  # æ¸…ç†LRUç¼“å­˜
        return cache_size
    return 0

def get_cache_info():
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    session_cache_size = len(st.session_state.hash_cache) if 'hash_cache' in st.session_state else 0
    lru_cache_info = _calculate_single_phash.cache_info()
    
    return {
        'session_cache_entries': session_cache_size,
        'lru_cache_hits': lru_cache_info.hits,
        'lru_cache_misses': lru_cache_info.misses,
        'lru_cache_size': lru_cache_info.currsize,
        'lru_cache_max': lru_cache_info.maxsize
    }

def main():
    config = load_config()
    app_config = config["app_config"]
    
    load_session_state()
    
    # æ ‡é¢˜
    st.markdown(f'<h1 class="main-header">{app_config["icon"]} {app_config["title"]}</h1>', unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    st.sidebar.title("âš™ï¸ é…ç½®é€‰é¡¹")
    
    # é€‰æ‹©æ¨¡å¼
    mode = st.sidebar.radio(
        "é€‰æ‹©æ¨¡å¼",
        ["æ–°å»ºåˆ†æ", "åŠ è½½å·²æœ‰ç»“æœ"],
        help="é€‰æ‹©æ–°å»ºåˆ†ææˆ–åŠ è½½ä¹‹å‰ä¿å­˜çš„ç»“æœ"
    )
    
    if mode == "æ–°å»ºåˆ†æ":        # æ–‡ä»¶å¤¹é€‰æ‹©
        image_folder = st.sidebar.text_input(
            "ğŸ“ å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„",
            value=app_config["default_folder"],
            help="è¾“å…¥åŒ…å«å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„"
        )
        
        # å“ˆå¸Œå°ºå¯¸é€‰æ‹©
        st.sidebar.markdown("### ğŸ¯ å“ˆå¸Œå°ºå¯¸è®¾ç½®")
        
        # é¢„è®¾é€‰é¡¹
        preset_options = ["è‡ªå®šä¹‰"] + list(config["hash_presets"].keys())
        preset_sizes = st.sidebar.selectbox(
            "é¢„è®¾å°ºå¯¸ç»„åˆ",
            preset_options
        )
        
        if preset_sizes == "è‡ªå®šä¹‰":
            # å¤šé€‰æ¡†é€‰æ‹©å°ºå¯¸
            selected_sizes = st.sidebar.multiselect(
                "é€‰æ‹©å“ˆå¸Œå°ºå¯¸",
                config["available_sizes"],
                default=[10, 12, 16],
                help="é€‰æ‹©è¦æµ‹è¯•çš„å“ˆå¸Œå°ºå¯¸ï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ª"
            )
        else:
            selected_sizes = config["hash_presets"][preset_sizes]
            st.sidebar.info(f"å·²é€‰æ‹©å°ºå¯¸: {selected_sizes}")
        
        # é«˜çº§è®¾ç½®
        with st.sidebar.expander("ğŸ”§ é«˜çº§è®¾ç½®"):
            similarity_threshold = st.slider(
                "ç–‘ä¼¼é‡å¤é˜ˆå€¼",
                min_value=0,
                max_value=20,
                value=10,
                help="æ±‰æ˜è·ç¦»å°äºæ­¤å€¼çš„å›¾ç‰‡å°†è¢«æ ‡è®°ä¸ºç–‘ä¼¼é‡å¤"
            )
            
            export_format = st.selectbox(
                "å¯¼å‡ºæ ¼å¼",
                ["JSON", "Excel"],
                help="é€‰æ‹©ç»“æœå¯¼å‡ºæ ¼å¼"
            )
        
        # å¼€å§‹åˆ†ææŒ‰é’®
        if st.sidebar.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            if not image_folder or not Path(image_folder).exists():
                st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼")
            elif not selected_sizes:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå“ˆå¸Œå°ºå¯¸ï¼")
            else:
                # æ‰§è¡Œåˆ†æ
                st.markdown("## ğŸ“Š åˆ†æè¿›åº¦")
                  # æ‰«æå›¾ç‰‡
                with st.expander("ğŸ“ æ‰«æå›¾ç‰‡æ–‡ä»¶", expanded=True):
                    image_files = scan_images(image_folder, config)
                    st.session_state.image_files = image_files
                
                if len(image_files) < 2:
                    st.error("å›¾ç‰‡æ•°é‡å¤ªå°‘ï¼Œéœ€è¦è‡³å°‘2å¼ å›¾ç‰‡è¿›è¡Œå¯¹æ¯”åˆ†æï¼")
                    return
                
                # åˆ†ææ¯ä¸ªå°ºå¯¸
                results = {}
                
                for size in selected_sizes:
                    with st.expander(f"ğŸ” åˆ†æå“ˆå¸Œå°ºå¯¸ {size}", expanded=True):
                        st.write(f"**å“ˆå¸Œå°ºå¯¸: {size}**")
                        
                        start_time = time.time()
                        
                        # è®¡ç®—å“ˆå¸Œ
                        st.write("è®¡ç®—å›¾ç‰‡å“ˆå¸Œ...")
                        hashes = calc_hashes_for_images(image_files, size)
                        
                        # è®¡ç®—æ±‰æ˜è·ç¦»
                        st.write("è®¡ç®—æ±‰æ˜è·ç¦»...")
                        pairs = calc_hamming_pairs(hashes)
                        
                        elapsed_time = time.time() - start_time
                        
                        # ç»Ÿè®¡ä¿¡æ¯
                        distances = [p["distance"] for p in pairs]
                        if distances:
                            avg_dist = sum(distances) / len(distances)
                            max_dist = max(distances)
                            min_dist = min(distances)
                            
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("è®¡ç®—è€—æ—¶", f"{elapsed_time:.2f}s")
                            with col2:
                                st.metric("å¹³å‡è·ç¦»", f"{avg_dist:.2f}")
                            with col3:
                                st.metric("æœ€å¤§è·ç¦»", str(max_dist))
                            with col4:
                                st.metric("æœ€å°è·ç¦»", str(min_dist))
                        
                        results[size] = {
                            "hashes": hashes,
                            "pairs": pairs,
                            "elapsed_time": elapsed_time,
                            "statistics": {
                                "avg_distance": avg_dist if distances else 0,
                                "max_distance": max_dist if distances else 0,
                                "min_distance": min_dist if distances else 0,
                                "total_pairs": len(pairs)
                            }
                        }
                
                st.session_state.results = results
                st.session_state.analysis_complete = True
                  # ä¿å­˜ç»“æœ
                output_path = Path(image_folder) / f"phash_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                if export_format == "JSON":
                    save_results_to_json(results, image_files, output_path)
                    st.success(f"âœ… åˆ†æå®Œæˆï¼JSONç»“æœå·²ä¿å­˜åˆ°: {output_path}")
                
                # æä¾›ä¸‹è½½æŒ‰é’®
                if export_format == "Excel":
                    excel_data = export_results_excel(results, image_files)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ExcelæŠ¥å‘Š",
                        data=excel_data,
                        file_name=f"phash_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
    
    else:  # åŠ è½½å·²æœ‰ç»“æœ
        json_file = st.sidebar.file_uploader(
            "ä¸Šä¼ JSONç»“æœæ–‡ä»¶",
            type=['json'],
            help="é€‰æ‹©ä¹‹å‰ä¿å­˜çš„åˆ†æç»“æœæ–‡ä»¶"
        )
        
        if json_file is not None:
            data = json.load(json_file)
            st.session_state.results = data["results"]
            st.session_state.image_files = [Path(f) for f in data["image_files"]]
            st.session_state.analysis_complete = True
            
            st.sidebar.success(f"âœ… å·²åŠ è½½ {data['total_images']} å¼ å›¾ç‰‡çš„åˆ†æç»“æœ")
            st.sidebar.info(f"åˆ†ææ—¶é—´: {data['timestamp']}")
      # æ˜¾ç¤ºç»“æœ
    if st.session_state.analysis_complete and st.session_state.results:
        st.markdown("## ğŸ“ˆ åˆ†æç»“æœ")
        
        # æ·»åŠ æ ‡ç­¾é¡µ
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š æ€»ä½“åˆ†æ", "ğŸ” è¯¦ç»†å¯¹æ¯”", "ğŸš¨ ç–‘ä¼¼é‡å¤", "ğŸ“¥ å¯¼å‡ºä¸‹è½½"])
        
        with tab1:
            # æ€»ä½“å¯¹æ¯”è¡¨
            st.markdown("### ğŸ“Š æ€»ä½“ç»Ÿè®¡")
            summary_data = []
            for size, info in st.session_state.results.items():
                stats = info["statistics"]
                summary_data.append({
                    "å“ˆå¸Œå°ºå¯¸": size,
                    "è®¡ç®—è€—æ—¶(ç§’)": f"{info['elapsed_time']:.2f}",
                    "å¹³å‡è·ç¦»": f"{stats['avg_distance']:.2f}",
                    "æœ€å¤§è·ç¦»": stats['max_distance'],
                    "æœ€å°è·ç¦»": stats['min_distance'],
                    "å¯¹æ¯”æ€»æ•°": stats['total_pairs']
                })
            
            df_summary = pd.DataFrame(summary_data)
            st.dataframe(df_summary, use_container_width=True)
            
            # å¯è§†åŒ–å¯¹æ¯”
            col1, col2 = st.columns(2)
            with col1:
                fig1 = px.bar(
                    df_summary,
                    x="å“ˆå¸Œå°ºå¯¸",
                    y="å¹³å‡è·ç¦»",
                    title="ä¸åŒå“ˆå¸Œå°ºå¯¸çš„å¹³å‡æ±‰æ˜è·ç¦»å¯¹æ¯”",
                    color="å¹³å‡è·ç¦»",
                    color_continuous_scale="viridis"
                )
                st.plotly_chart(fig1, use_container_width=True)
            
            with col2:
                fig2 = px.bar(
                    df_summary,
                    x="å“ˆå¸Œå°ºå¯¸",
                    y="è®¡ç®—è€—æ—¶(ç§’)",
                    title="ä¸åŒå“ˆå¸Œå°ºå¯¸çš„è®¡ç®—è€—æ—¶å¯¹æ¯”",
                    color="è®¡ç®—è€—æ—¶(ç§’)",
                    color_continuous_scale="reds"
                )
                st.plotly_chart(fig2, use_container_width=True)
        
        with tab2:
            # è¯¦ç»†ç»“æœå±•ç¤º
            st.markdown("### ğŸ” è¯¦ç»†å¯¹æ¯”ç»“æœ")
            
            for size, info in st.session_state.results.items():
                with st.expander(f"ğŸ“‹ å“ˆå¸Œå°ºå¯¸ {size} - è¯¦ç»†ç»“æœ", expanded=False):
                    pairs = info["pairs"]
                    stats = info["statistics"]
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("æ€»å¯¹æ¯”æ•°", stats['total_pairs'])
                    with col2:
                        st.metric("å¹³å‡è·ç¦»", f"{stats['avg_distance']:.2f}")
                    with col3:
                        st.metric("æœ€å¤§è·ç¦»", stats['max_distance'])
                    with col4:
                        st.metric("æœ€å°è·ç¦»", stats['min_distance'])
                    
                    # è·ç¦»åˆ†å¸ƒå›¾
                    distances = [p["distance"] for p in pairs]
                    fig_hist = px.histogram(
                        x=distances,
                        nbins=30,
                        title=f"å“ˆå¸Œå°ºå¯¸ {size} - æ±‰æ˜è·ç¦»åˆ†å¸ƒ",
                        labels={"x": "æ±‰æ˜è·ç¦»", "y": "é¢‘æ¬¡"}
                    )
                    st.plotly_chart(fig_hist, use_container_width=True)
                    
                    # è¿‡æ»¤å™¨
                    st.markdown("#### ğŸ”§ è¿‡æ»¤é€‰é¡¹")
                    col1, col2 = st.columns(2)
                    with col1:
                        max_distance = st.slider(
                            f"æœ€å¤§æ±‰æ˜è·ç¦» (size {size})",
                            min_value=0,
                            max_value=stats['max_distance'],
                            value=min(10, stats['max_distance']),
                            key=f"filter_{size}"
                        )
                    with col2:
                        show_images = st.checkbox(f"æ˜¾ç¤ºå›¾ç‰‡é¢„è§ˆ (size {size})", key=f"show_img_{size}")
                    
                    # è¿‡æ»¤å’Œæ’åº
                    filtered_pairs = [p for p in pairs if p["distance"] <= max_distance]
                    filtered_pairs.sort(key=lambda x: x["distance"])
                    
                    st.write(f"**æ˜¾ç¤º {len(filtered_pairs)} å¯¹ç›¸ä¼¼å›¾ç‰‡ (è·ç¦» â‰¤ {max_distance})**")
                    
                    # æ˜¾ç¤ºç»“æœ
                    if show_images:
                        # å›¾ç‰‡å±•ç¤ºæ¨¡å¼
                        max_display = app_config.get("max_images_display", 50)
                        for i, pair in enumerate(filtered_pairs[:max_display]):
                            distance = pair["distance"]
                            img1_path = Path(pair["image1"])
                            img2_path = Path(pair["image2"])
                            
                            st.markdown(f"**å¯¹æ¯” {i+1}** - æ±‰æ˜è·ç¦»: "
                                      f"<span class='{get_distance_color_class(distance, config)}'>{distance}</span>",
                                      unsafe_allow_html=True)
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                if img1_path.exists():
                                    st.image(str(img1_path), caption=img1_path.name, width=200)
                                st.caption(f"ğŸ“ {img1_path.parent}")
                            
                            with col2:
                                if img2_path.exists():
                                    st.image(str(img2_path), caption=img2_path.name, width=200)
                                st.caption(f"ğŸ“ {img2_path.parent}")
                            
                            st.markdown("---")
                        
                        if len(filtered_pairs) > max_display:
                            st.info(f"ä»…æ˜¾ç¤ºå‰ {max_display} å¯¹ï¼Œå…±æœ‰ {len(filtered_pairs)} å¯¹ç¬¦åˆæ¡ä»¶")
                    else:
                        # è¡¨æ ¼å±•ç¤ºæ¨¡å¼
                        table_data = []
                        for pair in filtered_pairs:
                            img1_path = Path(pair["image1"])
                            img2_path = Path(pair["image2"])
                            table_data.append({
                                "å›¾ç‰‡1": img1_path.name,
                                "å›¾ç‰‡1è·¯å¾„": str(img1_path.parent),
                                "å›¾ç‰‡2": img2_path.name,
                                "å›¾ç‰‡2è·¯å¾„": str(img2_path.parent),
                                "æ±‰æ˜è·ç¦»": pair["distance"]
                            })
                        
                        if table_data:
                            df_pairs = pd.DataFrame(table_data)
                            st.dataframe(df_pairs, use_container_width=True)
        with tab3:
            # ç–‘ä¼¼é‡å¤å›¾ç‰‡æŠ¥å‘Š
            st.markdown("### ğŸš¨ ç–‘ä¼¼é‡å¤å›¾ç‰‡æŠ¥å‘Š")
              # æ§åˆ¶é€‰é¡¹
            st.markdown("#### âš™ï¸ æ˜¾ç¤ºè®¾ç½®")
            col1, col2, col3 = st.columns(3)
            with col1:
                dup_threshold = st.slider(
                    "ç–‘ä¼¼é‡å¤é˜ˆå€¼",
                    min_value=0,
                    max_value=20,
                    value=config["distance_thresholds"]["very_similar"],
                    help="æ±‰æ˜è·ç¦»å°äºæ­¤å€¼çš„å›¾ç‰‡å°†è¢«è§†ä¸ºç–‘ä¼¼é‡å¤"
                )
            with col2:
                groups_per_row = st.slider(
                    "æ¯è¡Œæ˜¾ç¤ºç»„æ•°",
                    min_value=1,
                    max_value=4,
                    value=2,
                    help="æ§åˆ¶ä¸€è¡Œæ˜¾ç¤ºå¤šå°‘å¯¹ç–‘ä¼¼é‡å¤å›¾ç‰‡"
                )
            with col3:
                image_width = st.slider(
                    "å›¾ç‰‡æ˜¾ç¤ºå®½åº¦",
                    min_value=80,
                    max_value=200,
                    value=120,
                    step=10,
                    help="è°ƒæ•´å›¾ç‰‡çš„æ˜¾ç¤ºå¤§å°"
                )
            duplicates = create_duplicate_report(st.session_state.results, dup_threshold)
            
            # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
            total_duplicates = sum(len(size_duplicates) for size_duplicates in duplicates.values())
            if total_duplicates > 0:
                st.success(f"ğŸ¯ å…±å‘ç° **{total_duplicates}** å¯¹ç–‘ä¼¼é‡å¤å›¾ç‰‡")
            else:
                st.info("âœ¨ æœªå‘ç°ç–‘ä¼¼é‡å¤å›¾ç‰‡")
            
            for size, size_duplicates in duplicates.items():
                with st.expander(f"ğŸ“‹ å“ˆå¸Œå°ºå¯¸ {size} - å‘ç° {len(size_duplicates)} å¯¹ç–‘ä¼¼é‡å¤", expanded=len(size_duplicates) > 0):
                    if size_duplicates:
                        # æŒ‰è¡Œæ˜¾ç¤ºé‡å¤å›¾ç‰‡å¯¹
                        for row_start in range(0, len(size_duplicates), groups_per_row):
                            row_pairs = size_duplicates[row_start:row_start + groups_per_row]
                            
                            # åˆ›å»ºåˆ—å¸ƒå±€
                            cols = st.columns(groups_per_row)
                            
                            for col_idx, pair in enumerate(row_pairs):
                                with cols[col_idx]:
                                    img1_path = Path(pair["image1"])
                                    img2_path = Path(pair["image2"])
                                    
                                    # è·å–å›¾ç‰‡ä¿¡æ¯
                                    img1_info = get_image_info(img1_path)
                                    img2_info = get_image_info(img2_path)
                                      # ä½¿ç”¨å®¹å™¨åŒ…è£…æ¯ä¸€å¯¹å›¾ç‰‡
                                    with st.container():
                                        # è‡ªå®šä¹‰æ ·å¼çš„å¤´éƒ¨
                                        st.markdown(f'<div class="duplicate-header">ç–‘ä¼¼é‡å¤ {row_start + col_idx + 1}</div>', unsafe_allow_html=True)
                                        st.markdown(f"**æ±‰æ˜è·ç¦»:** {pair['distance']} | **ç›¸ä¼¼åº¦:** {100-pair['distance']*2:.1f}%")
                                          # å›¾ç‰‡1
                                        if img1_path.exists():
                                            st.image(str(img1_path), caption=f"ğŸ“¸ {img1_path.name}", width=image_width)
                                        st.caption(f"ğŸ“ {img1_path.parent.name}")
                                        if img1_info:
                                            st.caption(f"ğŸ“ {img1_info['dimensions']} | ğŸ’¾ {img1_info['size_str']}")
                                        
                                        # åˆ†éš”ç¬¦
                                        st.markdown('<div class="vs-separator">âš¡ VS âš¡</div>', unsafe_allow_html=True)
                                        
                                        # å›¾ç‰‡2
                                        if img2_path.exists():
                                            st.image(str(img2_path), caption=f"ğŸ“¸ {img2_path.name}", width=image_width)
                                        st.caption(f"ğŸ“ {img2_path.parent.name}")
                                        if img2_info:
                                            st.caption(f"ğŸ“ {img2_info['dimensions']} | ğŸ’¾ {img2_info['size_str']}")
                                        
                                        # æ–‡ä»¶å¤§å°æ¯”è¾ƒ
                                        if img1_info and img2_info:
                                            size1 = img1_info['size']
                                            size2 = img2_info['size']                                        # æ–‡ä»¶å¤§å°æ¯”è¾ƒ
                                        if img1_info and img2_info:
                                            size1 = img1_info['size']
                                            size2 = img2_info['size']
                                            if size1 < size2:
                                                st.success("ğŸŸ¢ è¾ƒå°æ–‡ä»¶")
                                            elif size1 > size2:
                                                st.error("ğŸ”´ è¾ƒå¤§æ–‡ä»¶")
                                            else:
                                                st.info("ğŸŸ¡ å¤§å°ç›¸åŒ")
                                        elif img1_path.exists() and img2_path.exists():
                                            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è¯»å–æ–‡ä»¶å¤§å°
                                            size1 = img1_path.stat().st_size
                                            size2 = img2_path.stat().st_size
                                            if size1 < size2:
                                                st.success("ğŸŸ¢ è¾ƒå°æ–‡ä»¶")
                                            elif size1 > size2:
                                                st.error("ğŸ”´ è¾ƒå¤§æ–‡ä»¶")
                                            else:
                                                st.info("ğŸŸ¡ å¤§å°ç›¸åŒ")
                                        
                                        # æ·»åŠ åˆ†éš”çº¿
                                        st.markdown("---")
                            
                            # å¦‚æœè¿™ä¸€è¡Œæ²¡æœ‰å¡«æ»¡ï¼Œä¸ºç©ºåˆ—æ·»åŠ å ä½ç¬¦
                            for empty_col_idx in range(len(row_pairs), groups_per_row):
                                with cols[empty_col_idx]:
                                    st.empty()
                    else:
                        st.info("æœªå‘ç°ç–‘ä¼¼é‡å¤çš„å›¾ç‰‡")
        
        with tab4:
            # å¯¼å‡ºå’Œä¸‹è½½
            st.markdown("### ğŸ“¥ å¯¼å‡ºå’Œä¸‹è½½")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### JSON æ ¼å¼")
                json_data = {
                    "timestamp": datetime.now().isoformat(),
                    "total_images": len(st.session_state.image_files),
                    "image_files": [str(f) for f in st.session_state.image_files],
                    "results": st.session_state.results
                }
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ JSON æŠ¥å‘Š",
                    data=json.dumps(json_data, ensure_ascii=False, indent=2),
                    file_name=f"phash_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
            
            with col2:
                st.markdown("#### Excel æ ¼å¼")
                excel_data = export_results_excel(st.session_state.results, st.session_state.image_files)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ Excel æŠ¥å‘Š",
                    data=excel_data,
                    file_name=f"phash_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            
            st.markdown("#### ğŸ“Š æŠ¥å‘ŠåŒ…å«å†…å®¹")
            st.info("""
            - **æ€»ä½“ç»Ÿè®¡**: å„å“ˆå¸Œå°ºå¯¸çš„æ€§èƒ½å¯¹æ¯”
            - **è¯¦ç»†å¯¹æ¯”**: æ‰€æœ‰å›¾ç‰‡å¯¹çš„æ±‰æ˜è·ç¦»
            - **ç–‘ä¼¼é‡å¤**: é«˜ç›¸ä¼¼åº¦å›¾ç‰‡è¯†åˆ«
            - **å¯è§†åŒ–å›¾è¡¨**: è·ç¦»åˆ†å¸ƒå’Œæ€§èƒ½å¯¹æ¯”
            """)
    
    elif not st.session_state.analysis_complete:
        # æ¬¢è¿é¡µé¢
        st.markdown("""
        ## ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ pHash å›¾ç‰‡ç›¸ä¼¼åº¦åˆ†æå·¥å…·
        
        ### ğŸš€ åŠŸèƒ½ç‰¹ç‚¹
        - **å¤šå°ºå¯¸åˆ†æ**: æ”¯æŒåŒæ—¶åˆ†æå¤šä¸ªå“ˆå¸Œå°ºå¯¸
        - **äº¤äº’å¼ç•Œé¢**: ç¾è§‚çš„ Streamlit ç•Œé¢ï¼Œæ”¯æŒå®æ—¶è¿›åº¦æ˜¾ç¤º
        - **ç»“æœå¯è§†åŒ–**: ä¸°å¯Œçš„å›¾è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
        - **æ•°æ®æŒä¹…åŒ–**: ç»“æœä¿å­˜ä¸º JSON æ ¼å¼ï¼Œå¯é‡å¤åŠ è½½
        - **æ™ºèƒ½è¿‡æ»¤**: æŒ‰æ±‰æ˜è·ç¦»è¿‡æ»¤ç›¸ä¼¼å›¾ç‰‡
        - **å›¾ç‰‡é¢„è§ˆ**: ç›´è§‚çš„å›¾ç‰‡å¯¹æ¯”å±•ç¤º
        
        ### ğŸ“‹ ä½¿ç”¨è¯´æ˜
        1. åœ¨å·¦ä¾§é€‰æ‹©"æ–°å»ºåˆ†æ"æˆ–"åŠ è½½å·²æœ‰ç»“æœ"
        2. è¾“å…¥å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        3. é€‰æ‹©è¦åˆ†æçš„å“ˆå¸Œå°ºå¯¸
        4. ç‚¹å‡»"å¼€å§‹åˆ†æ"æŒ‰é’®
        5. æŸ¥çœ‹åˆ†æç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        
        ### ğŸ’¡ æç¤º
        - å“ˆå¸Œå°ºå¯¸è¶Šå¤§ï¼Œè®¡ç®—æ—¶é—´è¶Šé•¿ï¼Œä½†ç²¾åº¦æ›´é«˜
        - æ±‰æ˜è·ç¦»è¶Šå°ï¼Œå›¾ç‰‡è¶Šç›¸ä¼¼
        - å»ºè®®å…ˆç”¨è¾ƒå°çš„å°ºå¯¸è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        """)

if __name__ == "__main__":
    main()
