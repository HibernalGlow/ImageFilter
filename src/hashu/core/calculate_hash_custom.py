from hashu.log import logger

from PIL import Image
import pillow_avif
import pillow_jxl
import cv2
import numpy as np
from io import BytesIO
from pathlib import Path
import imagehash
from itertools import combinations
from rich.markdown import Markdown
from rich.console import Console
from datetime import datetime
import orjson
import os
from urllib.parse import quote, unquote, urlparse
from dataclasses import dataclass
from typing import Dict, Tuple, Union, List, Optional
import re
from functools import lru_cache
import time
from hashu.utils.path_uri import PathURIGenerator 
from hashu.utils.image_clarity import ImageClarityEvaluator
# å¯¼å‡ºè¿™äº›ç±»ï¼Œä½¿å…¶ä¿æŒå‘åå…¼å®¹
__all__ = [
    'PathURIGenerator',
    'ImageClarityEvaluator',
    'ImageHashCalculator',
    'HashCache',
]

# å¯¼å…¥SQLiteå­˜å‚¨æ¨¡å—
from hashu.core.sqlite_storage import HashDatabaseManager

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from hashu.config import get_config

# å¤šè¿›ç¨‹åŒæ­¥é”
import threading
_cache_lock = threading.RLock()  # ä½¿ç”¨é€’å½’é”é˜²æ­¢æ­»é”

# è·å–é…ç½®ç®¡ç†å™¨å®ä¾‹
_config = get_config()

# å…¨å±€é…ç½®ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰
GLOBAL_HASH_FILES = _config.get_json_hash_files()
CACHE_TIMEOUT = _config.get_cache_timeout()
# ä¿®æ”¹HASH_FILES_LISTçš„å®šä¹‰ï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²è€Œä¸æ˜¯åˆ—è¡¨
# å¦‚æœget_json_hash_filesè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œåˆ™å–æœ€åä¸€ä¸ªå…ƒç´ çš„ç›®å½•åŠ ä¸Šhash_files_list.txt
if isinstance(GLOBAL_HASH_FILES, list) and GLOBAL_HASH_FILES:
    HASH_FILES_LIST = str(Path(GLOBAL_HASH_FILES[-1]).parent / "hash_files_list.txt")
else:
    HASH_FILES_LIST = str(Path(_config.get_cache_dir()) / "hash_files_list.txt")

# å“ˆå¸Œè®¡ç®—å‚æ•°
HASH_PARAMS = _config.get_hash_params()

# å¤šè¿›ç¨‹ä¼˜åŒ–é…ç½®
MULTIPROCESS_CONFIG = _config.get_multiprocess_config()

@lru_cache(maxsize=1)
def get_db_cached():
    from hashu.core.sqlite_storage import get_database_instance
    return get_database_instance()

class HashCache:
    """å“ˆå¸Œå€¼ç¼“å­˜ç®¡ç†ç±»ï¼ˆSQLite + JSONåŒå­˜å‚¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    _instance = None
    _cache = {}
    _initialized = False
    _last_refresh = 0
    _last_save = 0  # æ–°å¢ï¼šè®°å½•ä¸Šæ¬¡ä¿å­˜æ—¶é—´
    _hash_counter = 0  # æ–°å¢ï¼šå“ˆå¸Œè®¡ç®—è®¡æ•°å™¨
    _sqlite_db = None  # SQLiteæ•°æ®åº“å®ä¾‹

    def __new__(cls):
        """çº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼"""
        with _cache_lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
            return cls._instance

    @classmethod
    def _get_sqlite_db(cls) -> Optional[HashDatabaseManager]:
        """è·å–SQLiteæ•°æ®åº“å®ä¾‹"""
        if cls._sqlite_db is None and MULTIPROCESS_CONFIG.get('use_sqlite', True):
            try:
                cls._sqlite_db = get_db_cached()
                # logger.info("SQLiteæ•°æ®åº“å·²åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–SQLiteæ•°æ®åº“å¤±è´¥: {e}")
        return cls._sqlite_db

    @classmethod
    def get_cache(cls, use_preload: bool = False):
        """è·å–å†…å­˜ä¸­çš„ç¼“å­˜æ•°æ®
        
        Args:
            use_preload: æ˜¯å¦ä½¿ç”¨é¢„åŠ è½½ç¼“å­˜ï¼ˆå¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ¨èï¼‰
        """
        with _cache_lock:
            # å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½ç¼“å­˜
            if use_preload and MULTIPROCESS_CONFIG.get('preload_cache'):
                return MULTIPROCESS_CONFIG['preload_cache']
                
            current_time = time.time()
            # å¦‚æœæœªåˆå§‹åŒ–æˆ–è€…è·ç¦»ä¸Šæ¬¡åˆ·æ–°è¶…è¿‡è¶…æ—¶æ—¶é—´ï¼Œåˆ™åˆ·æ–°ç¼“å­˜
            if not cls._initialized or (current_time - cls._last_refresh > CACHE_TIMEOUT):
                cls.refresh_cache()
            return cls._cache.copy()  # è¿”å›å‰¯æœ¬é¿å…å¹¶å‘ä¿®æ”¹

    @classmethod
    def refresh_cache(cls):
        """åˆ·æ–°ç¼“å­˜å¹¶ä¿æŒå†…å­˜é©»ç•™ï¼ˆå¤šè¿›ç¨‹ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        with _cache_lock:
            try:
                new_cache = {}
                loaded_files = []
                
                for hash_file in GLOBAL_HASH_FILES:
                    try:
                        if not os.path.exists(hash_file):
                            logger.debug(f"å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {hash_file}")
                            continue
                            
                        with open(hash_file, 'rb') as f:
                            data = orjson.loads(f.read())
                            if not data:
                                logger.debug(f"å“ˆå¸Œæ–‡ä»¶ä¸ºç©º: {hash_file}")
                                continue
                        
                        # å¤„ç†æ–°æ ¼å¼ (image_hashes_collection.json)
                        if "hashes" in data:
                            hashes = data["hashes"]
                            if not hashes:
                                logger.debug(f"å“ˆå¸Œæ•°æ®ä¸ºç©º: {hash_file}")
                                continue
                                
                            for uri, hash_data in hashes.items():
                                if isinstance(hash_data, dict):
                                    if hash_str := hash_data.get('hash'):
                                        new_cache[uri] = hash_str
                                else:
                                    new_cache[uri] = str(hash_data)
                        else:
                            # å¤„ç†æ—§æ ¼å¼ (image_hashes_global.json)
                            # æ’é™¤ç‰¹æ®Šé”®
                            special_keys = {'_hash_params', 'dry_run', 'input_paths'}
                            for k, v in data.items():
                                if k not in special_keys:
                                    if isinstance(v, dict):
                                        if hash_str := v.get('hash'):
                                            new_cache[k] = hash_str
                                    else:
                                        new_cache[k] = str(v)
                                        
                        loaded_files.append(hash_file)
                        logger.debug(f"ä» {hash_file} åŠ è½½äº†å“ˆå¸Œå€¼")
                                
                    except Exception as e:
                        logger.error(f"åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥ {hash_file}: {e}")
                        continue
                        
                if loaded_files:
                    cls._cache = new_cache  # ç›´æ¥æ›¿æ¢å¼•ç”¨ä¿è¯åŸå­æ€§
                    cls._initialized = True
                    cls._last_refresh = time.time()
                    # logger.debug(f"å“ˆå¸Œç¼“å­˜å·²æ›´æ–°ï¼Œå…± {len(cls._cache)} ä¸ªæ¡ç›®ï¼Œæ¥æº: {loaded_files}")
                else:
                    logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å“ˆå¸Œæ–‡ä»¶")
                    if not cls._initialized:
                        cls._cache = {}  # å¦‚æœæ˜¯é¦–æ¬¡åˆå§‹åŒ–å¤±è´¥ï¼Œç¡®ä¿æœ‰ä¸€ä¸ªç©ºç¼“å­˜
                        cls._initialized = True
                    
            except Exception as e:
                logger.error(f"åˆ·æ–°å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}")
                if not cls._initialized:
                    cls._cache = {}  # å¦‚æœæ˜¯é¦–æ¬¡åˆå§‹åŒ–å¤±è´¥ï¼Œç¡®ä¿æœ‰ä¸€ä¸ªç©ºç¼“å­˜                    cls._initialized = True

    @classmethod
    def sync_to_file(cls, force=False):
        """å°†å†…å­˜ç¼“å­˜åŒæ­¥åˆ°æ–‡ä»¶
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶åŒæ­¥ï¼Œå¿½ç•¥è®¡æ—¶å™¨å’Œè®¡æ•°å™¨
        
        Returns:
            bool: æ˜¯å¦æ‰§è¡Œäº†ä¿å­˜æ“ä½œ
        """
        # å¤šè¿›ç¨‹ç¯å¢ƒä¸‹å¦‚æœç¦ç”¨è‡ªåŠ¨ä¿å­˜ï¼Œåˆ™ç›´æ¥è¿”å›
        if not MULTIPROCESS_CONFIG.get('enable_auto_save', True) and not force:
            return False
            
        with _cache_lock:
            current_time = time.time()
            should_save_by_time = (current_time - cls._last_save > 300)  # 5åˆ†é’Ÿä¿å­˜ä¸€æ¬¡
            should_save_by_count = (cls._hash_counter >= 10)  # ç´¯ç§¯10ä¸ªæ–°å“ˆå¸Œå€¼ä¿å­˜ä¸€æ¬¡
            
            if force or should_save_by_time or should_save_by_count:                
                try:
                    logger.info(f"åŒæ­¥å“ˆå¸Œç¼“å­˜åˆ°æ–‡ä»¶, å…±{len(cls._cache)}ä¸ªæ¡ç›® [è®¡æ•°:{cls._hash_counter}, é—´éš”:{int(current_time-cls._last_save)}ç§’]")
                    ImageHashCalculator.save_global_hashes(cls._cache)
                    cls._last_save = current_time
                    cls._hash_counter = 0  # é‡ç½®è®¡æ•°å™¨
                    return True
                except Exception as e:
                    logger.error(f"åŒæ­¥ç¼“å­˜åˆ°æ–‡ä»¶å¤±è´¥: {e}")
                    return False
            
            return False
    
    @classmethod
    def add_hash(cls, uri: str, hash_value: str, auto_sync: bool = True, metadata: Dict[str, any] = None):
        """æ·»åŠ å“ˆå¸Œå€¼åˆ°ç¼“å­˜ï¼ˆæ”¯æŒSQLiteå’ŒJSONåŒå­˜å‚¨ï¼‰
        
        Args:
            uri: æ ‡å‡†åŒ–çš„URI
            hash_value: å“ˆå¸Œå€¼
            auto_sync: æ˜¯å¦è‡ªåŠ¨åŒæ­¥åˆ°æ–‡ä»¶
            metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆæ–‡ä»¶å¤§å°ã€å›¾ç‰‡å°ºå¯¸ç­‰ï¼‰
        """
        with _cache_lock:
            # æ›´æ–°å†…å­˜ç¼“å­˜
            cls._cache[uri] = hash_value
            
            # å¦‚æœå¯ç”¨SQLiteï¼ŒåŒæ—¶å†™å…¥æ•°æ®åº“
            sqlite_db = cls._get_sqlite_db()
            if sqlite_db:
                try:
                    sqlite_db.add_hash(uri, hash_value, metadata=metadata or {})
                    sqlite_db._get_connection().commit()
                    logger.debug(f"å“ˆå¸Œå€¼å·²å†™å…¥SQLite: {uri}")
                except Exception as e:
                    logger.error(f"å†™å…¥SQLiteå¤±è´¥: {e}")
            
            if auto_sync:
                cls._hash_counter += 1
                cls.sync_to_file()

    @classmethod
    def get_hash(cls, uri: str, use_preload: bool = False) -> Optional[str]:
        """è·å–æŒ‡å®šURIçš„å“ˆå¸Œå€¼ï¼ˆæ™ºèƒ½æŸ¥è¯¢ï¼šSQLiteä¼˜å…ˆ + æ ¼å¼è½¬æ¢åŒ¹é…ï¼‰
        
        Args:
            uri: æ ‡å‡†åŒ–çš„URI
            use_preload: æ˜¯å¦ä½¿ç”¨é¢„åŠ è½½ç¼“å­˜
            
        Returns:
            Optional[str]: å“ˆå¸Œå€¼ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        # 1. å¦‚æœå¯ç”¨SQLiteä¸”è®¾ç½®ä¸ºä¼˜å…ˆï¼Œå…ˆæŸ¥è¯¢SQLite
        if MULTIPROCESS_CONFIG.get('use_sqlite', True) and MULTIPROCESS_CONFIG.get('sqlite_priority', True):
            sqlite_db = cls._get_sqlite_db()
            if sqlite_db:
                try:
                    # ä½¿ç”¨æ™ºèƒ½æŸ¥è¯¢ï¼Œæ”¯æŒæ ¼å¼è½¬æ¢åŒ¹é…
                    hash_value = sqlite_db.smart_query(uri)
                    if hash_value:
                        logger.debug(f"SQLiteæ™ºèƒ½æŸ¥è¯¢å‘½ä¸­: {uri}")
                        return hash_value
                except Exception as e:
                    logger.error(f"SQLiteæŸ¥è¯¢å¤±è´¥: {e}")
        
        # 2. æŸ¥è¯¢å†…å­˜ç¼“å­˜
        cache = cls.get_cache(use_preload=use_preload)
        hash_value = cache.get(uri)
        if hash_value:
            logger.debug(f"å†…å­˜ç¼“å­˜å‘½ä¸­: {uri}")
            return hash_value
        
        # 3. å¦‚æœSQLiteä¸æ˜¯ä¼˜å…ˆçº§æˆ–è€…å‰é¢æŸ¥è¯¢å¤±è´¥ï¼Œå†æŸ¥è¯¢SQLite
        if MULTIPROCESS_CONFIG.get('use_sqlite', True) and not MULTIPROCESS_CONFIG.get('sqlite_priority', True):
            sqlite_db = cls._get_sqlite_db()
            if sqlite_db:
                try:
                    hash_value = sqlite_db.smart_query(uri)
                    if hash_value:
                        logger.debug(f"SQLiteå¤‡ç”¨æŸ¥è¯¢å‘½ä¸­: {uri}")
                        return hash_value
                except Exception as e:
                    logger.error(f"SQLiteå¤‡ç”¨æŸ¥è¯¢å¤±è´¥: {e}")
        
        logger.debug(f"å“ˆå¸Œå€¼æœªæ‰¾åˆ°: {uri}")
        return None

    @classmethod
    def preload_cache_for_multiprocess(cls, cache_dict: Dict[str, str]) -> None:
        """ä¸ºå¤šè¿›ç¨‹ç¯å¢ƒé¢„åŠ è½½ç¼“å­˜
        
        Args:
            cache_dict: é¢„åŠ è½½çš„ç¼“å­˜å­—å…¸        """
        MULTIPROCESS_CONFIG['preload_cache'] = cache_dict
        logger.info(f"å·²é¢„åŠ è½½ç¼“å­˜ï¼Œå…± {len(cache_dict)} ä¸ªæ¡ç›®")
    
    @classmethod
    def configure_multiprocess(cls, enable_auto_save: bool = False,
                             enable_global_cache: bool = True,
                             preload_cache: Optional[Dict[str, str]] = None,
                             use_sqlite: bool = True,
                             sqlite_priority: bool = True) -> None:
        """é…ç½®å¤šè¿›ç¨‹ç¯å¢ƒ
        
        Args:
            enable_auto_save: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¿å­˜ï¼ˆå¤šè¿›ç¨‹ä¸‹å»ºè®®å…³é—­ï¼‰
            enable_global_cache: æ˜¯å¦å¯ç”¨å…¨å±€ç¼“å­˜æŸ¥è¯¢
            preload_cache: é¢„åŠ è½½çš„ç¼“å­˜å­—å…¸
            use_sqlite: æ˜¯å¦å¯ç”¨SQLiteå­˜å‚¨
            sqlite_priority: SQLiteæŸ¥è¯¢æ˜¯å¦ä¼˜å…ˆäºå†…å­˜ç¼“å­˜
        """
        MULTIPROCESS_CONFIG.update({
            'enable_auto_save': enable_auto_save,
            'enable_global_cache': enable_global_cache,
            'preload_cache': preload_cache,
            'use_sqlite': use_sqlite,
            'sqlite_priority': sqlite_priority
        })
        logger.info(f"å¤šè¿›ç¨‹é…ç½®å·²æ›´æ–°: auto_save={enable_auto_save}, global_cache={enable_global_cache}, "
                   f"sqlite={use_sqlite}, sqlite_priority={sqlite_priority}")

    @classmethod
    def migrate_to_sqlite(cls, force_refresh: bool = False) -> int:
        """å°†JSONç¼“å­˜æ•°æ®è¿ç§»åˆ°SQLite
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            int: è¿ç§»çš„è®°å½•æ•°
        """
        sqlite_db = cls._get_sqlite_db()
        if not sqlite_db:
            logger.error("SQLiteæ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œè¿ç§»")
            return 0
        
        total_migrated = 0
        
        try:
            # 1. ä»JSONæ–‡ä»¶è¿ç§»
            for json_file in GLOBAL_HASH_FILES:
                if os.path.exists(json_file):
                    count = sqlite_db.migrate_from_json(json_file)
                    sqlite_db._get_connection().commit()
                    total_migrated += count
                    logger.info(f"ä» {json_file} è¿ç§»äº† {count} æ¡è®°å½•")
            
            # 2. ä»å†…å­˜ç¼“å­˜è¿ç§»
            if force_refresh:
                cls.refresh_cache()
            
            cache = cls.get_cache()
            if cache:
                # å°†å†…å­˜ç¼“å­˜è½¬æ¢ä¸ºSQLiteè®°å½•æ ¼å¼
                records = []
                for uri, hash_value in cache.items():
                    records.append((uri, hash_value, {}))  # ç©ºå…ƒæ•°æ®
                
                count = sqlite_db.batch_add_hashes(records)
                sqlite_db._get_connection().commit()
                total_migrated += count
                logger.info(f"ä»å†…å­˜ç¼“å­˜è¿ç§»äº† {count} æ¡è®°å½•")
            
            logger.info(f"SQLiteè¿ç§»å®Œæˆï¼Œæ€»å…±è¿ç§» {total_migrated} æ¡è®°å½•")
            return total_migrated
            
        except Exception as e:
            logger.error(f"SQLiteè¿ç§»å¤±è´¥: {e}")
            return 0

    @classmethod
    def export_sqlite_to_json(cls, output_file: str = None, format_type: str = 'new') -> bool:
        """å°†SQLiteæ•°æ®å¯¼å‡ºåˆ°JSONæ ¼å¼ï¼ˆå…¼å®¹æ€§æ”¯æŒï¼‰
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ŒNoneä½¿ç”¨é»˜è®¤è·¯å¾„
            format_type: æ ¼å¼ç±»å‹ ('new' æˆ– 'old')
            
        Returns:
            bool: æ˜¯å¦å¯¼å‡ºæˆåŠŸ
        """
        sqlite_db = cls._get_sqlite_db()
        if not sqlite_db:
            logger.error("SQLiteæ•°æ®åº“æœªåˆå§‹åŒ–")
            return False
        
        if output_file is None:
            output_file = GLOBAL_HASH_FILES[0].replace('.json', '_exported.json')
        
        try:
            success = sqlite_db.export_to_json(output_file, format_type)
            sqlite_db._get_connection().commit()
            if success:
                logger.info(f"SQLiteæ•°æ®å·²å¯¼å‡ºåˆ° {output_file}")
            return success
        except Exception as e:
            logger.error(f"å¯¼å‡ºSQLiteæ•°æ®å¤±è´¥: {e}")
            return False

    @classmethod
    def get_database_statistics(cls) -> Dict[str, any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'memory_cache': cls.get_cache_stats(),
            'sqlite': None
        }
        
        sqlite_db = cls._get_sqlite_db()
        if sqlite_db:
            try:
                stats['sqlite'] = sqlite_db.get_statistics()
            except Exception as e:
                logger.error(f"è·å–SQLiteç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
                stats['sqlite'] = {'error': str(e)}
        
        return stats

    @classmethod
    def smart_query_with_formats(cls, uri: str, target_formats: List[str] = None) -> List[Dict[str, any]]:
        """æ™ºèƒ½æŸ¥è¯¢ï¼Œæ”¯æŒæ ¼å¼è½¬æ¢åŒ¹é…
        
        Args:
            uri: æŸ¥è¯¢çš„URI
            target_formats: ç›®æ ‡æ ¼å¼åˆ—è¡¨ï¼Œå¦‚ ['jpg', 'png', 'webp']
            
        Returns:
            List[Dict]: åŒ¹é…çš„è®°å½•åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        sqlite_db = cls._get_sqlite_db()
        if not sqlite_db:
            return []
        try:
            # ä½¿ç”¨SQLiteçš„æ™ºèƒ½æŸ¥è¯¢åŠŸèƒ½
            hash_value = sqlite_db.smart_query(uri)
            if hash_value:
                logger.debug(f"æ™ºèƒ½æŸ¥è¯¢ {uri} æ‰¾åˆ°å“ˆå¸Œå€¼: {hash_value}")
                return [{'uri': uri, 'hash_value': hash_value}]
            return []
        except Exception as e:
            logger.error(f"æ™ºèƒ½æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    @classmethod
    def get_cache_stats(cls) -> Dict[str, any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with _cache_lock:
            stats = {
                'cache_size': len(cls._cache),
                'initialized': cls._initialized,
                'last_refresh': cls._last_refresh,
                'last_save': cls._last_save,
                'hash_counter': cls._hash_counter,
                'multiprocess_config': MULTIPROCESS_CONFIG.copy(),
                'sqlite_enabled': cls._sqlite_db is not None
            }
            
            # å¦‚æœSQLiteå¯ç”¨ï¼Œæ·»åŠ SQLiteç»Ÿè®¡ä¿¡æ¯
            if cls._sqlite_db:
                try:
                    sqlite_stats = cls._sqlite_db.get_statistics()
                    stats['sqlite_stats'] = sqlite_stats
                except Exception as e:
                    stats['sqlite_error'] = str(e)
            
            return stats

class ImgUtils:
    """å›¾ç‰‡å·¥å…·ç±»"""
    
    @staticmethod
    def get_img_files(directory):
        """è·å–ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            list: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        image_files = []
        image_extensions = ('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.bmp', '.gif', '.tiff')
        
        try:
            for root, _, files in os.walk(directory):
                for file in files:
                    if file.lower().endswith(image_extensions):
                        image_files.append(os.path.join(root, file))
        except Exception as e:
            logger.error(f"æ‰«æç›®å½•å¤±è´¥ {directory}: {e}")
            return []
                    
        return image_files
    
@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœçš„æ•°æ®ç±»"""
    uri: str  # æ ‡å‡†åŒ–çš„URI
    hash_value: dict  # å›¾ç‰‡å“ˆå¸Œå€¼
    file_type: str  # æ–‡ä»¶ç±»å‹ï¼ˆ'image' æˆ– 'archive'ï¼‰
    original_path: str  # åŸå§‹æ–‡ä»¶è·¯å¾„

class ImageHashCalculator:
    """å›¾ç‰‡å“ˆå¸Œè®¡ç®—ç±»"""
    
    @staticmethod
    def normalize_path(path: str, internal_path: str = None) -> str:
        """æ ‡å‡†åŒ–è·¯å¾„ä¸ºURIæ ¼å¼
        
        Args:
            path: æ–‡ä»¶è·¯å¾„
            internal_path: å‹ç¼©åŒ…å†…éƒ¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: æ ‡å‡†åŒ–çš„URI
        """
        if internal_path:
            return PathURIGenerator.generate(f"{path}!{internal_path}")
        return PathURIGenerator.generate(path)

    @staticmethod
    def get_hash_from_url(url: str) -> Optional[str]:
        """
        æ ¹æ®URLæŸ¥è¯¢å…¨å±€å“ˆå¸Œå€¼
        Args:
            url: æ ‡å‡†åŒ–çš„URI
        Returns:
            str: å“ˆå¸Œå€¼å­—ç¬¦ä¸²ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            if not url:
                logger.warning("URLä¸ºç©º")
                return None

            # æ ‡å‡†åŒ–URLæ ¼å¼
            normalized_url = PathURIGenerator.generate(url) if '://' not in url else url
            if not normalized_url:
                logger.warning(f"[#update_log]URLæ ‡å‡†åŒ–å¤±è´¥: {url}")
                return None
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            cached_hashes = HashCache.get_cache()
            if not cached_hashes:
                logger.debug("[#hash_calc]å“ˆå¸Œç¼“å­˜ä¸ºç©º")
            else:
                if hash_value := cached_hashes.get(normalized_url):
                    logger.debug(f"[#hash_calc]ä»ç¼“å­˜æ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
                    return hash_value
            
            # æœªå‘½ä¸­ç¼“å­˜æ—¶ä¸»åŠ¨æ‰«æå…¨å±€æ–‡ä»¶
            for hash_file in GLOBAL_HASH_FILES:
                if not os.path.exists(hash_file):
                    logger.debug(f"[#hash_calc]å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {hash_file}")
                    continue
                try:
                    with open(hash_file, 'rb') as f:
                        data = orjson.loads(f.read())
                        if not data:
                            logger.debug(f"[#hash_calc]å“ˆå¸Œæ–‡ä»¶ä¸ºç©º: {hash_file}")
                            continue
                        # å¤„ç†æ–°æ—§æ ¼å¼
                        hashes = data.get('hashes', data) if 'hashes' in data else data
                        if not hashes:
                            logger.debug(f"[#hash_calc]å“ˆå¸Œæ•°æ®ä¸ºç©º: {hash_file}")
                            continue
                        if hash_value := hashes.get(normalized_url):
                            if isinstance(hash_value, dict):
                                hash_str = hash_value.get('hash')
                                if hash_str:
                                    logger.debug(f"[#hash_calc]ä»å…¨å±€æ–‡ä»¶æ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
                                    return hash_str
                            else:
                                logger.debug(f"[#hash_calc]ä»å…¨å±€æ–‡ä»¶æ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
                                return str(hash_value)
                except Exception as e:
                    logger.warning(f"[#update_log]è¯»å–å“ˆå¸Œæ–‡ä»¶å¤±è´¥ {hash_file}: {e}")
                    continue

            logger.debug(f"[#hash_calc]æœªæ‰¾åˆ°å“ˆå¸Œå€¼: {normalized_url}")
            return None
            
        except Exception as e:
            logger.warning(f"[#update_log]æŸ¥è¯¢å“ˆå¸Œå¤±è´¥ {url}: {e}")
            return None

    @staticmethod
    def calculate_phash(image_path_or_data, hash_size=10, url=None, auto_save=True, use_preload=False):
        """ä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œç®—æ³•è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼ï¼ˆSQLite + JSONåŒå­˜å‚¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            image_path_or_data: å¯ä»¥æ˜¯å›¾ç‰‡è·¯å¾„(str/Path)ã€BytesIOå¯¹è±¡ã€byteså¯¹è±¡æˆ–PIL.Imageå¯¹è±¡
            hash_size: å“ˆå¸Œå¤§å°ï¼Œé»˜è®¤å€¼ä¸º10
            url: å›¾ç‰‡çš„URLï¼Œç”¨äºè®°å½•æ¥æºã€‚å¦‚æœä¸ºNoneä¸”image_path_or_dataæ˜¯è·¯å¾„ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†åŒ–çš„URI
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜åˆ°å…¨å±€æ–‡ä»¶ï¼ˆå¤šè¿›ç¨‹ç¯å¢ƒä¸‹å»ºè®®å…³é—­ï¼‰
            use_preload: æ˜¯å¦ä½¿ç”¨é¢„åŠ è½½ç¼“å­˜ï¼ˆå¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ¨èå¼€å¯ï¼‰
            
        Returns:
            dict: åŒ…å«å“ˆå¸Œå€¼å’Œå…ƒæ•°æ®çš„å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
            {
                'hash': str,  # 16è¿›åˆ¶æ ¼å¼çš„æ„ŸçŸ¥å“ˆå¸Œå€¼
                'size': int,  # å“ˆå¸Œå¤§å°
                'url': str,   # æ ‡å‡†åŒ–çš„URI
                'from_cache': bool,  # æ˜¯å¦æ¥è‡ªç¼“å­˜
                'storage_backend': str,  # å­˜å‚¨åç«¯ ('sqlite', 'json', 'memory')
            }
        """
        try:
            # ç”Ÿæˆæ ‡å‡†åŒ–çš„URI
            if url is None and isinstance(image_path_or_data, (str, Path)):
                path_str = str(image_path_or_data)
                url = PathURIGenerator.generate(path_str)
              # ä¼˜å…ˆä»ç¼“å­˜æŸ¥è¯¢ï¼ˆæ”¯æŒå¤šè¿›ç¨‹é¢„åŠ è½½ç¼“å­˜å’ŒSQLiteæ™ºèƒ½æŸ¥è¯¢ï¼‰
            if url and MULTIPROCESS_CONFIG.get('enable_global_cache', True):
                if use_preload:
                    cached_hash = HashCache.get_hash(url, use_preload=True)
                else:
                    cached_hash = HashCache.get_hash(url)
                    
                if cached_hash:
                    # åˆ¤æ–­å“ˆå¸Œæ¥æºçš„å­˜å‚¨åç«¯
                    storage_backend = 'memory'
                    if MULTIPROCESS_CONFIG.get('use_sqlite', True):
                        storage_backend = 'sqlite'
                    elif MULTIPROCESS_CONFIG.get('preload_cache'):
                        storage_backend = 'preload'
                    else:
                        storage_backend = 'json'
                    
                    return {
                        'hash': cached_hash,
                        'size': HASH_PARAMS['hash_size'],
                        'url': url,
                        'from_cache': True,
                        'storage_backend': storage_backend
                    }
            
            # ------ æ–°å¢ï¼šå‹ç¼©åŒ…åŒåä¸åŒè·¯å¾„å…±ç”¨å“ˆå¸Œ ------
            # åœ¨è®¡ç®—æ–°å“ˆå¸Œä¹‹å‰æ£€æŸ¥åŒåæ–‡ä»¶
            if url and MULTIPROCESS_CONFIG.get('enable_global_cache', True):
                db = get_db_cached()
                uri_info = db.parse_uri(url)
                if uri_info.get('source_type') == 'archive' and uri_info.get('filename'):
                    filename = uri_info['filename']
                    try:
                        conn = db._get_connection()
                        cursor = conn.execute(
                            "SELECT * FROM image_hashes WHERE filename = ? AND source_type = 'archive' ORDER BY calculated_time DESC",
                            (filename,)
                        )
                        rows = cursor.fetchall()
                    except Exception as e:
                        logger.error(f"[hash_calc] æ•°æ®åº“åŒåå‹ç¼©åŒ…æŸ¥è¯¢å¤±è´¥: {e}")
                        rows = []
                    if rows:
                        hash_value = rows[0]['hash_value']
                        # æ’å…¥å½“å‰è·¯å¾„æ–°è®°å½•
                        db.add_hash(url, hash_value)
                        logger.info(f"[hash_calc] åŒåä¸åŒè·¯å¾„å‹ç¼©åŒ…å…±ç”¨å“ˆå¸Œ: {filename} -> {hash_value}")
                        return {
                            'hash': hash_value,
                            'size': HASH_PARAMS['hash_size'], 
                            'url': url,
                            'from_cache': True,
                            'storage_backend': 'sqlite_same_name'
                        }
            # ------ æ–°å¢é€»è¾‘ç»“æŸ ------
            
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ™è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
            # å¦‚æœæ²¡æœ‰æä¾›URLä¸”è¾“å…¥æ˜¯è·¯å¾„ï¼Œåˆ™ç”Ÿæˆæ ‡å‡†åŒ–çš„URI
            if url is None and isinstance(image_path_or_data, (str, Path)):
                path_str = str(image_path_or_data)
                url = PathURIGenerator.generate(path_str)  # ä½¿ç”¨æ–°ç±»ç”ŸæˆURI
                logger.debug(f"[#hash_calc]æ­£åœ¨è®¡ç®—URI: {url} çš„å“ˆå¸Œå€¼")
            
            # æ”¶é›†å›¾ç‰‡å…ƒæ•°æ®
            image_metadata = {}
            file_size = None
            image_dimensions = None
            file_times = {}
            
            # æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©ä¸åŒçš„æ‰“å¼€æ–¹å¼
            if isinstance(image_path_or_data, (str, Path)):
                pil_img = Image.open(image_path_or_data)
                # è·å–æ–‡ä»¶å…ƒæ•°æ®
                try:
                    file_stat = os.stat(image_path_or_data)
                    file_size = file_stat.st_size
                    file_times = {
                        'created': file_stat.st_ctime,
                        'modified': file_stat.st_mtime,
                        'accessed': file_stat.st_atime
                    }
                except:
                    pass
            elif isinstance(image_path_or_data, BytesIO):
                pil_img = Image.open(image_path_or_data)
                # å°è¯•è·å–BytesIOçš„å¤§å°
                try:
                    current_pos = image_path_or_data.tell()
                    image_path_or_data.seek(0, 2)  # ç§»åˆ°æœ«å°¾
                    file_size = image_path_or_data.tell()
                    image_path_or_data.seek(current_pos)  # æ¢å¤ä½ç½®
                except:
                    pass
            elif isinstance(image_path_or_data, bytes):
                pil_img = Image.open(BytesIO(image_path_or_data))
                file_size = len(image_path_or_data)
            elif isinstance(image_path_or_data, Image.Image):
                pil_img = image_path_or_data
            elif hasattr(image_path_or_data, 'read') and hasattr(image_path_or_data, 'seek'):
                # æ”¯æŒmmapå’Œç±»æ–‡ä»¶å¯¹è±¡
                try:
                    # é¦–å…ˆå°è¯•ç›´æ¥è½¬æ¢ä¸ºBytesIOï¼ˆé€‚ç”¨äºmmapå¯¹è±¡ï¼‰
                    buffer = BytesIO(image_path_or_data)
                    pil_img = Image.open(buffer)
                except Exception as inner_e:
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•è¯»å–å†…å®¹åå†è½¬æ¢
                    logger.debug(f"[#hash_calc]ç›´æ¥è½¬æ¢å¤±è´¥ï¼Œå°è¯•è¯»å–å†…å®¹: {inner_e}")
                    try:
                        position = image_path_or_data.tell()  # ä¿å­˜å½“å‰ä½ç½®
                        image_path_or_data.seek(0)  # å›åˆ°å¼€å¤´
                        content = image_path_or_data.read()  # è¯»å–å…¨éƒ¨å†…å®¹
                        image_path_or_data.seek(position)  # æ¢å¤ä½ç½®
                        pil_img = Image.open(BytesIO(content))
                        file_size = len(content)
                    except Exception as e2:
                        raise ValueError(f"æ— æ³•ä»ç±»æ–‡ä»¶å¯¹è±¡è¯»å–å›¾ç‰‡æ•°æ®: {e2}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(image_path_or_data)}")
            
            # è·å–å›¾ç‰‡å°ºå¯¸
            if pil_img:
                image_dimensions = (pil_img.width, pil_img.height)
                image_metadata['width'] = pil_img.width
                image_metadata['height'] = pil_img.height
                image_metadata['mode'] = pil_img.mode
                image_metadata['format'] = getattr(pil_img, 'format', None)
            
            # ä½¿ç”¨imagehashåº“çš„phashå®ç°
            hash_obj = imagehash.phash(pil_img, hash_size=hash_size)
            
            # åªåœ¨æ‰“å¼€æ–°å›¾ç‰‡æ—¶å…³é—­
            if not isinstance(image_path_or_data, Image.Image):
                pil_img.close()
            
            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
            hash_str = str(hash_obj)
            
            if not hash_str:
                raise ValueError("ç”Ÿæˆçš„å“ˆå¸Œå€¼ä¸ºç©º")
                
            # å°†æ–°ç»“æœæ·»åŠ åˆ°ç¼“å­˜ï¼ˆæ”¯æŒSQLiteå’ŒJSONåŒå­˜å‚¨ï¼‰
            if url and MULTIPROCESS_CONFIG.get('enable_global_cache', True):
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'file_size': file_size,
                    'calculated_time': time.time(),
                    **image_metadata
                }
                
                # åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨ä¿å­˜
                save_enabled = MULTIPROCESS_CONFIG.get('enable_auto_save', True) and auto_save
                HashCache.add_hash(url, hash_str, auto_sync=save_enabled, metadata=metadata)
                
            logger.debug(f"è®¡ç®—çš„å“ˆå¸Œå€¼: {hash_str}")
            return {
                'hash': hash_str,
                'size': hash_size,
                'url': url,
                'from_cache': False,
                'storage_backend': 'computed',
                'metadata': image_metadata
            }
            
        except Exception as e:
            logger.warning(f"è®¡ç®—å¤±è´¥: {e}")
            return None

    @staticmethod
    def calculate_hamming_distance(hash1, hash2):
        """è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œå€¼ä¹‹é—´çš„æ±‰æ˜è·ç¦»
        
        Args:
            hash1: ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼ï¼ˆå¯ä»¥æ˜¯å­—å…¸æ ¼å¼æˆ–å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            hash2: ç¬¬äºŒä¸ªå“ˆå¸Œå€¼ï¼ˆå¯ä»¥æ˜¯å­—å…¸æ ¼å¼æˆ–å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            
        Returns:
            int: æ±‰æ˜è·ç¦»ï¼Œå¦‚æœè®¡ç®—å¤±è´¥åˆ™è¿”å›float('inf')
        """
        try:
            # æ–°å¢ä»£ç ï¼šç»Ÿä¸€è½¬æ¢ä¸ºå°å†™
            hash1_str = hash1['hash'].lower() if isinstance(hash1, dict) else hash1.lower()
            hash2_str = hash2['hash'].lower() if isinstance(hash2, dict) else hash2.lower()
            
            # ç¡®ä¿ä¸¤ä¸ªå“ˆå¸Œå€¼é•¿åº¦ç›¸åŒ
            if len(hash1_str) != len(hash2_str):
                logger.info(f"å“ˆå¸Œé•¿åº¦ä¸ä¸€è‡´: {len(hash1_str)} vs {len(hash2_str)}")
                return float('inf')
            
            # å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°
            hash1_int = int(hash1_str, 16)
            hash2_int = int(hash2_str, 16)
            
            # è®¡ç®—å¼‚æˆ–å€¼
            xor = hash1_int ^ hash2_int
            
            # ä½¿ç”¨Python 3.10+çš„bit_count()æ–¹æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(int, 'bit_count'):
                distance = xor.bit_count()
            else:
                # ä¼˜åŒ–çš„åˆ†æ²»æ³•å®ç°
                x = xor
                x = (x & 0x5555555555555555) + ((x >> 1) & 0x5555555555555555)  # æ¯2ä½åˆ†ç»„
                x = (x & 0x3333333333333333) + ((x >> 2) & 0x3333333333333333)  # æ¯4ä½åˆ†ç»„
                x = (x & 0x0F0F0F0F0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F0F0F0F0F)  # æ¯8ä½åˆ†ç»„
                # ç”±äºå“ˆå¸Œå€¼ä¸è¶…è¿‡64ä½ï¼Œå¯ä»¥ç›´æ¥ç´¯åŠ é«˜ä½
                x = (x + (x >> 8)) & 0x00FF00FF00FF00FF  # ç´¯åŠ æ¯ä¸ªå­—èŠ‚
                x = (x + (x >> 16)) & 0x0000FFFF0000FFFF  # ç´¯åŠ æ¯2ä¸ªå­—èŠ‚
                distance = (x + (x >> 32)) & 0x7F  # æœ€ç»ˆç»“æœä¸ä¼šè¶…è¿‡64
            
            logger.info(f"æ¯”è¾ƒå“ˆå¸Œå€¼: {hash1_str} vs {hash2_str}, æ±‰æ˜è·ç¦»: {distance}")
            return distance
            
        except Exception as e:
            logger.info(f"è®¡ç®—æ±‰æ˜è·ç¦»æ—¶å‡ºé”™: {e}")
            return float('inf')

    @staticmethod
    def match_existing_hashes(path: Path, existing_hashes: Dict[str, dict], is_global: bool = False) -> Dict[str, ProcessResult]:
        """åŒ¹é…è·¯å¾„ä¸ç°æœ‰å“ˆå¸Œå€¼"""
        results = {}
        # if 'å»å›¾' in path:
        #     return results
        if not existing_hashes:
            return results
            
        file_path = str(path).replace('\\', '/')
        
        # ç»Ÿä¸€ä½¿ç”¨åŒ…å«åŒ¹é…
        for uri, hash_value in existing_hashes.items():
            if file_path in uri:
                # å¦‚æœæ˜¯å…¨å±€å“ˆå¸Œï¼Œhash_valueæ˜¯å­—ç¬¦ä¸²ï¼›å¦‚æœæ˜¯æœ¬åœ°å“ˆå¸Œï¼Œhash_valueæ˜¯å­—å…¸
                if isinstance(hash_value, str):
                    hash_str = hash_value
                else:
                    hash_str = hash_value.get('hash', '')
                    
                file_type = 'archive' if '!' in uri else 'image'
                results[uri] = ProcessResult(
                    uri=uri,
                    hash_value={'hash': hash_str, 'size': HASH_PARAMS['hash_size'], 'url': uri},
                    file_type=file_type,
                    original_path=file_path
                )
                # æ ¹æ®æ¥æºæ˜¾ç¤ºä¸åŒçš„æ—¥å¿—
                log_prefix = "[ğŸŒå…¨å±€ç¼“å­˜]" if is_global else "[ğŸ“æœ¬åœ°ç¼“å­˜]"
                logger.info(f"[#hash_calc]{log_prefix} {file_type}: {file_path}  å“ˆå¸Œå€¼: {hash_str}")
        
        if results:
            logger.info(f"[#hash_calc]âœ… ä½¿ç”¨ç°æœ‰å“ˆå¸Œæ–‡ä»¶çš„ç»“æœï¼Œè·³è¿‡å¤„ç†")
            logger.info(f"[#current_progress]å¤„ç†è¿›åº¦: [å·²å®Œæˆ] ä½¿ç”¨ç°æœ‰å“ˆå¸Œ")
            
        return results



    @staticmethod
    def are_images_similar(hash1_str, hash2_str, threshold=2):
        """åˆ¤æ–­ä¸¤ä¸ªå›¾ç‰‡æ˜¯å¦ç›¸ä¼¼
        
        Args:
            hash1_str: ç¬¬ä¸€ä¸ªå›¾ç‰‡çš„å“ˆå¸Œå€¼
            hash2_str: ç¬¬äºŒä¸ªå›¾ç‰‡çš„å“ˆå¸Œå€¼
            threshold: æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œå°äºç­‰äºæ­¤å€¼è®¤ä¸ºç›¸ä¼¼
            
        Returns:
            bool: æ˜¯å¦ç›¸ä¼¼
        """
        distance = ImageHashCalculator.calculate_hamming_distance(hash1_str, hash2_str)
        return distance <= threshold 

    @staticmethod
    def compare_folder_images(folder_path, hash_type='phash', threshold=2, output_html=None):
        """æ”¹è¿›ç‰ˆï¼šå¢åŠ å°ºå¯¸å’Œæ¸…æ™°åº¦å¯¹æ¯”"""
        console = Console()
        folder = Path(folder_path)
        image_exts = ('*.jpg', '*.jpeg', '*.png', '*.avif', '*.jxl', '*.webp', '*.JPG', '*.JPEG')
        image_files = [f for ext in image_exts for f in folder.glob(f'**/{ext}')]
        
        results = []
        # æ–°å¢ï¼šé¢„è®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å…ƒæ•°æ®
        meta_data = {}
        for img in image_files:
            width, height = ImageClarityEvaluator.get_image_size(img)
            meta_data[str(img)] = {
                'width': width,
                'height': height,
                'clarity': 0.0  # ç¨åå¡«å……
            }
        
        # æ‰¹é‡è®¡ç®—æ¸…æ™°åº¦
        clarity_scores = ImageClarityEvaluator.batch_evaluate(image_files)
        for path, score in clarity_scores.items():
            meta_data[path]['clarity'] = score
        
        for img1, img2 in combinations(image_files, 2):
            try:
                hash1 = getattr(ImageHashCalculator, f'calculate_{hash_type}')(img1)
                hash2 = getattr(ImageHashCalculator, f'calculate_{hash_type}')(img2)
                distance = ImageHashCalculator.calculate_hamming_distance(hash1, hash2)
                is_similar = distance <= threshold
                
                results.append({
                    'pair': (img1, img2),
                    'distance': distance,
                    'similar': is_similar
                })
            except Exception as e:
                logger.warning(f"å¯¹æ¯” {img1} å’Œ {img2} å¤±è´¥: {e}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_content = [
            '<!DOCTYPE html>',
            '<html><head>',
            '<meta charset="UTF-8">',
            '<title>å›¾ç‰‡ç›¸ä¼¼åº¦å¯¹æ¯”æŠ¥å‘Š</title>',
            '<style>',
            '  table {border-collapse: collapse; width: 100%; margin: 20px 0;}',
            '  th, td {border: 1px solid #ddd; padding: 12px; text-align: center;}',
            '  img {max-width: 200px; height: auto; transition: transform 0.3s;}',
            '  img:hover {transform: scale(1.5); cursor: zoom-in;}',
            '  .similar {color: #28a745;}',
            '  .different {color: #dc3545;}',
            '  body {font-family: Arial, sans-serif; margin: 30px;}',
            '</style></head><body>',
            '<h1>å›¾ç‰‡ç›¸ä¼¼åº¦å¯¹æ¯”æŠ¥å‘Š</h1>',
            f'<p><strong>å¯¹æ¯”æ—¶é—´</strong>ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>',
            f'<p><strong>å“ˆå¸Œç®—æ³•</strong>ï¼š{hash_type.upper()}</p>',
            f'<p><strong>ç›¸ä¼¼é˜ˆå€¼</strong>ï¼š{threshold}</p>',
            '<table>',
            '  <tr><th>å›¾ç‰‡1</th><th>å›¾ç‰‡2</th><th>å°ºå¯¸</th><th>æ¸…æ™°åº¦</th><th>æ±‰æ˜è·ç¦»</th><th>ç›¸ä¼¼åˆ¤å®š</th></tr>'
        ]

        for res in results:
            status_class = 'similar' if res['similar'] else 'different'
            status_icon = 'âœ…' if res['similar'] else 'âŒ'
            img1_path = str(res['pair'][0].resolve()).replace('\\', '/')
            img2_path = str(res['pair'][1].resolve()).replace('\\', '/')
            img1_meta = meta_data[str(res['pair'][0])]
            img2_meta = meta_data[str(res['pair'][1])]
            
            html_content.append(
                f'<tr>'
                f'<td><img src="file:///{img1_path}" alt="{img1_path}"><br>{img1_meta["width"]}x{img1_meta["height"]}</td>'
                f'<td><img src="file:///{img2_path}" alt="{img2_path}"><br>{img2_meta["width"]}x{img2_meta["height"]}</td>'
                f'<td>{img1_meta["width"]}x{img1_meta["height"]} vs<br>{img2_meta["width"]}x{img2_meta["height"]}</td>'
                f'<td>{img1_meta["clarity"]:.1f} vs {img2_meta["clarity"]:.1f}</td>'
                f'<td>{res["distance"]}</td>'
                f'<td class="{status_class}">{status_icon} {"ç›¸ä¼¼" if res["similar"] else "ä¸ç›¸ä¼¼"}</td>'
                f'</tr>'
            )
            
        html_content.extend(['</table></body></html>'])
        
        # æ§åˆ¶å°ç®€åŒ–è¾“å‡º
        console.print(f"å®Œæˆå¯¹æ¯”ï¼Œå…±å¤„ç† {len(results)} ç»„å›¾ç‰‡å¯¹")
        
        if output_html:
            output_path = Path(output_html)
            output_path.write_text('\n'.join(html_content), encoding='utf-8')
            console.print(f"HTMLæŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š[bold green]{output_path.resolve()}[/]")
            console.print("æç¤ºï¼šåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ–‡ä»¶å¯æŸ¥çœ‹äº¤äº’å¼å›¾ç‰‡ç¼©æ”¾æ•ˆæœ")

    @staticmethod
    def save_global_hashes(hash_dict: Dict[str, str]) -> None:
        """ä¿å­˜å“ˆå¸Œå€¼åˆ°å…¨å±€ç¼“å­˜æ–‡ä»¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            output_dict = {
                "_hash_params": f"hash_size={HASH_PARAMS['hash_size']};hash_version={HASH_PARAMS['hash_version']}",
                "hashes": hash_dict  # ç›´æ¥å­˜å‚¨å­—ç¬¦ä¸²å­—å…¸ï¼Œè·³è¿‡ä¸­é—´è½¬æ¢
            }
            
            os.makedirs(os.path.dirname(GLOBAL_HASH_FILES[-1]), exist_ok=True)
            with open(GLOBAL_HASH_FILES[-1], 'wb') as f:
                # ä½¿ç”¨orjsonçš„OPT_SERIALIZE_NUMPYé€‰é¡¹æå‡æ•°å€¼å¤„ç†æ€§èƒ½
                f.write(orjson.dumps(output_dict, 
                    option=orjson.OPT_INDENT_2 | 
                    orjson.OPT_SERIALIZE_NUMPY |
                    orjson.OPT_APPEND_NEWLINE))
            logger.debug(f"å·²ä¿å­˜å“ˆå¸Œç¼“å­˜åˆ°: {GLOBAL_HASH_FILES[-1]}")  # æ”¹ä¸ºdebugçº§åˆ«å‡å°‘æ—¥å¿—é‡
        except Exception as e:
            logger.warning(f"ä¿å­˜å…¨å±€å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}", exc_info=True)

    @staticmethod
    def load_global_hashes() -> Dict[str, str]:
        """ä»å…¨å±€ç¼“å­˜æ–‡ä»¶åŠ è½½æ‰€æœ‰å“ˆå¸Œå€¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            if os.path.exists(GLOBAL_HASH_FILES[-1]):
                with open(GLOBAL_HASH_FILES[-1], 'rb') as f:
                    data = orjson.loads(f.read())
                    return {
                        uri: entry["hash"] if isinstance(entry, dict) else entry
                        for uri, entry in data.get("hashes", {}).items()
                    }
            return {}
        except Exception as e:
            logger.warning(f"åŠ è½½å…¨å±€å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}", exc_info=True)
            return {}

    @staticmethod
    def save_hash_file_path(file_path) -> None:
        """å°†å“ˆå¸Œæ–‡ä»¶è·¯å¾„ä¿å­˜åˆ°è·¯å¾„é›†åˆæ–‡ä»¶ä¸­
        
        Args:
            file_path: è¦ä¿å­˜çš„å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²æˆ–å¯ä»¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²çš„å¯¹è±¡ï¼‰
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(file_path, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•ä½¿ç”¨æœ€åä¸€ä¸ªå…ƒç´ 
                if file_path:
                    file_path = str(file_path[-1])
                else:
                    raise TypeError("æ— æ³•ä»ç©ºåˆ—è¡¨è·å–æ–‡ä»¶è·¯å¾„")
            elif not isinstance(file_path, (str, bytes, os.PathLike)):
                # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ã€å­—èŠ‚æˆ–PathLikeå¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                file_path = str(file_path)
                
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(HASH_FILES_LIST), exist_ok=True)
            # è¿½åŠ æ¨¡å¼å†™å…¥è·¯å¾„
            with open(HASH_FILES_LIST, 'a', encoding='utf-8') as f:
                f.write(f"{file_path}\n")
            logger.info(f"å·²å°†å“ˆå¸Œæ–‡ä»¶è·¯å¾„ä¿å­˜åˆ°é›†åˆæ–‡ä»¶: {HASH_FILES_LIST}")
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œæ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")

    @staticmethod
    def get_latest_hash_file_path() -> Optional[str]:
        """è·å–æœ€æ–°çš„å“ˆå¸Œæ–‡ä»¶è·¯å¾„
        
        Returns:
            Optional[str]: æœ€æ–°çš„å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        try:
            if not os.path.exists(HASH_FILES_LIST):
                return None
                
            with open(HASH_FILES_LIST, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            if not lines:
                return None
                
            # è·å–æœ€åä¸€è¡Œå¹¶å»é™¤ç©ºç™½å­—ç¬¦
            latest_path = lines[-1].strip()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(latest_path):
                return latest_path
            else:
                logger.error(f"æœ€æ–°çš„å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {latest_path}")
                return None
                
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°å“ˆå¸Œæ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return None

    @staticmethod
    def load_existing_hashes(directory: Path) -> Dict[str, str]:
        """æœ€ç»ˆä¿®å¤ç‰ˆå“ˆå¸ŒåŠ è½½"""
        existing_hashes = {}
        try:
            hash_file = directory / 'image_hashes.json'
            if not hash_file.exists():
                return existing_hashes
            
            with open(hash_file, 'rb') as f:
                data = orjson.loads(f.read())
                
                if 'results' in data:
                    results = data['results']
                    for uri, result in results.items():
                        # ä¿®å¤å­—æ®µæ˜ å°„é—®é¢˜
                        if isinstance(result, dict):
                            # ç»Ÿä¸€ä½¿ç”¨hashå­—æ®µ
                            hash_str = str(result.get('hash', ''))
                            # æ·»åŠ ç±»å‹éªŒè¯
                            if len(hash_str) >= 8:  # è°ƒæ•´ä¸ºæ›´å®½æ¾çš„é•¿åº¦éªŒè¯
                                existing_hashes[uri] = {
                                    'hash': hash_str.lower(),
                                    'size': HASH_PARAMS['hash_size'],
                                    'url': uri
                                }
                                continue
                        logger.warning(f"æ— æ•ˆçš„å“ˆå¸Œæ¡ç›®: {uri} - {result}")
                
                logger.info(f"ä» {hash_file} åŠ è½½åˆ°æœ‰æ•ˆæ¡ç›®: {len(existing_hashes)}")
                return existing_hashes
            
        except Exception as e:
            logger.error(f"åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
            return {}

    @staticmethod
    def save_hash_results(results: Dict[str, ProcessResult], output_path: Path, dry_run: bool = False) -> None:
        """ä¿å­˜å“ˆå¸Œç»“æœåˆ°æ–‡ä»¶"""
        try:
            output = {
                "_hash_params": f"hash_size={HASH_PARAMS['hash_size']};hash_version={HASH_PARAMS['hash_version']}",
                "dry_run": dry_run,
                "hashes": {uri: {"hash": result.hash_value['hash']} for uri, result in results.items()}  # ä¸å…¨å±€ç»“æ„ä¸€è‡´
            }
            
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'wb') as f:
                f.write(orjson.dumps(output, option=orjson.OPT_INDENT_2))
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path} (å…± {len(output['hashes'])} ä¸ªå“ˆå¸Œå€¼)")
            
            ImageHashCalculator.save_hash_file_path(str(output_path))
            
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œç»“æœå¤±è´¥: {e}") 


    @staticmethod
    def load_hashes(file_path: Path) -> Tuple[Dict[str, str], dict]:
        """åŠ è½½å“ˆå¸Œæ–‡ä»¶ï¼ˆä»…å¤„ç†æ–°ç»“æ„ï¼‰"""
        try:
            with open(file_path, 'rb') as f:
                data = orjson.loads(f.read())
                hash_params = ImageHashCalculator.parse_hash_params(data.get('_hash_params', ''))
                return {
                    k: v['hash']  # æ–°ç»“æ„å¼ºåˆ¶è¦æ±‚hashå­—æ®µ
                    for k, v in data.get('hashes', {}).items()
                }, hash_params
        except Exception as e:
            logger.debug(f"å°è¯•æ–°ç»“æ„åŠ è½½å¤±è´¥ï¼Œå›é€€æ—§ç»“æ„: {e}")
            return LegacyHashLoader.load(file_path)  # åˆ†ç¦»çš„æ—§ç»“æ„åŠ è½½

    @staticmethod
    def migrate_hashes(file_path: Path) -> None:
        """è¿ç§»æ—§å“ˆå¸Œæ–‡ä»¶åˆ°æ–°æ ¼å¼"""
        hashes, params = ImageHashCalculator.load_hashes(file_path)
        if hashes:
            ImageHashCalculator.save_hash_results(
                results={uri: ProcessResult(uri=uri, hash_value={"hash": h}, file_type="unknown", original_path=None) for uri, h in hashes.items()},
                output_path=file_path,
                dry_run=False
            )
            logger.info(f"å·²è¿ç§»å“ˆå¸Œæ–‡ä»¶æ ¼å¼: {file_path}")

    @staticmethod
    def test_hash_cache():
        """ç¼“å­˜åŠŸèƒ½æµ‹è¯•demo"""
        console = Console()
        test_file = r"E:\2EHV\test\0.jpg"  # æ›¿æ¢ä¸ºå®é™…æµ‹è¯•æ–‡ä»¶è·¯å¾„
        url=ImageHashCalculator.normalize_path(test_file)
        # ç¬¬ä¸€æ¬¡è®¡ç®—ï¼ˆåº”åŠ è½½ç¼“å­˜ï¼‰
        console.print("\n[bold cyan]=== ç¬¬ä¸€æ¬¡è®¡ç®—ï¼ˆåŠ è½½ç¼“å­˜ï¼‰===[/]")
        start_time = time.time()
        hash1 = ImageHashCalculator.calculate_phash(test_file)
        load_hashes=ImageHashCalculator.load_hashes(test_file)
        console.print(f"è€—æ—¶: {time.time()-start_time:.2f}s")
        
        # ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆåº”ä½¿ç”¨ç¼“å­˜ï¼‰
        console.print("\n[bold cyan]=== ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰===[/]")
        start_time = time.time()
        hash2 = ImageHashCalculator.calculate_phash(test_file)
        console.print(f"è€—æ—¶: {time.time()-start_time:.2f}s")
        
        # éªŒè¯ç»“æœ
        console.print("\n[bold]æµ‹è¯•ç»“æœ:[/]")
        console.print(f"å“ˆå¸Œå€¼æ˜¯å¦ä¸€è‡´: {hash1['hash'] == hash2['hash']}")
        console.print(f"æ˜¯å¦æ¥è‡ªç¼“å­˜: {hash1.get('from_cache', False)} | {hash2.get('from_cache', False)}")

class LegacyHashLoader:
    """æ—§ç»“æ„å“ˆå¸Œæ–‡ä»¶åŠ è½½å™¨ï¼ˆåæœŸå¯æ•´ä½“ç§»é™¤ï¼‰"""
    
    @staticmethod
    def load(file_path: Path) -> Tuple[Dict[str, str], dict]:
        """åŠ è½½æ—§ç‰ˆå“ˆå¸Œæ–‡ä»¶ç»“æ„"""
        try:
            with open(file_path, 'rb') as f:
                data = orjson.loads(f.read())
                return LegacyHashLoader._parse_old_structure(data)
        except:
            return {}, {}
    @staticmethod
    def parse_hash_params(param_str: str) -> dict:
        """è§£æå“ˆå¸Œå‚æ•°å­—ç¬¦ä¸²"""
        params = {
            'hash_size': HASH_PARAMS['hash_size'],
            'hash_version': HASH_PARAMS['hash_version']
        }
        for pair in param_str.split(';'):
            if '=' in pair:
                key, val = pair.split('=', 1)
                if key in params:
                    params[key] = int(val)
        return params
    @staticmethod
    def _parse_old_structure(data: dict) -> Tuple[Dict[str, str], dict]:
        """è§£æä¸åŒæ—§ç‰ˆç»“æ„"""
        hash_params = ImageHashCalculator.parse_hash_params(data.get('_hash_params', ''))
        
        # ç‰ˆæœ¬1: åŒ…å«resultsçš„ç»“æ„
        if 'results' in data:
            return {
                uri: item.get('hash') or uri.split('[hash-')[1].split(']')[0]
                for uri, item in data['results'].items()
            }, hash_params
            
        # ç‰ˆæœ¬2: åŒ…å«filesçš„ç»“æ„
        if 'files' in data:
            return {
                k: v if isinstance(v, str) else v.get('hash', '')
                for k, v in data['files'].items()
            }, hash_params
            
        # ç‰ˆæœ¬3: æœ€æ—§å…¨å±€æ–‡ä»¶ç»“æ„
        return {
            k: v['hash'] if isinstance(v, dict) else v
            for k, v in data.items()
            if k not in ['_hash_params', 'dry_run', 'input_paths']
        }, hash_params 
        

if __name__ == "__main__":
    # æ‰§è¡Œç¼“å­˜æµ‹è¯•
    ImageHashCalculator.test_hash_cache()
    # åŸæœ‰æ¸…æ™°åº¦æµ‹è¯•ä¿æŒä¸å˜
    def test_image_clarity():
        """æ¸…æ™°åº¦è¯„ä¼°æµ‹è¯•demo"""
        test_dir = Path(r"E:\2EHV\test")
        console = Console()
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = list(test_dir.glob("*.jpg")) + list(test_dir.glob("*.png"))
        console.print(f"æ‰¾åˆ° {len(image_files)} å¼ æµ‹è¯•å›¾ç‰‡")
        
        # è®¡ç®—æ¸…æ™°åº¦å¹¶æ’åº
        results = []
        for img_path in image_files[:1300]:  # é™åˆ¶å‰1300å¼ 
            score = ImageClarityEvaluator.calculate_definition(img_path)
            results.append((img_path.name, score))
        
        # æŒ‰æ¸…æ™°åº¦é™åºæ’åº
        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)
        
        # è¾“å‡ºç»“æœ
        console.print(Markdown("## å›¾åƒæ¸…æ™°åº¦æ’å"))
        console.print("| æ’å | æ–‡ä»¶å | æ¸…æ™°åº¦å¾—åˆ† |")
        console.print("|------|--------|------------|")
        for idx, (name, score) in enumerate(sorted_results[:20], 1):
            console.print(f"| {idx:2d} | {name} | {score:.2f} |")
            
    # æ‰§è¡Œæµ‹è¯•
    # test_image_clarity()

