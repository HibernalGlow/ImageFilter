import os
import re
import shutil
import random
import zipfile
import io
from datetime import datetime
from typing import List, Dict, Set, Tuple, Optional, Union
from PIL import Image
from hashu.core.calculate_hash_custom import ImageClarityEvaluator
from loguru import logger
from .core.utils import (
    handle_multi_main_file, create_shortcut
)

# æ”¯æŒçš„å‹ç¼©åŒ…æ ¼å¼
ARCHIVE_EXTENSIONS = {
    '.zip', '.rar', '.7z', '.cbr', '.cbz', 
    '.cb7', '.cbt', '.tar', '.gz', '.bz2'
}
# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
IMAGE_EXTENSIONS = {
    '.jpg', '.jpeg', '.png', '.webp', '.avif', '.jxl',
    '.gif', '.bmp', '.tiff', '.tif', '.heic', '.heif'
}

# è™šæ‹Ÿæ–‡ä»¶å¤¹ä¼ªæ‰©å±•ï¼ˆç”¨äºæŠŠæ–‡ä»¶å¤¹å½“ä½œå‹ç¼©åŒ…å‚ä¸åˆ†ç»„ / æŒ‡æ ‡è®¡ç®—ï¼‰
VIRTUAL_FOLDER_SUFFIX = '.folderzip'

def get_image_count(archive_path: str) -> int:
    try:
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                count = sum(1 for f in zf.namelist() 
                           if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS)
                return count
        except zipfile.BadZipFile:
            return 0
    except Exception as e:
        logger.error("[#error_log] âŒ ç»Ÿè®¡å›¾ç‰‡æ•°é‡å¤±è´¥ {}: {}", archive_path, e)
        return 0

def calculate_representative_width(archive_path: str, sample_count: int = 3) -> int:
    try:
        ext = os.path.splitext(archive_path)[1].lower()
        if ext not in {'.zip', '.cbz'}:
            return 0
        image_files = []
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for info in zf.infolist():
                    if os.path.splitext(info.filename.lower())[1] in IMAGE_EXTENSIONS:
                        image_files.append((info.filename, info.file_size))
        except zipfile.BadZipFile:
            logger.info("[#error_log] âš ï¸ æ— æ•ˆçš„ZIPæ–‡ä»¶: {}", archive_path)
            return 0
        if not image_files:
            return 0
        image_files.sort(key=lambda x: x[1], reverse=True)
        samples = []
        if image_files:
            samples.append(image_files[0][0])
            if len(image_files) > 2:
                samples.append(image_files[len(image_files)//2][0])
            top_30_percent = image_files[:max(3, len(image_files) // 3)]
            while len(samples) < sample_count and top_30_percent:
                sample = random.choice(top_30_percent)[0]
                if sample not in samples:
                    samples.append(sample)
        widths = []
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for sample in samples:
                    try:
                        with zf.open(sample) as file:
                            img_data = file.read()
                            with Image.open(io.BytesIO(img_data)) as img:
                                widths.append(img.width)
                    except Exception as e:
                        logger.info("[#error_log] âš ï¸ è¯»å–å›¾ç‰‡å®½åº¦å¤±è´¥ {}: {}", sample, str(e))
                        continue
        except Exception as e:
            logger.info("[#error_log] âš ï¸ æ‰“å¼€ZIPæ–‡ä»¶å¤±è´¥: {}", str(e))
            return 0
        if not widths:
            return 0
        return int(sorted(widths)[len(widths)//2])
    except Exception as e:
        logger.info("[#error_log] âŒ è®¡ç®—ä»£è¡¨å®½åº¦å¤±è´¥ {}: {}", archive_path, str(e))
        return 0
def shorten_number_cn(
    number: int, 
    precision: int = 1,
    use_w: bool = True
) -> str:
    """
    å°†å¤§æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡ä¹ æƒ¯çš„ç¼©å†™æ ¼å¼
    
    Args:
        number: è¦è½¬æ¢çš„æ•°å­—
        precision: å°æ•°ä½ç²¾åº¦ï¼ˆé»˜è®¤1ä½ï¼‰
        use_w: æ˜¯å¦ä½¿ç”¨"ä¸‡"ä¸ºå•ä½ï¼ˆTrueæ—¶ä¸‡è¿›åˆ¶ï¼ŒFalseæ—¶åƒè¿›åˆ¶ï¼‰
    
    Returns:
        str: æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        
    Examples:
        >>> shorten_number_cn(18500)
        '1.8w'
        >>> shorten_number_cn(215_0000)
        '215w'
        >>> shorten_number_cn(3_5000_0000)
        '3.5äº¿'
    """
    number=round(number)
    if number < 1000:
        return str(number)
        
    if use_w:
        # ä¸‡è¿›åˆ¶å¤„ç†
        if number >= 1_0000_0000:
            # äº¿å•ä½å¤„ç†
            value = number / 1_0000_0000
            unit = 'äº¿'
        elif number >= 1_0000:
            # ä¸‡å•ä½å¤„ç†
            value = number / 1_0000
            unit = 'w'
        else:
            # åƒå•ä½å¤„ç†ï¼ˆå½“å°äº1ä¸‡æ—¶ï¼‰
            value = number / 1000
            unit = 'k'
    else:
        # åƒè¿›åˆ¶å¤„ç†
        if number >= 1_000_000_000:
            value = number / 1_000_000_000
            unit = 'B'
        elif number >= 1_000_000:
            value = number / 1_000_000
            unit = 'M'
        else:
            value = number / 1000
            unit = 'k'

    # å¤„ç†ç²¾åº¦
    if value == int(value):
        # æ•´æ•°æƒ…å†µçœç•¥å°æ•°éƒ¨åˆ†
        return f"{int(value)}{unit}"
    else:
        # ä¿ç•™æŒ‡å®šä½æ•°å°æ•°
        return f"{value:.{precision}f}{unit}".rstrip('0').rstrip('.') 


class ReportGenerator:
    """ç”Ÿæˆå¤„ç†æŠ¥å‘Šçš„ç±»"""
    def __init__(self):
        self.report_sections = []
        self.stats = {
            'total_files': 0,
            'total_groups': 0,
            'moved_to_trash': 0,
            'moved_to_multi': 0,
            'skipped_files': 0,
            'created_shortcuts': 0
        }
        self.group_details = []
    def add_group_detail(self, group_name: str, details: Dict):
        self.group_details.append({'name': group_name, 'details': details})
    def update_stats(self, key: str, value: int = 1):
        self.stats[key] = self.stats.get(key, 0) + value
    def add_section(self, title: str, content: str):
        self.report_sections.append({'title': title, 'content': content})
    def generate_report(self, base_dir: str) -> str:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report = [
            f"# æ–‡ä»¶å¤„ç†æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"å¤„ç†ç›®å½•: {base_dir}",
            "",
            "## å¤„ç†ç»Ÿè®¡",
            f"- æ€»æ–‡ä»¶æ•°: {shorten_number_cn(self.stats['total_files'])}",
            f"- æ€»åˆ†ç»„æ•°: {shorten_number_cn(self.stats['total_groups'])}",
            f"- ç§»åŠ¨åˆ°trashç›®å½•: {shorten_number_cn(self.stats['moved_to_trash'])}",
            f"- ç§»åŠ¨åˆ°multiç›®å½•: {shorten_number_cn(self.stats['moved_to_multi'])}",
            f"- è·³è¿‡çš„æ–‡ä»¶: {shorten_number_cn(self.stats['skipped_files'])}",
            f"- åˆ›å»ºçš„å¿«æ·æ–¹å¼: {shorten_number_cn(self.stats['created_shortcuts'])}",
            ""
        ]
        if self.group_details:
            report.append("## å¤„ç†è¯¦æƒ…åˆ—è¡¨")
            for group in self.group_details:
                report.append(f"- **{group['name']}**")
                details = group['details']
                if 'chinese_versions' in details:
                    report.append("  - æ±‰åŒ–ç‰ˆæœ¬:")
                    for file in details['chinese_versions']:
                        report.append(f"    - {file}")
                if 'other_versions' in details:
                    report.append("  - å…¶ä»–ç‰ˆæœ¬:")
                    for file in details['other_versions']:
                        report.append(f"    - {file}")
                if 'actions' in details:
                    report.append("  - æ‰§è¡Œæ“ä½œ:")
                    for action in details['actions']:
                        report.append(f"    - {action}")
                report.append("")
        for section in self.report_sections:
            report.append(f"## {section['title']}")
            report.append(section['content'])
            report.append("")
        return "\n".join(report)
    def save_report(self, base_dir: str, filename: Optional[str] = None):
        if filename is None:
            filename = f"å¤„ç†æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path = os.path.join(base_dir, filename)
        report_content = self.generate_report(base_dir)
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            return report_path
        except Exception as e:
            logger.error("[#error_log] âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {}", str(e))
            logger.exception("[#error_log] å¼‚å¸¸å †æ ˆ:")
            logger.info("[#process] ğŸ’¥ é‡åˆ°ä¸¥é‡é”™è¯¯ï¼Œè¯·æ£€æŸ¥error_logé¢æ¿")
            return None

def process_file_with_count(file_path: str, name_only_mode: bool = False) -> Tuple[str, str, Dict[str, Union[int, float]]]:
    import re, os, zipfile, random
    from PIL import Image
    from hashu.core.calculate_hash_custom import ImageClarityEvaluator
    from rawfilter.run import shorten_number_cn
    from loguru import logger
    full_path = file_path
    dir_name = os.path.dirname(file_path)
    file_name = os.path.basename(file_path)
    name, ext = os.path.splitext(file_name)
    name = re.sub(r'\{[^}]*\}', '', name)
    metrics = {'width': 0, 'page_count': 0, 'clarity_score': 0.0}

    # å¤„ç†è™šæ‹Ÿæ–‡ä»¶å¤¹æƒ…å½¢ï¼šè·¯å¾„ä¸å­˜åœ¨ä¸”ä»¥ä¼ªæ‰©å±•ç»“å°¾ï¼ŒçœŸå®ç›®å½•ä¸ºå»æ‰ä¼ªæ‰©å±•åçš„åŒåç›®å½•
    is_virtual = False
    real_folder = None
    if not os.path.exists(full_path) and file_name.endswith(VIRTUAL_FOLDER_SUFFIX):
        # çœŸå®ç›®å½• = å»æ‰ .folderzip åç¼€ï¼Œä¾‹å¦‚ A/B/C.folderzip -> A/B/C
        real_folder = os.path.splitext(full_path)[0]
        # å¦‚æœ real_folder ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºæ ¹ç›®å½•
        # ä»…å½“è¯¥ç›®å½•çœŸå®å­˜åœ¨æ‰æ ‡è®°ä¸ºè™šæ‹Ÿ
        if real_folder and os.path.isdir(real_folder):
            is_virtual = True
        else:
            # å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ç»“åˆè¿›ç¨‹å·¥ä½œç›®å½•å†åˆ¤æ–­
            if os.path.isdir(os.path.abspath(real_folder)):
                is_virtual = True
        if is_virtual:
            logger.info("[#virtual] ğŸ“‚ ä½œä¸ºè™šæ‹Ÿå‹ç¼©åŒ…å¤„ç†ç›®å½•: {}", real_folder or '.')

    if is_virtual:
        # éå†ç›®å½•æ”¶é›†å›¾ç‰‡æ–‡ä»¶ï¼ˆä»…ç¬¬ä¸€å±‚ï¼Œé¿å…æ·±åº¦éå†æˆæœ¬ï¼›å¯æŒ‰éœ€æ›´æ”¹ä¸º os.walkï¼‰
        try:
            abs_folder = os.path.abspath(real_folder) if real_folder else os.getcwd()
            image_files = []
            for root, _, files in os.walk(abs_folder):
                for f in files:
                    if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS:
                        fpath = os.path.join(root, f)
                        try:
                            size = os.path.getsize(fpath)
                        except OSError:
                            size = 0
                        image_files.append((fpath, size))
                break  # åªå¤„ç†ä¸€å±‚
            metrics['page_count'] = len(image_files)
            if image_files:
                # é€‰æ ·æœ¬åŒå½’æ¡£é€»è¾‘
                image_files.sort(key=lambda x: x[1], reverse=True)
                samples = []
                samples.append(image_files[0][0])
                if len(image_files) > 2:
                    samples.append(image_files[len(image_files)//2][0])
                top_30 = image_files[:max(3, len(image_files)//3)]
                import random as _r
                while len(samples) < 3 and top_30:
                    c = _r.choice(top_30)[0]
                    if c not in samples:
                        samples.append(c)
                widths = []
                clarity_scores = []
                for sp in samples:
                    try:
                        with Image.open(sp) as img:
                            widths.append(img.width)
                            # è¯»å–äºŒè¿›åˆ¶ç”¨äºæ¸…æ™°åº¦è®¡ç®—
                            with open(sp, 'rb') as rf:
                                clarity_scores.append(ImageClarityEvaluator.calculate_definition(rf.read()))
                    except Exception as e:
                        logger.info("[#virtual] âš ï¸ æ ·æœ¬è¯»å–å¤±è´¥ {}: {}", sp, e)
                if widths:
                    metrics['width'] = int(sorted(widths)[len(widths)//2])
                if clarity_scores:
                    metrics['clarity_score'] = sum(clarity_scores)/len(clarity_scores)
        except Exception as e:
            logger.error("[#error_log] è™šæ‹Ÿç›®å½•æŒ‡æ ‡è®¡ç®—å¤±è´¥ {}: {}", real_folder, e)
        # è™šæ‹Ÿç›®å½•ä¸é‡å‘½åï¼Œä¿æŒ pseudo åç§°ä¾›åˆ†ç»„å¼•ç”¨
        return file_path, file_path, metrics

    # å¦‚æœæ˜¯ä»…åç§°æ¨¡å¼ï¼Œè·³è¿‡æ‰€æœ‰å†…éƒ¨åˆ†æ
    if name_only_mode:
        logger.info("[#name_only] ğŸ·ï¸ ä»…åç§°æ¨¡å¼ï¼Œè·³è¿‡å†…éƒ¨åˆ†æ: {}", file_name)
        # ç›´æ¥è¿”å›åŸå§‹æ–‡ä»¶åï¼ˆå·²ç§»é™¤{}æ ‡è®°ï¼‰
        new_name = f"{name}{ext}"
        new_path = os.path.join(dir_name, new_name) if dir_name else new_name
        return file_path, new_path, metrics

    page_match = re.search(r'\{(\d+)@PX\}', file_name)
    if page_match:
        metrics['page_count'] = int(page_match.group(1))
    else:
        metrics['page_count'] = get_image_count(full_path)
    metrics['width'] = calculate_representative_width(full_path)
    try:
        with zipfile.ZipFile(full_path, 'r') as zf:
            image_files = [f for f in zf.namelist() if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS]
            if image_files:
                sample_files = random.sample(image_files, min(5, len(image_files)))
                scores = []
                for sample in sample_files:
                    with zf.open(sample) as f:
                        img_data = f.read()
                        scores.append(ImageClarityEvaluator.calculate_definition(img_data))
                metrics['clarity_score'] = sum(scores) / len(scores) if scores else 0.0
    except Exception as e:
        logger.error("[#error_log] æ¸…æ™°åº¦è®¡ç®—å¤±è´¥ {}: {}", file_path, str(e))
    parts = []
    if metrics['width'] > 0:
        parts.append(f"{shorten_number_cn(metrics['width'], use_w=True)}@WD")
    if metrics['page_count'] > 0:
        parts.append(f"{shorten_number_cn(metrics['page_count'], use_w=True)}@PX")
    if metrics['clarity_score'] > 0:
        parts.append(f"{shorten_number_cn(int(metrics['clarity_score']), use_w=True)}@DE")
    metrics_str = "{" + ",".join(parts) + "}" if parts else ""
    new_name = f"{name}{metrics_str}{ext}"
    new_path = os.path.join(dir_name, new_name) if dir_name else new_name
    return file_path, new_path, metrics

def process_file_group(group_files: List[str], base_dir: str, trash_dir: str, create_shortcuts: bool = False, enable_multi_main: bool = False, name_only_mode: bool = False, trash_only: bool = False) -> Dict:
    from .core.utils import handle_multi_main_file, create_shortcut
    from rawfilter.__main__ import clean_filename, is_in_blacklist, is_chinese_version, has_original_keywords, group_similar_files, safe_move_file
    from rawfilter.run import shorten_number_cn
    from loguru import logger
    result_stats = {'moved_to_trash': 0, 'moved_to_multi': 0, 'created_shortcuts': 0}
    
    # å°†è™šæ‹Ÿä¼ªæ–‡ä»¶ (.folderzip) è§£æä¸ºçœŸå®ç›®å½•è·¯å¾„ï¼Œç”¨äºåç»­ç‰©ç†æ“ä½œ
    def _resolve_virtual_path(path: str) -> Tuple[str, bool]:
        if path.endswith(VIRTUAL_FOLDER_SUFFIX):
            return os.path.dirname(path), True
        return path, False

    # ç»Ÿä¸€çš„å®‰å…¨ç§»åŠ¨ï¼šæ–‡ä»¶èµ°åŸæœ‰é€»è¾‘ï¼Œç›®å½•ä½¿ç”¨ç›®å½•ç§»åŠ¨æ ¡éªŒ
    def safe_move_entry(src_path: str, dst_path: str) -> bool:
        real_src, is_virtual = _resolve_virtual_path(src_path)
        try:
            if os.path.isdir(real_src):
                # ç›®å½•ç§»åŠ¨ï¼šç¡®ä¿ç›®æ ‡ä¸Šçº§å­˜åœ¨ï¼Œç„¶åæ•´ä½“ç§»åŠ¨
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                try:
                    # è‹¥ç›®æ ‡ä½ç½®å·²æœ‰åŒåç›®å½•ï¼Œå°è¯•åˆå¹¶/å›é€€
                    if os.path.exists(dst_path):
                        # ç›´æ¥å°†æºç›®å½•ç§»åˆ°ç›®æ ‡ç›®å½•çš„åŒåä¸‹ï¼ˆé¿å…è¦†ç›–ï¼‰ï¼Œè¿½åŠ æ—¶é—´æˆ³åç¼€
                        base = os.path.basename(real_src)
                        dst_parent = dst_path
                        if not os.path.isdir(dst_parent):
                            # è‹¥ dst_path ä¸æ˜¯ç›®å½•ï¼Œå–å…¶çˆ¶ç›®å½•
                            dst_parent = os.path.dirname(dst_path)
                        os.makedirs(dst_parent, exist_ok=True)
                        ts = datetime.now().strftime('%H%M%S')
                        final_dst = os.path.join(dst_parent, f"{base}__mv_{ts}")
                        shutil.move(real_src, final_dst)
                        return os.path.exists(final_dst)
                    else:
                        shutil.move(real_src, dst_path)
                        return os.path.exists(dst_path)
                except Exception as e:
                    logger.error("[#error_log] ç›®å½•ç§»åŠ¨å¤±è´¥ {} -> {}: {}", real_src, dst_path, e)
                    return False
            else:
                # æ–‡ä»¶ç§»åŠ¨ï¼šè°ƒç”¨æ—¢æœ‰çš„å®‰å…¨é€»è¾‘
                from rawfilter.__main__ import safe_move_file as _safe_move_file
                return _safe_move_file(real_src, dst_path)
        except Exception as e:
            logger.error("[#error_log] ç§»åŠ¨å¼‚å¸¸ {} -> {}: {}", src_path, dst_path, e)
            return False
    # å‚æ•°è°ƒè¯•æ—¥å¿—ï¼Œä¾¿äºç¡®è®¤ trash_only ç­‰å¼€å…³æ˜¯å¦æ­£ç¡®ä¼ é€’
    logger.info("[#debug] å‚æ•°: trash_only={} enable_multi_main={} name_only_mode={} æ–‡ä»¶æ•°={}", trash_only, enable_multi_main, name_only_mode, len(group_files))
    group_base_name, _ = clean_filename(group_files[0])
    group_id = abs(hash(group_base_name)) % 10000
    filtered_files = [f for f in group_files if not is_in_blacklist(f)]
    if not filtered_files:
        logger.info("[#group_info] â­ï¸ ç»„[{}]è·³è¿‡: æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨é»‘åå•ä¸­", group_base_name)
        return result_stats
    chinese_versions = []
    other_versions = []
    for f in filtered_files:
        full_path = os.path.join(base_dir, f)
        if is_chinese_version(f):
            chinese_versions.append(full_path)
        else:
            other_versions.append(full_path)
    chinese_has_original = any(has_original_keywords(f) for f in chinese_versions)
    if not chinese_has_original:
        original_keyword_versions = [f for f in other_versions if has_original_keywords(os.path.basename(f))]
        if original_keyword_versions:
            chinese_versions.extend(original_keyword_versions)
            other_versions = [f for f in other_versions if not has_original_keywords(os.path.basename(f))]
            logger.info(f"[#file_ops] ğŸ“ å°†{len(original_keyword_versions)}ä¸ªåŒ…å«åŸç‰ˆå…³é”®è¯çš„æ–‡ä»¶å½’å…¥ä¿ç•™åˆ—è¡¨")
    processed_files = []
    file_metrics = {}
    for file in chinese_versions + other_versions:
        old_path, new_path, metrics = process_file_with_count(file, name_only_mode)
        processed_files.append((old_path, new_path))
        file_metrics[old_path] = metrics
    best_metrics = {
        'width': max((m['width'] for m in file_metrics.values()), default=0),
        'page_count': min((m['page_count'] for m in file_metrics.values() if m['page_count'] > 0), default=0),
        'clarity_score': max((m['clarity_score'] for m in file_metrics.values()), default=0)
    }
    metrics_same = {
        'width': len(set(m['width'] for m in file_metrics.values() if m['width'] > 0)) <= 1,
        'page_count': len(set(m['page_count'] for m in file_metrics.values() if m['page_count'] > 0)) <= 1,
        'clarity_score': len(set(m['clarity_score'] for m in file_metrics.values() if m['clarity_score'] > 0)) <= 1
    }
    updated_files = []
    if name_only_mode:
        # ä»…åç§°æ¨¡å¼ï¼šè·³è¿‡é‡å‘½åï¼Œä¿æŒåŸå§‹æ–‡ä»¶å
        logger.info("[#name_only] ğŸ·ï¸ ä»…åç§°æ¨¡å¼ï¼Œè·³è¿‡ç»„å·å’ŒæŒ‡æ ‡æ·»åŠ ")
        updated_files = [(old_path, old_path) for old_path, _ in processed_files]
    else:
        # æ ‡å‡†æ¨¡å¼ï¼šæ·»åŠ ç»„å·å’ŒæŒ‡æ ‡
        for old_path, _ in processed_files:
            # è™šæ‹Ÿä¼ªæ–‡ä»¶ä¸åšçœŸå®æ–‡ä»¶ç³»ç»Ÿé‡å‘½å
            if old_path.endswith(VIRTUAL_FOLDER_SUFFIX):
                updated_files.append((old_path, old_path))
                continue
            metrics = file_metrics[old_path]
            parts = []
            parts.append(f"ğŸª†G{group_id:04d}")
            if metrics['width'] > 0:
                width_str = f"{shorten_number_cn(metrics['width'], use_w=True)}@WD"
                if not metrics_same['width'] and metrics['width'] == best_metrics['width']:
                    width_str = f"ğŸ“{width_str}"
                parts.append(width_str)
            if metrics['page_count'] > 0:
                page_str = f"{shorten_number_cn(metrics['page_count'], use_w=True)}@PX"
                if not metrics_same['page_count'] and metrics['page_count'] == best_metrics['page_count']:
                    page_str = f"ğŸ“„{page_str}"
                parts.append(page_str)
            if metrics['clarity_score'] > 0:
                clarity_str = f"{shorten_number_cn(int(metrics['clarity_score']), use_w=True)}@DE"
                if not metrics_same['clarity_score'] and metrics['clarity_score'] == best_metrics['clarity_score']:
                    clarity_str = f"ğŸ”{clarity_str}"
                parts.append(clarity_str)
            dir_name = os.path.dirname(old_path)
            file_name = os.path.basename(old_path)
            name, ext = os.path.splitext(file_name)
            name = re.sub(r'\{[^}]*\}', '', name)
            metrics_str = "{" + ",".join(parts) + "}" if parts else ""
            new_name = f"{metrics_str}{name}{ext}"
            new_path = os.path.join(dir_name, new_name)
            old_full_path = os.path.join(base_dir, old_path)
            new_full_path = os.path.join(base_dir, new_path)
            try:
                os.rename(old_full_path, new_full_path)
                updated_files.append((old_path, new_path))
                logger.info("[#file_ops] âœ… å·²é‡å‘½å: {} -> {}", old_path, new_path)
            except Exception as e:
                logger.error(f"[#error_log] âŒ é‡å‘½åå¤±è´¥ {old_path}: {str(e)}")
                updated_files.append((old_path, old_path))
    chinese_versions = [new_path for old_path, new_path in updated_files if old_path in chinese_versions]
    other_versions = [new_path for old_path, new_path in updated_files if old_path in other_versions]

    # ç»Ÿä¸€è°ƒç”¨å¯é…ç½®çš„è£å‰ªè§„åˆ™å¼•æ“ï¼ˆç‰ˆæœ¬å· -> æ— ä¿®æ­£ -> DL ç­‰ï¼‰
    try:
        from .core.pruner import apply_prune_rules
        chinese_versions, other_versions = apply_prune_rules(
            chinese_versions,
            other_versions,
            base_dir,
            trash_dir,
            result_stats,
            safe_move_entry,
            logger,
            create_shortcuts,
            create_shortcut,
        )
    except Exception as e:
        logger.error("[#error_log] è£å‰ªè§„åˆ™å¼•æ“å¼‚å¸¸: {}", e)

    # å…è®¸å¯¹è™šæ‹Ÿç»„æ‰§è¡Œç‰©ç†æ“ä½œï¼šå¯¹ .folderzip è§£æä¸ºå…¶ç›®å½•åè¿›è¡Œç§»åŠ¨/å¿«æ·æ–¹å¼åˆ›å»º

    if chinese_versions:
        if len(chinese_versions) > 1:
            if not trash_only:
                multi_dir = os.path.join(base_dir, 'multi')
                os.makedirs(multi_dir, exist_ok=True)
                if enable_multi_main:
                    try:
                        main_file = max(chinese_versions, key=lambda x: os.path.getsize(os.path.join(base_dir, x)))
                    except Exception:
                        main_file = chinese_versions[0]
                    real_main, is_virtual = _resolve_virtual_path(os.path.join(base_dir, main_file))
                    if os.path.isdir(real_main):
                        logger.info("[#file_ops] â­ï¸ multi-main è·³è¿‡ç›®å½•å€™é€‰: {}", main_file)
                    else:
                        if handle_multi_main_file(main_file, base_dir):
                            logger.info("[#file_ops] âœ… å·²å¤„ç†multi-mainæ–‡ä»¶: {}", main_file)
                for file in chinese_versions:
                    src_entry = os.path.join(base_dir, file)
                    real_src, _ = _resolve_virtual_path(src_entry)
                    rel_path = os.path.relpath(real_src, base_dir)
                    dst_path = os.path.join(multi_dir, rel_path)
                    if safe_move_entry(real_src, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°multi: {}", file)
                        result_stats['moved_to_multi'] += 1
            else:
                logger.info("[#pruner] ğŸ›‘ trash_only æ¨¡å¼ï¼šè·³è¿‡ multi ç§»åŠ¨ (æ±‰åŒ–å¤šç‰ˆæœ¬å…± {} ä¸ª)", len(chinese_versions))
            for other_file in other_versions:
                src_entry = os.path.join(base_dir, other_file)
                real_src, _ = _resolve_virtual_path(src_entry)
                rel_path = os.path.relpath(real_src, base_dir)
                dst_path = os.path.join(trash_dir, rel_path)
                if create_shortcuts:
                    shortcut_path = os.path.splitext(dst_path)[0]
                    if create_shortcut(real_src, shortcut_path):
                        logger.info("[#file_ops] âœ… å·²åˆ›å»ºå¿«æ·æ–¹å¼: {}", other_file)
                        result_stats['created_shortcuts'] += 1
                else:
                    if safe_move_entry(real_src, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°trash: {}", other_file)
                        result_stats['moved_to_trash'] += 1
        else:
            logger.info("[#group_info] ğŸ” ç»„[{}]å¤„ç†: å‘ç°1ä¸ªéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬ï¼Œä¿æŒåŸä½ç½®", group_base_name)
            for other_file in other_versions:
                src_entry = os.path.join(base_dir, other_file)
                real_src, _ = _resolve_virtual_path(src_entry)
                rel_path = os.path.relpath(real_src, base_dir)
                dst_path = os.path.join(trash_dir, rel_path)
                if create_shortcuts:
                    shortcut_path = os.path.splitext(dst_path)[0]
                    if create_shortcut(real_src, shortcut_path):
                        logger.info("[#file_ops] âœ… å·²åˆ›å»ºå¿«æ·æ–¹å¼: {}", other_file)
                        result_stats['created_shortcuts'] += 1
                else:
                    if safe_move_entry(real_src, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°trash: {}", other_file)
                        result_stats['moved_to_trash'] += 1
    else:
        if len(other_versions) > 1:
            if not trash_only:
                multi_dir = os.path.join(base_dir, 'multi')
                os.makedirs(multi_dir, exist_ok=True)
                if enable_multi_main:
                    try:
                        main_file = max(other_versions, key=lambda x: os.path.getsize(os.path.join(base_dir, x)))
                    except Exception:
                        main_file = other_versions[0]
                    real_main, is_virtual = _resolve_virtual_path(os.path.join(base_dir, main_file))
                    if os.path.isdir(real_main):
                        logger.info("[#file_ops] â­ï¸ multi-main è·³è¿‡ç›®å½•å€™é€‰: {}", main_file)
                    else:
                        if handle_multi_main_file(main_file, base_dir):
                            logger.info("[#file_ops] âœ… å·²å¤„ç†multi-mainæ–‡ä»¶: {}", main_file)
                for file in other_versions:
                    src_entry = os.path.join(base_dir, file)
                    real_src, _ = _resolve_virtual_path(src_entry)
                    rel_path = os.path.relpath(real_src, base_dir)
                    dst_path = os.path.join(multi_dir, rel_path)
                    if safe_move_entry(real_src, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°multi: {}", file)
                        result_stats['moved_to_multi'] += 1
                logger.info("[#group_info] ğŸ” ç»„[{}]å¤„ç†: æœªå‘ç°æ±‰åŒ–ç‰ˆæœ¬ï¼Œå‘ç°{}ä¸ªåŸç‰ˆï¼Œå·²ç§»åŠ¨åˆ°multi", group_base_name, len(other_versions))
            else:
                logger.info("[#pruner] ğŸ›‘ trash_only æ¨¡å¼ï¼šè·³è¿‡ multi ç§»åŠ¨ (åŸç‰ˆå¤šç‰ˆæœ¬å…± {} ä¸ª)", len(other_versions))
        else:
            logger.info("[#group_info] ğŸ” ç»„[{}]å¤„ç†: æœªå‘ç°æ±‰åŒ–ç‰ˆæœ¬ï¼Œä»…æœ‰1ä¸ªåŸç‰ˆï¼Œä¿æŒåŸä½ç½®", group_base_name)
    return result_stats

def process_directory(
    directory: str,
    report_generator: ReportGenerator,
    dry_run: bool = False,
    create_shortcuts: bool = False,
    enable_multi_main: bool = False,
    name_only_mode: bool = False,
    trash_only: bool = False,
    virtual_folders: bool = False,
    repacku_config_path: Optional[str] = None,
    auto_repacku: bool = True,
) -> None:
    from rawfilter.__main__ import group_similar_files
    from loguru import logger
    import os
    import json
    from pathlib import Path
    # å»¶è¿Ÿå¯¼å…¥ repacku åˆ†æå™¨ï¼ˆå¯é€‰ï¼‰
    def _load_repacku_config(cfg_path: str) -> Optional[dict]:
        try:
            with open(cfg_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error("[#error_log] è¯»å– repacku é…ç½®å¤±è´¥ {}: {}", cfg_path, e)
            return None
    repacku_tree = None
    repacku_cfg_used = None
    if virtual_folders:
        # 1) å¦‚æœç”¨æˆ·æŒ‡å®šäº†é…ç½®è·¯å¾„ï¼Œç›´æ¥è¯»å–
        search_root = Path(directory)
        if repacku_config_path and Path(repacku_config_path).is_file():
            repacku_cfg_used = repacku_config_path
            repacku_data = _load_repacku_config(repacku_config_path)
            repacku_tree = (repacku_data or {}).get('folder_tree') if repacku_data else None
        else:
            # 2) åœ¨å½“å‰ç›®å½•å¯»æ‰¾ *_config.json
            candidates = list(search_root.glob('*_config.json'))
            if candidates:
                repacku_cfg_used = str(candidates[0])
                repacku_data = _load_repacku_config(repacku_cfg_used)
                repacku_tree = (repacku_data or {}).get('folder_tree') if repacku_data else None
            elif auto_repacku:
                # 3) è‡ªåŠ¨è°ƒç”¨ repacku ç”Ÿæˆ
                try:
                    from repacku.core.folder_analyzer import analyze_folder
                    repacku_cfg_used = analyze_folder(search_root, target_file_types=["image"], display=False)
                    repacku_data = _load_repacku_config(repacku_cfg_used)
                    repacku_tree = (repacku_data or {}).get('folder_tree') if repacku_data else None
                    logger.info("[#process] ğŸ¤ å·²è‡ªåŠ¨ç”Ÿæˆ repacku é…ç½®: {}", repacku_cfg_used)
                except Exception as e:
                    logger.error("[#error_log] è‡ªåŠ¨è°ƒç”¨ repacku å¤±è´¥: {}", e)
        if repacku_tree is None:
            logger.info("[#process] âš ï¸ æœªèƒ½è·å¾— repacku é…ç½®ï¼Œå¯ç”¨ç®€å•æ–‡ä»¶å¤¹è™šæ‹Ÿæ¨¡å¼ (é¦–å±‚å«å›¾ç‰‡çš„ç›®å½•) ")
            try:
                simple_nodes = []
                for child in Path(directory).iterdir():
                    if child.is_dir():
                        # åˆ¤æ–­æ˜¯å¦åŒ…å«å›¾ç‰‡æ–‡ä»¶ï¼ˆé¦–å±‚ï¼‰
                        has_image = any(
                            f.is_file() and f.suffix.lower() in IMAGE_EXTENSIONS
                            for f in child.iterdir() if f.is_file()
                        )
                        if has_image:
                            simple_nodes.append(child)
                if simple_nodes:
                    repacku_tree = {
                        'path': directory,
                        'compress_mode': 'skip',
                        'children': [
                            {
                                'path': str(n),
                                'compress_mode': 'entire',
                                'file_types': {'image': 1},
                                'children': []
                            } for n in simple_nodes
                        ]
                    }
                    logger.info("[#process] ğŸ§© ç®€æ˜“è™šæ‹Ÿç›®å½•æ•°é‡: {}", len(simple_nodes))
            except Exception as e:
                logger.error("[#error_log] ç®€æ˜“è™šæ‹Ÿç›®å½•æšä¸¾å¤±è´¥: {}", e)
        else:
            logger.info("[#process] ğŸ§© å·²åŠ è½½ repacku é…ç½® (virtual folders): {}", repacku_cfg_used)
    trash_dir = os.path.join(directory, 'trash')
    if not dry_run:
        os.makedirs(trash_dir, exist_ok=True)
    all_files = []
    logger.info("[#process] ğŸ” æ­£åœ¨æ‰«æç›®å½•: {}", directory)
    for root, _, files in os.walk(directory):
        if 'trash' in root or 'multi' in root:
            logger.info("[#file_ops] â­ï¸ è·³è¿‡ç›®å½•: {}", root)
            continue
        for file in files:
            if os.path.splitext(file.lower())[1] in ARCHIVE_EXTENSIONS:
                rel_path = os.path.relpath(os.path.join(root, file), directory)
                all_files.append(rel_path)
                total = len(all_files)
                if total % 10 == 0:
                    logger.info("[@process] æ‰«æè¿›åº¦: {} / {}", total, total)
    # æ ¹æ® repacku æŠŠç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¤¹ä½œä¸ºâ€œè™šæ‹Ÿå‹ç¼©åŒ…â€è¿½åŠ 
    if virtual_folders and repacku_tree:
        def collect_virtual(node: dict):
            mode = node.get('compress_mode')
            path = node.get('path') or ''
            file_types = node.get('file_types') or {}
            # ä»…æŠŠåŒ…å« image æˆ– archive çš„ä¸”æ¨¡å¼ä¸º entire/selective çš„ç›®å½•çº³å…¥
            if mode in ('entire', 'selective') and (file_types.get('image') or file_types.get('archive')):
                # ä»¥ç›®å½•è·¯å¾„æœ«çº§åä¼ªé€ ä¸€ä¸ª zip åç§°ï¼Œåç»­ group_similar_files ä½¿ç”¨æ–‡ä»¶åèšç±»
                p = Path(path)
                if p.is_dir() and p.exists():
                    # ä¼ªæ–‡ä»¶æ”¾åœ¨è¯¥ç›®å½•çš„çˆ¶çº§ä¸‹ï¼šå½¢å¦‚ A/B/C.folderzip ï¼ˆè€Œä¸æ˜¯ A/B/C/C.folderzipï¼‰
                    rel = os.path.relpath(str(p), directory)
                    marker = rel + VIRTUAL_FOLDER_SUFFIX
                    all_files.append(marker)
            for child in node.get('children', []) or []:
                collect_virtual(child)
        collect_virtual(repacku_tree)
        if all_files:
            count_virtual = sum(1 for f in all_files if f.endswith(VIRTUAL_FOLDER_SUFFIX))
            if count_virtual:
                logger.info("[#process] ğŸ“¦ å·²è¿½åŠ è™šæ‹Ÿæ–‡ä»¶å¤¹æ ‡è®°æ•°: {}", count_virtual)
    if not all_files:
        logger.info("[#error_log] âš ï¸ ç›®å½• {} ä¸­æœªæ‰¾åˆ°å‹ç¼©æ–‡ä»¶", directory)
        return
    report_generator.update_stats('total_files', len(all_files))
    groups = group_similar_files(all_files)
    logger.info("[#stats] ğŸ“Š æ€»è®¡: {} ä¸ªæ–‡ä»¶, {} ä¸ªç»„", len(all_files), len(groups))
    report_generator.update_stats('total_groups', len(groups))
    logger.info("[#process] ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶ç»„...")
    from concurrent.futures import ProcessPoolExecutor, as_completed
    import os
    with ProcessPoolExecutor(max_workers=min(os.cpu_count() * 2, 8)) as executor:
        futures = {}
        for group_base_name, group_files in groups.items():
            if len(group_files) > 1:
                future = executor.submit(
                    process_file_group,
                    group_files,
                    directory,
                    trash_dir,
                    create_shortcuts,
                    enable_multi_main,
                    name_only_mode,
                    trash_only,
                )
                futures[future] = group_base_name
        completed = 0
        for future in as_completed(futures.keys()):
            completed += 1
            future_count = len(futures)
            scan_percent = completed / future_count * 100
            try:
                result_stats = future.result()
                for key, value in result_stats.items():
                    if value > 0:
                        report_generator.update_stats(key, value)
            except Exception as e:
                logger.error(f"[#error_log] âŒ å¤„ç†ç»„æ—¶å‡ºé”™: {futures[future]}, é”™è¯¯: {str(e)}")
            logger.info(f"[@stats] ç»„è¿›åº¦: ({completed}/{future_count}) {scan_percent:.2f}%")
